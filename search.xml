<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[智能网联汽车——安全问题]]></title>
    <url>%2F2019%2F12%2F26%2F%E6%99%BA%E8%83%BD%E7%BD%91%E8%81%94%E6%B1%BD%E8%BD%A6%E2%80%94%E2%80%94%E5%AE%89%E5%85%A8%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[乘车出行，安全是基本保障。本篇推送主要介绍汽车在安全方面的分类以及具体的策略。 一、汽车被动安全汽车发生碰撞事故，保护驾驶员的安全。主要包括安全气囊和安全带。 二、汽车主动安全汽车行驶是靠轮胎和地面产生的摩擦力来进行驱动和制动。一旦车辆加速度非常大，汽车的惯性比地面提供的摩擦力大时就会发生滑移，然后就会失稳发生事故。 主动安全就是保证汽车在失稳的情况下汽车可以稳定行驶。主要包括ABS（制动防抱死系统）、TCS（牵引力控制系统）、ESP（车身电子稳定系统）。 三、汽车智能安全主要是基于车辆传感器的一些驾驶辅助系统来预防事故的发生，比如前方碰撞预警系统、并线辅助系统、车道保持系统、疲劳预警系统等。之前也专门在《智能网联汽车——传感器与驾驶辅助》这一篇推送详细介绍过。 前三种安全对于汽车的架构不需要做太大的调整，接下来介绍的两种安全需要对汽车本身做重大的调整。 四、汽车功能安全功能安全是指智能车辆发生感知错误、决策失误、控制失效等故障时,系统仍能保持车辆的安全条件或保证其进入到安全状态。 1.感知错误1）传感器损坏导致丧失某一方向视野，比如前向摄像头损坏。策略就是冗余配置传感器，在某一方向上要有一套以上的传感器系统去覆盖视野。 2）实际道路中，由于目标种类繁多，利用摄像头识别可能捕捉不到全部的目标，目标缺失。有时由于光照原因不能有效获取，比如强光、弱光条件下摄像头会失效。策略是利用多源传感器进行融合，比如激光雷达在雨雪天气受到影响就需要毫米波雷达作为辅助。 2.决策失误当本车对周围车辆的轨迹、速度预测，决定本车驾驶策略时，可能会产生误判或者预测出错的情况。那么在整个安全驾驶策略里面必须有一个安全的底线，即保守的驾驶策略。这个在软件架构里面必须要考虑的。 3.控制失效当场景内障碍物较多，过于复杂，系统难以应对，即控制策略失效时。就需要人类驾驶员接管车辆，实现人机共驾。 4.总结汽车的快速行驶、低排放、保证安全性等，这些都是汽车要求的基本功能。在自动驾驶车辆诞生之前，都是由驾驶员应对系统失效、各种传感器失效。现在由主控制器代替人类驾驶员，应对以上问题功能安全必须要考虑。 五、汽车信息安全1.概念汽车信息安全指由于汽车在智能化、网联化、电动化中的技术变革带来的信息安全问题，具体表现为 1）智能化： 程序复杂:至少100台车载电脑,适行6000万行代码,无人驾驶2亿行以上的代码。 智能控制:采用感知一决策一控制来代替人对机械部分直接控制。信息安全不仅损失经济,可能会成为社会安全问题。 2）网联化： 固有缺陷:使用的计算和联网系统沿袭了既有的计算和联网架构,也继承了这些系统天然的安全缺陷。 数据联通:车载信息系统与外界互联互通,共享信息指数增加,涉及用户隐私安全。 3）电动化： 充电桩:控制模块的PLC电路通过以太网与管理系统连接,在整个网络内部缺少防护。 电池管理系统:通过攻击BMS的控制算法从而影响电动车的电池性能。 2.事件1）2015年宝马数字服务系统漏洞 2）2016年特斯拉远程入侵事件 3）2016年尼桑LEAF汽车API泄露事件 4）2015年JEEP自由光入侵事件 3.风险的来源两张网和一个端： 具体包括： 4.各国的应对之策1）总体趋势：整个世界范围内从入侵检测这样的研究转向正向的智能汽车信息安全架构设计 2）国外： 3）国内（从国家各个层面都非常关注）： 尤其是中国汽车工程协会（CAICV）作出了巨大贡献。 2016年制定了智能网联汽车的信息安全的路线图。 在智能网联汽车信息安全的关键技术研发方面提出了“端-网-云”一体化的入侵检测系统。 5.信息安全立体防护体系对于智能网联汽车现在所要构建的是包括这样的一种立体防护体系在内的， 然后在具体的车辆信息安全防护平台方面，主要是构建四个体系来推动。 标准体系：汽车内部的标准其实和互联网的标准发展相比，和通信的标准发展相比相对而言比较滞后的 密码体系：自主可控的密码体系的推动 架构体系：纵深防御的架构体系的设计 IDS（入侵检测系统）技术 6.产品博世： 中央网关（Cross-Domain Communication）：实现通信功能，在线升级以及对信息安全的防护，实现类似于防火墙的功能； 入侵检测防护系统（IDPS）：将驾驶行为的数据能够在线收集起来传递到云平台或云平台分析究竟哪些是入侵行为，哪些是它的一个正常的数据行为，从而能够对于入侵行为提出一些的解决方案。通过OTA，能够对于车载的信息系统提供一个保护和防护。 六、总结]]></content>
      <categories>
        <category>智能网联汽车</category>
      </categories>
      <tags>
        <tag>功能安全</tag>
        <tag>信息安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智能网联汽车——车联网]]></title>
    <url>%2F2019%2F12%2F25%2F%E6%99%BA%E8%83%BD%E7%BD%91%E8%81%94%E6%B1%BD%E8%BD%A6%E2%80%94%E2%80%94%E8%BD%A6%E8%81%94%E7%BD%91%2F</url>
    <content type="text"><![CDATA[一、什么是车联网1.研究背景交通安全:全球每年约有124万人死于交通事故,每年交通事故造成的经济损失高达5180亿美元（引自WHO2013年道路安全全球现状报告)； 交通效率:交通拥堵让北京市年损失占GDP的7.5%,堵塞严重影响人们出行,如2010.9.17共145条道路塞车(北京市交通发展研究中心《2011北京市交通发展年度报告》的评估报告）； 2.研究意义自主式感知不仅需要留意周边的其他的车辆，还必须完全依赖自身能力检测到周围的道路、车道等一系列的因素。而且，它受天气影响比较大，会影响它的探测的距离与精度，并且没有办法对周围的障碍物进行预测。 车联网主要是解决通信问题，具体来说，通过车辆间通信的V2X系统，在车辆之间进行一个实时高效的通信，可以有效地解决传统的激光、雷达、摄像头分析等存在的距离、角度的缺陷，全方位的提升汽车主动安全系统的感知范围和感知的程度。 3.概念车联网(汽车移动互联网)是利用先进传感技术、网络技术、计算技术、控制技术、智能技术,对道路交通进行全面感知，对每部汽车进行交通全程控制,对每条道路进行交通全时空控制,实现道路交通“零堵塞”、“零伤亡”和“极限通行能力“的专门控制网络。 4.车联网中的通信车联网是以车内网、车际网和车载移动互联网为基础,按照约定的通信协议和数据交换标准,在车和车、车和人、车和环境之间进行无线通信及信息传输,实现车辆智能化控制,智能动态服务,智能交通管理的一体化网络。 车内网是指通过应用成熟的总线技术建立一个标准化的整车网络。 车云(车载移动互联)网是指车载终端通过3G/4G/5G等通信技术与互联网进行无线连接。 车际网是指基于DSRC技术和IEEE802.11系列无线局域网协议组建的动态网络。 二、车联网的应用场景1.行车安全类1）行人安全保证： 通过路侧设备检测并跟踪行人 路侧设备将行人状态告诉周围的车辆 车辆根据速度、位置判断是否会相撞 若事故不可避免,自动紧急刹车 2）碰撞预警： 实时共享前方车辆或障碍物的速度和位置 车载系统计算两车的行驶轨迹,判断是否会发生碰撞 当碰撞不可避免时,启动紧急刹车 2.交通效率类 3.信息娱乐类1）动态导航 驾驶员输入目的地 RSU将车辆信息传给控制中心 控制中心根据交通状况判断最佳路径 将各道路车流状况最佳路径和地图传回车内 三、车联网的系统架构1.整体系统 1）中心子系统 2）道路子系统 3）车辆子系统 4）行人子系统 人所携带的手持智能终端设备 公交车站的电子显示屏 2.信息流动 3.部署案例 车辆A可以检测到个有危险的行驶情况 它向最近的路边单元RSU1发送了一个报警消息 该消息被发送到控制单元(同时,报警消息被车辆B直接接收到,并传播到车辆C) 中央控制单元,在确认了信息后发送报警消息到路边单RSU2,RSU2可以将该消息传播给车辆D、E、F 四、车联网通信技术1.车际网专用中短距离通信技术,实现车车/车路协同,包括DSRC、LTE-V,5G​ 优点：时延极短,可靠性高,需要支撑主动安全应用 1.1DSRC信道分配： 中心频段是在5.9GHz，总共有75MHz的频带宽度，前面的5MHz被预留，后面的70MHz被平均分成7个信道，有一个控制信道，其他六个都是服务信道。172和184是用于公共安全专用信道，比如说和生命财产相关的应用就在这两个信道当中使用。 通信协议栈: 1.2ETC（近距离车路通讯技术）它是通过安装在车辆上的车载装置和安装在收费站车道上的天线之间进行无线通信和信息交换。 组成： 车辆自动识别系统：包括用于车际网的车载单元OBU（存在车辆的识别信息，一般安装于车辆前面的挡风玻璃上）、路边单元RSU（安装于收费站旁边）、环路感应器（安装于车道地面下）等组成 中心管理系统 其他辅助设施 ETC与DSRC的区别（横线处）： ETC与DSRC的关系： 专用短距离通信技术(DSRC)在美囯被用于和WAVE协议相关的无线电频谱或技术。美国机动车工程师学会(SAE)已经明确提出WAVE协议要使用5.9GHz的频带。 在美国以外,DSRC可能指的是一个使用5.8GHz频带的不同的无线电技术,例如电子收费( Electronic Fee CollectionEFC)。 ETC可以作为一个单独的系统独立运行,也可以通过实现了包括IEEE160911标准的DSRC系统来提供支付服务。 1.3LTE-VLTE-V类似于基站发射4G信号,比前面那种靠WiFi的更稳定,让车与车之间的沟通更便利。被认为是实现车联网的重要基石，基于4.5G网络以LTE蜂窝网络作为V2X的基础，面向未来5G的重点研究方向。也是车联网的专有协议，面向车联网应用场景，实现车与车、车与路测设施、车与人、车与网络的互联和数据传输，也就是V2X。 为了应对车辆主动安全、行车效率、车载娱乐等多场景不同的需求，LTE-V采用的是广域蜂窝式和短程直通式的通信，前者是基于现有蜂窝技术的扩展，主要承载着传统的车联网业务，后者引入LTE—D2D，由于LTE-D具备了能够寻找500公尺以内数以千计设备的能力，因此能让两个以上最接近LTE-D设备在网内通信。 广域蜂窝式通信： 短程直通式通信： LTE-V与DSRC的区别 1.4智能交通通信技术演进 2.车云网 概述：车云网目前主要提供 Telematics服务, Telematics是指通过车载计算机系统、无线通信技术、卫星导航装置、互联网信息技术向车主提供驾驶所需的包括汽车安防、车载通信、导航定位、交通信息、新闻资讯、娱乐应用等功能的综合信息服务系统。 优缺点：覆盖范围广,能够与 Internet连接,时延较大,不适合紧急安全应用 实例： 通用的安吉星 上汽的inkaNet 3.车内网驾驶辅助系统及娱乐信息系统等功能大幅增加,需要高带宽、高实时性、高可靠性的车载通信网络；车载以太网由于具有高数据带宽和高通信速率,非常适合汽车ADAS系统以及车载信息娱乐系统的应用。 目前已在全球范围内形成了openAlliance、AVnu等联盟以推进汽车以太网标准的制定工作。 分类： 车与内部传感器的有线连接, 如CAN BUS、高速以太网 车机与手机等设备的无线连接,包括蓝牙、WiFi、NFC 五、总结 给出本文的思维导图，回忆一下~]]></content>
      <categories>
        <category>智能网联汽车</category>
      </categories>
      <tags>
        <tag>车际网</tag>
        <tag>车云网</tag>
        <tag>车内网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智能网联汽车——深度学习与无人驾驶（二）]]></title>
    <url>%2F2019%2F12%2F24%2F%E6%99%BA%E8%83%BD%E7%BD%91%E8%81%94%E6%B1%BD%E8%BD%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、卷积神经网络卷积神经网络，也叫做convert，它是计算机视觉中图像分类问题几乎都在使用的一种深度学习模型。首先给出一张用于图像分类问题的一种卷积神经网络架构的图像。 1.输入层对于包含两个空间轴(高度和宽度)和一个深度轴(也叫通道轴)的3D张量,其卷积也叫做特征图( feature map)。对于RGB图像,深度轴的维度大小等于3,因为图像有3个颜色通道：红色、绿色和蓝色。对于黑白图像(比如 MNIST数字图像),深度等于1(表示灰度等级)。 从输入层开始,卷积神经网络通过不同的神经网络结构将上一层的三维矩阵转化为下一层的三维矩阵,直到最后的全连接层。 2.卷积层1）全连接层和卷积层的根本区别在于，全连接层从输入特征空间中学习到的是全局模式（涉及所有像素的模式），而卷积层学到的是局部模式，对于图像来说，学到的就是输入图像的二维小窗口中的边缘、纹理等。 2）这个重要特性使卷积神经网络具有以下两个有趣的性质。 卷积神经网络学到的模式具有平移不变性。卷积神经网络在图像右下角学到某个模式之后,它可以在任何地方识别这个模式,比如左上角。对于密集连接网络来说,如果模式出现在新的位置,它只能重新学习这个模式。这使得卷积神经网络在处理图像时可以高效利用数据。它只需要更少的训练样本就可以学到具有泛化能力的数据表示。 卷积神经网络可以学到模式的空间层次结构。第一个卷积层将学习较小的局部模式(比如边缘),第二个卷积层将学习由第一层特征组成的更大的模式,以此类推。这使得卷积神经网络可以有效地学习越来越杂、越抽象的视觉概念。 视觉世界形成了视觉模块的空间层次结构：超局部的边缘组合成局部的对象，比如眼睛或者耳朵，这些局部对象又组合成高级概念，比如“猫”。 3）卷积运算从输入层中提取图块,并对所有这些图块应用相同的变换,生成输出特征图。该输出特征图仍是一个3D张量,具有宽度和高度,其深度一般为过滤器的数量。比如在MNIST示例中，第一个卷积层接收一个大小为（28,28,1）的特征图，经过第一层的卷积运算后，输出一个大小为（26,26,32）的特征图，即它在输入上计算32个过滤器。 4）注意： 卷积由以下两个关键参数所定义。 从输入特征图中提取的图块尺寸：这些图块的大小通常是3x3或5x5，比如MNIST示例使用的是3x3。 输出特征图的深度：卷积所计算的过滤器的数量。比如第一层的深度为32。 输出的宽度和高度可能与输入的宽度和高度不同。不同的原因有两点。 边界效应，可以通过对输入特征图进行填充来抵消。（参数padding=valid表示不使用填充，参数padding=same表示使用填充，即填充后输出的宽度和高度与输入相同。默认值是不使用填充） 使用了步幅（stride）。 综上，经过一层卷积层计算的输入输出公式为 使用填充： 不使用填充（向上取整）： 且卷机层输出特征图深度与该层参与运算的过滤器的数量一致。 3.池化层1）池化层神经网络不会改变三维矩阵的深度,但是它可以缩小矩阵的大小。池化操作可以认为是将一张分辨率较高的图片转化为分辨率较低的图片。通过池化层,可以进一步缩小最后全连接层中节点的个数,从而达到减少整个神经网络中参数的目的。使用池化层既可以加快计算速度也有防止过拟合问题的作用。 2）使用最大值操作的池化层被称之为最大池化层,这是被使用得最多的池化层结构。使用平均值操作的池化层被称之为平均池化层。 3）最大池化通常使用2x2的窗口和步幅2，其目的是将特征图下采样2倍。如在第一个池化层之前，特征图的尺寸是26x26，最大池化运算将其减半为13x13。 4.全连接层在经过多轮卷积层和池化层的处理之后,在卷积神经网络的最后一般会是由1到2个全连接层来给出最后的分类结果。经过几轮卷积层和池化层的处理之后,可以认为图像中的信息已经被抽象成了信息含量更高的特征。我们可以将卷积层和池化层看成自动图像特征提取的过程。在特征提取完成之后,仍然需要使用全连接层来完成分类任务。 5.Softmax层Softmax层主要用于分类问题。通过 Softmax层,可以得到当前样例属于不同种类的概率分布情况。然后取最大概率所属的种类作为最后的结果。 二、深度学习网络架构模型针对深度学习在计算机视觉领域的应用，涌现出了许多杰出的网络架构模型。顾名思义，计算机视觉即通过创建人工模型来模拟本由人类执行的视觉任务。 计算机视觉任务的主要类型如下： 图像分类：在图像分类中，给出一张原始图像，你的任务是识别出该图像属于哪个类别。 目标检测：在目标检测中，你的任务是找到图像中一个或多个物体的各自位置。这些物体可能属于同一类别，或者各自不同。 图像分割：图像分割是一个稍微复杂的任务，其目标是将每一个像素映射到正确的分类。 1.图像分类1. 1AlexNet在2012年夺得了ImageNet图像分类的冠军，而且远远超过当年的第二名。架构有5个卷积层和3个最大池化层，使用了ReLU激活函数、dropout（随机失活）层、overlap pooling（重叠池化）、LRN(Local Responce Normalization，局部响应归一化)、数据增强及CUDA加速等技术。 1.2VGG NetILSVRC2014的亚军，结构非常简洁，反复堆叠3x3的小型卷积核和2x2的最大池化层构建。相比于 AlexNet 有更小的卷积核和更深的层级，但是参数数量太多。 1.3GoogleNet2014年ImageNet竞赛的冠军。该架构中，随着深度增加（它包含 22 层，而 VGG 只有 19 层），研究者还开发了一种叫作「Inception 模块」的新型方法。 最终架构包括堆叠在一起的多个 inception 模块。GoogleNet 的训练过程也有稍许不同，即最上层有自己的输出层。这一细微差别帮助模型更快地进行卷积，因为模型内存在联合训练和层本身的并行训练。另一方面，Inception 网络是复杂的（需要大量工程工作）。它使用大量 trick 来提升性能，包括速度和准确率两方面。 1.4ResNetILSVRC 2015年的冠军。ResNet网络主要是为了解决网络加深后梯度消失的问题，提升训练过程中的优化能力。残差网络（ResNet）包含多个后续残差模块，是建立 ResNet 架构的基础。下图是残差模块的表示图。 类似于 GoogleNet，这些残差模块一个接一个地堆叠，组成了完整的端到端网络。深度残差网络,在网络深度上不断加深，但其网络较瘦，控制了参数数量，存在明显层级，特征图个数逐层递进，保证输出特征表达能力，没有使用Dropout，利用Batch Normalize和全局平均池化进行正则化，加快了训练速度。ResNet有多个模型，常用的有ResNet-50， ResNet-101，ResNet-152等。 1.5ResNeXtResNeXt 据说是解决目标识别问题的最先进技术。它建立在 Inception 和 ResNet 的概念上，并带来改进的新架构。在《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning》一文中，研究者通过实验明确地证实了，结合残差连接可以显著加速 Inception 的训练。 2.目标检测3.图像分割3.1分类1）普通分割 将不同分属不同物体的像素区域分开。如前景与后景分割开，狗的区域与猫的区域与背景分割开。 2）语义分割 在普通分割的基础上，分类出每一块区域的语义（即这块区域是什么物体）。如把画面中的所有物体都指出它们各自的类别。 3）实例分割 在语义分割的基础上，给每个物体编号。 如这个是该画面中的狗A，那个是画面中的狗B。 3.2族谱1）FCN2）DeepLab3）SegNet4）PSPNet5）Mask-RCNN关于各模型的具体介绍和图像分割更详细的内容可以参照这篇博客。（https://blog.csdn.net/weixin_41923961/article/details/80946586） 三、深度学习与无人驾驶1.基于深度学习的环境感知（目标检测）1.1.传统的检测算法多是基于特征的目标检测算法1）基于形状和颜色进行目标检测； 2）利用梯度直方图特征和支持向量机(Support Vector Machine，SVM)算法进行结合； 3）基于不同的阈值分割灰度图，使用搜索连通区域将最大稳定极值区域作为候选区域等等。 这些算法都有一个共性:利用人们设计好的特征，提取并用于检测。人为地提取特征存在着一定的主观性，会受限于一些复杂的道路情况，且受限于算法本身。 1.2近些年基于深度学习的目标检测算法由于无需进行人工的特征设计，良好的特征表达能力以及优良的检测精度。目前，基于深度学习的目标检测算法已经超越传统检测方法，成为当前目标检测算法的主流。依据其设计思想，主要可分为两种，即基于区域提名的目标检测算法（二阶检测算法）和基于端到端学习的目标检测算法（一阶检测算法）。 二阶段检测算法：第一步：生成可能包含物体的候选区域（Region Proposal），第二步：对候选区域做进一步的分类和定位，得到最终的检测结果。代表是：R-CNN，Fast R-CNN, Faster R-CNN。 一阶段检测算法：直接给出最终结果，没有生成候选区域的步骤。代表是：Yolo，SSD。 2）R-CNN：目标检测里程碑之作，利用selective search算法从待检测图像中提取2000个左右的候选框，用CNN提取每个候选框的特征，得到固定长度的特征向量并送入SVM中进行分类得到类别信息，送入全连接网络进行回归得到对应位置的坐标信息。 3）Fast-RCNN：在R-CNN的基础上，采用自适应尺度池化对整个网络进行优化，从而规避了R-CNN中冗余的特征提取操作，提高了网络识别的准确率。此外,使用感兴趣区域池化层,用以提取特征层上各个候选框的固定维度的特征表示;同时，使用 SoftMax 非线性分类器,以多任务学习的方式同时进行分类和回归。由于Fast R-CNN 无需存储训练和测试过程产生的中间值,因此其速度相较于 R-CNN 大为提升。 4）Faster R-CNN：在主干网络增加了RPN网络，通过一定的规则设置不同尺度的锚点在RPN的卷积特征层提取候选框来代替Selective Search等传统的候选框生成方法，实现了网络的端到端训练。 ​ 5）YOLO（检测20个种类）：简化了目标检测的整个流程,视频帧图像被缩放至统一尺度大小的图像,分为 S×S 个格子,每个格子需要预测 B个包含物体的矩形框的信息和 C 个类别的归属概率值,每个矩形框包含 4 维坐标信息和 1 维目标置信度,则每个格子输出 5×B+C 维向量。 YOLO 整合了目标判定和识别,运行速度有了极大的提高。 结构：20层卷积层之上加上随机初始化的4个卷积层和2个全连接层。 优势：检测速度快、背景误检率比 R-CNN 等要低、支持对非自然图像的检测。 缺点:对于密集型目标检测和小物体检测都不能很好适用。 6）YOLO-v2:可以在速度和准确率上进行tradeoff(折中)：在67帧率下，v2在VOC2007数据集的mAP可以达到76.8; 在40帧率下，mAP可以达到78.6。 加入当下热门的Batch Normalize（BN层的添加直接将mAP硬拔了2个百分点，这一操作在yolo_v3上依然有所保留，BN层从v2开始便成了yolo算法的标配。）以及残差网络结构外； 还针对性的训练了一个高分辨率的分类网络（把这种更高分辨率的分类网络（448x448）用到detection上，发现map提升了4% ）。 7）YOLO-v3（检测240个种类）:采用多尺度预测（13x13x255,26x26x255,52x52x255）及更好的backbone网络(darknet-53,可以和resnet-152正面刚)，分类损失采用binary cross-entropy损失函数替换softmax损失函数。 8）SSD：由于 YOLO 网络的 S×S 网格的粗糙划分导致了回归的目标位置误差较大,SSD 借鉴了区域提名的思想作出改进,使用与 Faster R-CNN 类似的 RPN 网络,不同的是 SSD 在 CNN 的多个特征层上使用 RPN 之后再作分类和边框回归,原图上小物体的检测也能有较准确的检测结果。与 YOLO 相比,SSD 仍能保持快速的检测速度,并且改进了小物体的定位精确度。 2.基于深度学习的无人驾驶决策控制以车载传感器（摄像头和激光雷达）为输入，以驾驶员操作（油门、刹车、方向盘）为输出,在深度神经网络模型的训练下，可学习优秀驾驶员的驾驶模式，将决策过程视作一个不可分解的黑箱,主要的问题是决策结果不具有逻辑可解释性，并且预覆盖所有场景困难，数据需求量大。 四、总结 给出本文的思维导图，回忆一下~]]></content>
      <categories>
        <category>智能网联汽车</category>
      </categories>
      <tags>
        <tag>卷积神经网络</tag>
        <tag>图像分类</tag>
        <tag>目标检测</tag>
        <tag>图像分割</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智能网联汽车——深度学习与无人驾驶（一）]]></title>
    <url>%2F2019%2F12%2F23%2F%E6%99%BA%E8%83%BD%E7%BD%91%E8%81%94%E6%B1%BD%E8%BD%A6%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前面留下关于智能网联汽车环境感知部分基于深度学习的目标检测的坑还没填，接下来两篇推送就来梳理一下基于深度学习的目标检测的具体实现，顺便再介绍下深度学习的发展历程。当然，这一切得先从人工智能开始谈起。 一、人工智能1.定义1）人工智能是研究模拟和扩展人类智能的理论、方法、技术及应用系统的一门交叉学科（学术角度）。 2）人工智能是智能机器执行的、与人类智能有关的智能行为,如感知、识别、判断、理解、设计、思考、学习和问题求解等思维活动（应用角度）。 3）近期目标：研究如问使系统能够胜任一些通常需要人类智能才能完成的复杂工作。 4）最终目标：探讨智能形成的基本机理,研究利用智能机器模拟人的思维过程。 2.研究领域机器学习是在人工智能的基础上发展起来，而深度学习又是在机器学习的基础上发展起来的。 3.分类1）弱人工智能：也称“专用人工智能”或限定领域人工智能”,是指专注于某个特定领域,只能解决单一问题的人工智能,例如语音识别、图像识别、翻译等。 2）强人工智能：也称“通用人工智能”,是指能胜任人类所有工作的人工智能,有知觉,有自我意识,既可下棋、又可驾驶、考试、翻译、玩游戏等。 3）超人工智能：几乎所有领域都比最聪明的人类大脑聪明很多,包括科学创新、通识和社交技能。达到这个阶段的人工智能的计算和思维能力远超人脑。 4.发展历程 在早期，许多专家认为只要程序员精心编写足够多的明确规则来处理知识，就可以实现与人类相当的人工智能。这一方法被称为符号主义人工智能，在20世纪80年代的专家系统热潮中，这一方法的热度达到了顶峰。 二、机器学习虽然符号主义人工智能适合用来解决定义明确的逻辑问题，比如下国际象棋，但它难以给出明确的规则来解决更加复杂、模糊的问题，比如图像分类、语音识别和语言翻译。于是出现了一种新的方法来代替符号主义人工智能，这就是机器学习。 虽然机器学习在20世纪90年代才开始蓬勃发展，但它迅速成为人工智能最受欢迎且最成功的分支领域。这一发展的驱动力来自于速度更快的硬件与更大的数据集。 1.定义计算机程序从经验E（程序与自己下几万次跳棋）中学习，解决某一任务T（下跳棋），进行某一性能度量P（与新对手玩跳棋时赢得概率），通过P测定在T上的表现因经验E而提高。 2.三个分支2.1监督学习监督学习是目前最常见的机器学习类型。给定一组样本（通常由人工标注），它可以学会将输入数据映射到已知目标。比如手写体数字识别、分类问题、回归问题。一般来说，深度学习应用几乎都属于监督学习，比如光学字符识别、语音识别、图像分类和语言翻译。 虽然监督学习主要包括分类和回归，但还有其他主要包括： 序列生成。给定一张图像，预测描述图像的文字。序列有时生成有时可以被重新表示为一系列分类问题，比如反复预测序列中的单词或标记。 语法树预测。给定一个句子，预测其分解生成的语法树。 目标检测。给定一张图像，在图中特定目标的周围画一个边界框。这个问题也可以表示为分类问题。（给定多个候选边界框，对每个框内的目标进行分类）或分类与回归联合问题（用向量回归来预测边界框的坐标）。 无人驾驶环境感知可以根据这一原理来检测车辆、行人、交通标志等目标。 图像分割。给定一张图像，在特定物体上画一个像素级的掩模。 无人驾驶环境感知可以根据这一原理来检测车辆、行人、交通标志等目标。 注：分类包括二分类（比如电影评论的分类，经过训练的模型可以对一条新的评论给出是好评还是差评的预测。）和多分类（比如手写数字识别，经过训练的模型可以对一张新的手写体数字图像给出0-9之一的数字的预测。） 回归问题：比如现有某一处房价受面积、房间数和地理位置、年限等因素影响，待预测参数是房屋售价。根据现有的数据可以训练模型并预测出该处某个影响因素变化时对应房屋价格的具体变化值。 2.2无监督学习无监督序学习是指在没有目标的情况下寻找输入数据的有趣变换，其目的在于数据可视化、数据压缩、数据去躁或更好得理解数据中的相关性。无监督学习是数据分析的必备技能，在解决监督学习问题之前，为了更好地了解数据集，它通常是一个必要步骤。降维和聚类都是众所周知的无监督学习方法。 2.3增强学习在强化学习中，智能体接收有关其环境的信息，并学会选择使某种奖励最大化的行动。例如，神经网络会“观察”视频游戏的屏幕并输出游戏操作，目的是尽可能得高分，这种神经网络可以通过强化学习来训练。 3.从数据中学习表示（机器学习原理）机器学习，特殊在学习。经典的程序设计往往是给定规则和数据得出预期的结果（比如给定变量a=1和变量b=1以及规则f（x）= a+b就可以得出预期的结果为2），而机器学习的编程范式则是给定数据和预期的结果，通过学习习得表示。 通常需要以下三个要素来进行机器学习。 1）输入数据点。例如你的任务是为图像添加标签（这张图像表示一只猫），那么这些数据点就是图像以及对应的标签（猫）。（今天新学了两节数学课） 2）预期输出的示例。对于图像标记任务来说，预期输出结果可能就是“猫”、“狗”之类的标签。（自己做数学作业的答案） 3）衡量算法好坏的方法。这一衡量方法为计算算法的正确输出与预期输出的差距。当然我们的目的是为了最小化差距。对于图像标记任务来说，机器学习模型预测输出的结果（标签）与正确结果（标签）一致所占总数的比例越高越好。（自己的答案与标准答案相比较） 4.评估机器学习模型机器学习的目的是得到可以泛化的模型，即在前所未见的数据上表现很好的模型。讨论机器学习模型学习和泛化的好坏时,通常使用术语:过拟合和欠拟合。（平时通过上课学习并做作业，目的是看考试结果如何） 过拟合是指在训练数据上表现良好,在未知数据上表现差。——高考复习太死,改一个数就不会做。 欠拟合在训练数据和未知数据上表现都很差。——高考复习不充分,啥也不会。 5.数据预处理、特征工程除模型评估外，我们还必须解决另一个重要问题：将数据输入神经网络之前，如何准备输入数据和目标？（必要的学习方法，听课更有效率、做作业效果更好） 5.1神经网络数据预处理1）向量化 神经网络的所有输入和目标都必须是浮点数张量。无论处理什么数据（声音、图像还是文本），都必须先将其转换为张量，这一步叫做数据向量化。 可以通过one-hot编码实现。 2）值标准化 一般来说，将取值相对较大的数据（比如多位整数，比网络权重的初始值大的多）或异质数据（比如数据的一个特征在0-1范围内，另一个特征在100-200范围内）。输入到神经网络中是不安全的。这么做可能导致较大的梯度更新，进而导致网络无法收敛。 可以通过对每个特征分别做标准化，使其均值为0、标准差为1。 5.2特征工程特征工程是指将数据输入模型之前，利用你自己关于数据和机器学习算法的知识对数据进行硬编码的变换，以改善模型的效果。多数情况下，一个机器学习模型无法从完全任意的数据中进行学习。呈现给模型的数据应该便于模型进行学习。比如如何识别钟表时间？（每个学生对老师教授的内容理解程度不同，就会有不同的学习效果） 通过像素的分布。 通过时钟指针的坐标。 通过时钟指针的角度。 很明显第三种特征使问题变得非常简单，根本不需要机器学习，因为简单的舍入运算和字典查找就可以给出大致的时间。 这就是特征工程的本质：用更简单的方式表述问题，从而使问题变得更容易。它通常需要深入理解问题。 深度学习出现之前,特征工程曾经非常重要,因为经典的浅层算法没有足够大的假设空间来自己学习有用的表示。将数据呈现给算法的方式对解决问题至关重要。例如,卷积神经在MNIST数字分类问题上取得成功之前,其解决方法通常是基于硬编码的特征,比如数字图像中的圆圈个数、图像中每个数字的高度、像素值的直方图等。 幸运的是,对于现代深度学习,大部分特征工程都是不需要的,因为神经网络能够从原数据中自动提取有用的特征。这是否意味着,只要使用深度神经网络,就无须担心特征工程呢？并不是这样,原因有两点。 良好的特征仍然可以让你用更少的资源更优雅地解决问题。例如,使用卷积神经网络来读取钟面上的时间是非常可笑的。 良好的特征可以让你用更少的数据解决问题。深度学习模型自主学习特征的能力依赖于大量的训练数据。如果只有很少的样本,那么特征的信息价值就变得非常重要。 6.已经取得的进展 浏览器网页排序 过滤垃圾邮件 Facebook 或苹果的图片分类程序能认出你朋友的照片 浏览器开发者收集 web 上的单击数据，也称为点击流数据，并尝试使用机器学习算法来分析数据，更好的了解用户，并为用户提供更好的服务 电子医疗记录。我们可以把医疗记录变成医学知识，我们就可以更好地理解疾病 计算生物学。生物学家们收集的大量基因数据序列、DNA 序列等，机器运行算法让我们更好地了解人类基因组 自然语言处理 计算机视觉 每次你去亚马逊或 Netflix 或 iTunes Genius，它都会给出其他电影或产品或音乐的建议 7.局限性有限样本和计算单元情况下对复杂函数的表示能力有限,针对复杂分类问题其泛化能力受限。 三、深度学习1.概述深度学习是机器学习的一个分支领域，它是从数据中学习的一种新方法，强调从连续的层中进行学习，这些层对应于越来越有意义的表示。“深度学习”中的“深度“指的并不是利用这种方法所获取的更深层次的理解，而是指一系列连续的表示层。数据模型中包含多少层，这被称为模型的深度。现代深度学习通常包含数十个甚至上百个连续的表示层，这些表示层全都是从训练数据中自动学习的。与此相反，其他机器学习的重点往往是仅仅学习一两层的数据表示，故有时也被称为浅层学习。 在深度学习中，这些不同层所学习到的表示总是通过叫做神经网络的模型来学习得到的。上图所示的深度神经网络将数字图像转换成与原始图像差异越来越大的表示 ，而其中关于最终结果的信息却越来越丰富。 2.工作原理2.1神经网络由其权重来参数化神经网络中每层对输入数据所做的具体操作保存在该层的权重中，其本质是一串数字。用术语来说，每层实现的变换由其权重来参数化。权重有时也被称为该层的参数。在这种语境下，学习的意思是为神经网络的所有层找到一组权重值，使得该网络能够将每个示例输入与其目标正确的一一对应。（通过做查看例题并去尝试做一些课后习题） 2.2损失函数用来衡量网络输出结果的质量想要控制一件事物，首先需要能够观察它。想要控制神经网络的输出，就需要能够衡量该输出与预期值之间的距离。这是神经网络损失函数的任务，该函数也叫目标函数。损失函数的输入是网络预测值与真实目标值，然后计算一个距离值，衡量该网络在这个示例上的效果好坏。（将自己课后习题的答案与标准答案作比较，得出准确率） 2.3将损失值作为反馈信号来调节权重利用这个距离作为反馈信号对权重值进行微调，以降低当前示例对应的损失值。这种调节由优化器来完成，它实现了所谓的反向传播算法，这是深度学习的核心算法。（通过多做题，多请教别人等方法是自己的正确率越来越高） 一开始对神经网络的权重随机赋值，因此网络知识实现了一系列随机变换。其输出结果自然也和理想值相去甚远，相应地，损失值也很高。但随着网络处理的示例越来越多，权重值也在向正确的方向逐步微调，损失值也逐渐降低。这就是训练循环，将这种循环重复足够多的次数，得到的权重值可以使损失函数最小。具有最小损失的函数，其输出值与目标值尽可能的接近，这就是训练好的网络。也就是完成了所谓深度学习的过程。（最终达到能应付考试的能力） 3.特点（与传统机器学习相比）：3.1通过渐进的、逐层的方式形成越来越复杂的表示先前的机器学习技术仅包含将输入数据变换到一两个连续的表示空间，通常使用简单的变换，比如SVM和或决策树。但这些技术无法得到复杂问题所需要的精确表示。因此，人们必须竭尽全力让初始数据更适合用这些方法来处理，也必须手动为数据设计好的表示层。这叫做特征工程。与此相反，深度学习完全将这个步骤自动化：利用深度学习，你可以一次性学习所有特征，而无需自己动手设计。这将原本复杂的多阶段流程替换为一个简单的、端到端的深度学习模型。 3.2对中间这些渐进的表示共同进行学习在实践中，如果想要通过重复连续应用浅层学习方法来达到多个连续表示层的效果，其收益会随着层数增加迅速降低。因为三层模型中最优的第一表示层并不是单层或双层模型中最优的第一表示层。深度学习的变革在于，模型可以在同一时间共同学习所有表示层，而不是依次连续学习。通过共同的特征学习，一旦模型修改某个内部特征，所有依赖于该特征的其他特征值都会相应地自动调节适应，无须人为干扰。 4.兴起的原因在带来第三次人工智能热潮，深度学习功不可没。总的来说，有这么几方面。 4.1硬件在21世纪前十年里，NVIDIA和AMD等公司投资数十亿美元来开发快速的大规模并行芯片（GPU），以便为越来越逼真的视频游戏提供图形显示支持。2007年，NVIDIA推出了CUDA，作为其GPU系列的编程接口。今天，NVIDIA TITAN X可以实现每秒进行6.6万亿次float32运算。这比一台现代笔记本电脑的速度快约350倍。 4.2数据就数据而言，除了过去20年里存储硬件的指数级增长，最大的变革来自于互联网的兴起，它使得收集和分发用于机器学习的超大型数据集变得可行。 4.3算法到21世纪前十年的末期，神经网络仍然很浅，仅使用一两个表示层，无法超越更为精确的浅层方法，比如SVM和随机森林。关键问题在于通过多层叠加的梯度传播。随着层数的增加，用于训练神经网络的反馈信号会消失。 这一情况在2009-2010年左右发生了变化，当时出现了几个很简单但很重要的算法改进，可以实现更好的梯度传播。 更好的神经层激活函数。 更好的权重初始化方案，一开始使用逐层预训练的方法，不过这种方案很快就被放弃了。 更好的优化方案，比如RMSProp和Adam。 4.4其他1）在早期，从事深度学习需要精通C++和CUDA，而他们只有少数人才能掌握。如今，具有基本的Python脚本技能，就可以从事高级的深度学习研究。这主要得益于TensorFlow的开发，以及Keras等用户友好型库的兴起。TensorFlow是符号式的张量运算的Python框架，支持自动求微分，这极大地简化了新模型的实现过程。Keras等用户友好型库则使深度学习变得像操纵乐高积木一样简单。 2）在Kaggle上，有一系列不断更新的数据科学竞赛，其中许多都涉及深度学习。公开竞赛是激励研究人员和工程师挑战极限的极好方法。研究人员通过通过竞争来挑战共同基准，这极大地促进了深度学习的兴起。 5.关建网络架构每种类型的网络都针对于特定的输入模式，某种架构能否解决某个问题，这完全取决于数据结构与网络架构的假设之间的匹配度。（比如有的知识点需要理解，有的知识点需要熟记，有的知识点了解就行） 向量数据：全连接层 图像数据：二维卷积神经网络 声音数据（比如波形）：一维卷积神经网络（首选）或循环神经网络 文本数据：一维卷积神经网络（首选）或循环神经网络 时间序列数据：一维卷积神经网络或循环神经网络（首选） 其他类型的序列数据：循环神经网络或一维卷积神经网络。如果数据顺序非常重要（比如时间序列，但文本不是），那么首选循环神经网络。 视频数据：三维卷积神经网络（如果你需要捕捉运动物体），或者帧级的二维神经网络（用于特征提取）+循环神经网络或一维卷积神经网络（用于处理得到的序列）。 立体数据：三维卷积神经网络 5.1密集连接网络密集连接网络是Dense层的堆叠，它用于处理向量数据（向量批量）。这种网络假设输入特征中没有特定结构：之所以叫密集连接，是因为Dense层的每个单元都和其他所有单元相连接。 密集连接网络最常用于分类数据，比如房价预测。它还用于大多数网络最终分类或回归的阶段。例如卷积神经网络最后通常是一两个Dense层，循环神经网络也是如此。 5.2卷积神经网络卷积神经网络能够查看空间局部模式，其方法是对输入张量的不同空间位置应用相同的几何变换。这样得到的表示具有平移不变性，这使得卷机层能够高效利用数据，并且能够高度模块化。 5.3循环神经网络循环神经网络对输入序列每次处理一个时间步，并且自始至终保存一个状态。如果序列中的模式不具有时间平移不变性，（比如时间序列数据，最近的过去比遥远的过去更加重要），那么应该优先使用循环神经网络，而不是一维卷积神经网络。 6.已经取得的进展深度学习已经取得了一下突破，它们都是机器学习历史上非常困难的领域。 接近人类水平的图像分类 接近人类水平的语音识别 更好的机器翻译 更好的文本到语音转换 接近人类水平的自动驾驶 更好的广告定向投放，Google 、百度、必应都在使用 更好的网络搜索结果 能够回答用自然语言提出的问题 在围棋上战胜人类 7.局限性7.1将机器学习拟人化的风险深度学习模型并不理解它们的输入，至少不是人类所说的理解。我们自己对图像、声音和语言的理解是基于我们作为人类的感觉运动体验。机器学习模型无法获得这些体验。如果向神经网络展示与训练数据不一样的数据，它们可能会给出荒缪的结果。下图是一个基于深度学习的图像描述系统的失败案例。 7.2局部泛化与极端泛化极端泛化：人类只用很少的数据，甚至没有新数据,凭借自己的抽象和推理能力就可以适应从未体验过的新情况。我们可以将已知的概念融合在一起，来表示之前从未体验过的事物,比如绘制一匹穿着牛仔裤的马，或者想象我们中了彩票会做什么。 局部泛化：深度网络执行从输入到输出的映射，如果新的输入与网络训练时所见到的输入稍有不同，这种映射就会变得没有意义。 四、总结 给出本文的思维导图，回忆一下~]]></content>
      <categories>
        <category>智能网联汽车</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>人工智能</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智能网联汽车——底盘线控系统]]></title>
    <url>%2F2019%2F12%2F20%2F%E6%99%BA%E8%83%BD%E7%BD%91%E8%81%94%E6%B1%BD%E8%BD%A6%E2%80%94%E2%80%94%E5%BA%95%E7%9B%98%E7%BA%BF%E6%8E%A7%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[目前L3/L4级别的自动驾驶车辆使用的都是线控制动、驱动、转向，本篇就来梳理一下制动、驱动、转向的发展历程。 一、线控驱动1.节气门 传统节气门：通过机械结构连接，反应延迟小；没有办法应对复杂道路下的各种工况，油耗和排放都不能得到很好地控制。 电子节气门：取消了踏板和节气门之间的机械结构，而是通过加速踏板位置传感器去检测油门踏板的位移，这个位移就代表了驾驶员的驾驶意图。把该信号传递给ECU，ECU根据其他传感器反馈回来的信息进行分析和计算得到最佳的节气门开度，然后再驱动节气门控制电机，节气门位置传感器检测节气门的实际开度，再把该信号反馈给ECU去实现整个节气门开度的闭环控制。 2.传统汽车线控驱动 对于传统内燃车而言，只需要能够实现油门踏板的自动控制就能够实现线控驱动。 方式一：在油门踏板的位置增加一套执行机构，去模拟驾驶员踩油门。同时还要增加一套控制系统，输入是目标车速信号，实际车速作为反馈。通过控制系统计算，去控制执行机构去执行动作。 方式二：接管节气门控制单元加速踏板的位置信号，只需要增加一套控制系统，输入目标车速信号，把实际的车速作为反馈，最后控制系统计算输出加速踏板位置信号给节气门控制单元。 3.电动车线控驱动 VCU（整车控制单元）的主要功能是实现扭矩需求的计算以及实现扭矩分配。 VCU接收车速信号、加速、车踏板信号以及一些其他信号，然后在VCU内部进行计算，发送扭矩指令给电机控制单元，电机控制单元接收到VCU的扭矩需求后进行电机转矩的控制，从而能够实现实时的响应VCU的扭矩需求，因此只需要VCU开放速度控制接囗就能实现线控驱动。 二、线控制动线控制动的核心也是速度控制，目的是为了使车辆减速或者维持一定的速度。 1.制动控制 在车上根据功能的不同，通常会有两套制动系统： 行车制动（脚刹）：通过制动踏板来实现车辆的减速 驻车制动（手刹）：保持车辆停止状态 2.ABS（制动防抱死系统）1）已经成为现在乘用车的标配。 轮速传感器：用于检测车轮的速度，这个速度信号会输入ABS ECU。 ABS ECU：接收轮速信号及其他信号，计算车轮的滑移率、车轮的加速度、减速度等信号，判断车轮是否有抱死的趋势从而输出控制指令给液压控制单元 ABS HCU（液压控制单元）：相当于执行器，接收电子控制单元的命令，执行压力调节的任务 2）车辆在湿滑的路面起步会出现打滑现象，这是ABS解决不了的 3）TCS（牵引力控制系统）：出现上述打滑现象，TCS会干预发动机和制动系统，避免车轮打滑发生 3.ESP（车身电子稳定系统）1）在ABS和TCS的基础上发展成为现在的ESP系统 2）功能：ESP系统可以实现车辆的纵向动力学和横向动力学的稳定性控制，会用到方向盘转角控制器、制动主缸压力传感器用来判断驾驶员的驾驶意图；横摆角速度传感器、横向加速度传感器用来确定车辆实际的运行轨迹，用来计算质心侧偏角 3）工作原理：ESP控制单元会接收到驾驶员的输入信号，通过汽车的动力学模型估计车辆的运动状态和车辆实际运行状态并且进行比较得到相应的控制指令（制动控制指令或者发动机的转矩需求控制指令），最后再由具体的执行器完成车辆的控制 4.iBooster智能化助力器系统 1）由博世推出 2）功能：实现的功能和真空助力器功能是一样的，当驾驶员踩下制动踏板的时候提供制动助力功能。结构和ESP几乎一样，都是由电机、蜗轮蜗杆，再加上一条齿轮齿条机构将电机的驱动力矩转为齿条的推力从而为驾驶员提供助力。 3）工作原理：驾驶员踩下制动踏板，iBooster的助力杆会朝助力器的阀体方向去移动，踏板行车传感器检测到助力器输入杆的位移后，将这个位移信号发给iBooster控制单元，控制单元计算出电机需求扭矩，再通过机械结构将扭矩转化为制动力 4）优缺点：根据具体的行车工况，提供最合适的辅助制动力矩；另外对于新能源车，尤其是纯电动车产生真空助力要麻烦，成本也比较高，iBooster为此提供了全新的解决方案。 5.线控制动的实现（以ABS为例）1）和线控驱动实现的方式一样，加装一套执行机构和控制系统，控制系统可以和线控驱动的控制系统结合起来，比较目标车速和实际车速进行驱动油门踏板或者驱动制动踏板。（适用情况:ABS系统控制接口无法获得） 2）对ABS ECU进行接管控制，比如ABS ECU接收期望制动压力信号，直接驱动HCU。（适用于ABS的控制接口开放的情况。） 三、线控转向线控转向的目的是实现横向控制，核心是实现方向盘的转角控制。 1.机械转向系统基本结构 转向器总成：把方向盘的转动转为齿条的直线运动 转向器拉杆总成：把齿条的直线运动转为轮胎的转动，这样就实现了车辆的转向 2.HPS（液压转向系统）1）在机械转向结构的基础上再加上转向油泵、转向控制阀、转向动力缸、储油罐、油管就构成了HPS。 2）工作原理：在直线行驶的时候，是一个不打方向的油路，助力缸当中是没有产生高压油的；当转动方向盘时，阀芯就会转动。油泵中高压油就会通过阀芯和阀套之间的间隙流向助力缸的一侧。比如说向右转，高压油就会进入到助力缸的左侧，最终实现助力转向。 3）优点 ①动力转向可以减小作用在转向盘上的力,提高转向轻便性 ②由于液压系统的阻尼作用，可以衰减道路冲击,提高行驶安全性 4）缺点 ①很难协调低速转向轻便性和高速沉稳的需求，HPS的油泵由发动机来驱动，低速的时候希望能获得大的助力，而发动机在低速时转速比较低，所以为了转向轻便可能使用大排放量的油泵；高速的时候希望能获得小的助力，而发动机在高速时转速比较高，油泵的流量会比较大，通常会有高速的时候转向过轻的感觉。 ②即使在不转向的时,油泵也一直运转,增加了能量损失 ③存在渗油与维护问题,提高了保修成本,泄漏的液压油会对环境造成污染 ④低温工作性能较差 3.EHPS（电动液压转向系统） 1）将HPS中由发动机驱动油泵转为电机驱动油泵。这样一来，转向的助力特性随着车速的变化而变化，只需要根据车速去控制电机的转速就可以实现。 2）优点 ①电控液压动力转向是在原液压式动力转向系统上发展起来的,原来的系统都可以利用,不需要更改布置 ②低速时转向效果不变,高速时可以自动根据车速逐步减小助力,增大路感提高车辆行驶稳定性。 ③采用电动机驱动油泵时可以节省能量 3）缺点 ①依然存在渗油问题 ②零件增加,管路复杂，不便于安装维修及检测 ③原有液压系统的基础上又增加了电子系统,使系统越加复杂,成本增加 ④低温工作性能没有改善 4.EPS（电动助力转向系统）1)EPS的出现EHPS的问题,且是实现线控转向的基础 2)工作过程：驾驶员打方向盘，转矩传感器检测到有转矩的信号，ECU接收该信号同时也接收车速等信号并且实时计算助力电机需要的助力转矩的大小和方向，然后驱动助力电机转动，从而辅助驾驶员转向。 3）助力电机：现在市面上采用的助力电机大体包括永磁直流电机（用在前轴载荷不太大的车型上）和永磁同步电机（用在前轴载荷比较大的车型上）。 4)EPS因为是助力系统，所以控制的是电机的输出力矩而不是转角，线控转向是实现方向盘转角的控制，所以要实现转向控制，就需要在EPS模式下增加转角闭环模式控制。现在的方向盘自动转向也是这样实现的。 5.SBW（线控转向系统）1）汽车线控转向系统由方向盘总成、转向执行总成和主控制器(ECU)三个主要部分以及自动防故障系统、电源等辅助系统组成。 方向盘总成包括了方向盘、方向盘转角传感器、力矩传感器、方向盘回正力矩电机；将驾驶员的转向意图通过测量方向盘转角转换成数字信号，然后传递给主控制器，同时接受主控制器送来的力矩信号从而产生方向盘回正力矩提供给驾驶员相应的路感信息。 主控制器会对采集的信号进行分析处理判明汽车的运动状态，然后向方向盘回正力矩电机和转向电机发送指令控制两个电机的工作，保证在各种工况下都具有理想的车辆响应，从而减轻驾驶员的负担，同时控制器还可以对驾驶员的操作指令进行判别，判断在当前状态下驾驶员操作是否合理。 自动防故障系统：包括一些列的监控和实施算法，能够针对不同的故障形式和等级作出相应的处理从而实现最大限度的保持汽车正常行驶。 电源系统：承担着控制器、两个执行马达以及其他车用电机的供电任务。其中仅仅是前轮转角执行马达的功率就有500-800W，加上汽车上的其他电子设备，电源的负担就已经想相当沉重了。所以要保证电网在大负荷下稳定工作，电源的性能就显得十分重要了。 2）优点： 轻易实现主动转向的功能 获得比EPS更快的响应速度 滤掉路面上的激震信号 消除了撞车事故中转向柱后移引起伤害驾驶员的可能性 去掉了转向系功能模块间的机械连接，布置方式灵活,可以获得更大的驾驶员腿部空间。 四、智能泊车 1.车位识别1）超声波可以识别空间车位 2）摄像头可以识别线车位 2.轨迹规划基本原理是阿克曼转向几何原理，解决的是在碰撞约束下的车辆运动轨迹规划的问题 3.泊车动作1）在泊车过程中要求车速控制在5~12km/h范围内，可以根据实际情况做出轨迹修正的调整，这里就涉及到APA系统与线控底盘系统的交互。半自动泊车系统只需要线控转向的支持，全自动泊车就需要线控转向线控驱动线控制动的支持 2）轨迹跟踪 APA 控制器需要控制方向盘的转角，泊车过程中的车速和档位，因此需要线控底盘的支持 五、总结 给出本文的思维导图，回忆一下~]]></content>
      <categories>
        <category>智能网联汽车</category>
      </categories>
      <tags>
        <tag>线控制动</tag>
        <tag>线控驱动</tag>
        <tag>线控转向</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智能网联汽车——未来发展趋势]]></title>
    <url>%2F2019%2F12%2F19%2F%E6%99%BA%E8%83%BD%E7%BD%91%E8%81%94%E6%B1%BD%E8%BD%A6%E2%80%94%E2%80%94%E6%9C%AA%E6%9D%A5%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF%2F</url>
    <content type="text"><![CDATA[一、技术的发展趋势1.总体发展路线 渐进式：传统的主机厂受现有开发体系和供应链的牵制,选择以核心技术和终端产品为主线的逐级发展路线，即从L1开始逐级过渡到L5量产 跨越式：互联网企业,没有历史包袱,且以商业模式和未来产业生态布局为着眼点,故选择与传统车厂差异化的发展路径，直接由L4/L5高位切入，然后不断丰富车辆驾驶场景 2.现阶段应用场景2.1高速公路自动驾驶 工况相对简单、障碍物类型单一、车道线等结构化特征明显 长途驾驶易疲劳,驾驶员对自动驾驶系统需求较强 易发生重大安全事故,可明显改善交通安全 2.2低速与限定场景无人物流：借助无人驾驶技术，装卸、运输、收货、仓储等物流工作将逐渐实现无人化和机器化，促使物流领域大大降低成本。 公共交通：在园区、校区内应用公交车的无人驾驶系统，能及时对突发状况做出反应，可实现无人驾驶下的行人车辆检测、减速避让、紧急停车、障碍物绕行变道、自动按站停靠等功能，并且公交车的路线一般固定。 环卫：无人驾驶清洁车通过自主识别环境，规划路线并自动清洁，实现全自动、全工况、精细化、高效率的清洁作业。 港口码头：无人驾驶技术在港口码头场景的转化应用，可有效解决传统人工驾驶时，存在的行驶线路不精准、转弯造成视线盲区、司机疲劳驾驶等问题，节约人工成本。 矿山开采：无人驾驶在矿山开采中，通过技术支撑，矿山开采整体能耗下降、综合运营效益提升，提高矿区安全生产工作，加快智慧矿区的建设。 零售：无人驾驶技术让零售实体店突破以往的区域限制，打破线下有形场景与线上无形场景的边界，实现零售业态的全面升级。 3.环境感知3.1传感器的布置配置一：高精度地图+多线束激光雷达 成本高、数据量大，适合L3、L4级别智能车 配置二：毫米波雷达+少线束激光霤达+摄像头+超声波雷达 传感器成本相对较低，适用于L1、L2级别智能车 3.2 基于深度学习的多目标检测 优点： 自动提取特征,端到端的模型 发掘隐藏规律,更高的精度 大数据和硬件发展以及算法的驱动 缺点： 需要大量的高质量数据作为学习样本，对数据采集和标注提出了较高的要求 内在机理不清，边界条件不确定，需要与传统方法融合，以保证可靠性 受限于目前车载芯片处理能力的限制，高速场景下计算的实时性不能保证 恶意样本的欺骗性：噪声会影响对目标精准的识别 3.3激光雷达1）激光雷达的成本将进一步降低：多家企业均宣称量产后激光雷达价格将降至200-500美元。 2）激光雷达向多线束以及固态激光雷达发展：在2018年的CES(国际消费类电子产品展览会)上, Ve lodyne对外展示了两款产品:128线激光雷达VLS-128和固态激光雷达 Velarray。 3）国产激光雷达将占有一席之地。 3.4高精度地图与高精度定位高精度地图：地图与北斗导航、视觉/雷达CAN总线等系统将实现深度的信息融合，5G技术应用将帮助实现高精度三维地图的实时自动增量更新，高精度地图数据采集式样、交换格式和物理存储将逐步的标准化。 高精度定位：全国北斗地基增强系统将得到快速的推广，为车辆低成本的精确定位提供条件。 4.决策4.1端到端决策以车载传感器（摄像头和激光雷达）为输入，以驾驶员操作（油门、刹车、方向盘）为输出,在深度神经网络模型的训练下，将决策过程视作一个不可分解的黑箱,主要的问题是决策结果不具有逻辑可解释性，并且预覆盖所有场景困难，数据需求量大。 4.2分解式决策将决策过程分解为独立的简单的子问题,如场景认知、运动预测、行为决策、轨迹规划等,每一个问题独立求解。 5.底盘控制系统5.1转向在传统EPS基础上更新控制器，重新匹配电机功率或重新设计转向系统使其具备主动转向功能。 5.2制动在原ABS或ESP基础上进行改造或重新设计制动系统，使其具备主动制动功能。 5.3驱动在电动汽车上易于实现线控（VCU的主要功能是实现扭矩需求的计算以及实现扭矩分配。因此只需要VCU开放速度控制接囗就能实现主动驱动），在传统燃油车上，发动机转矩控制精度有限，且变速箱需要进行一定改造。 6.车联网车辆通信技术的发展存在DSRC与LTE-V两条不同的路径。 DSRC：DSRC技术与标准已较为成熟，美日欧等国可能通过强制法规手段大力推动，应用主要限于安全相关的领域。 LET-V：正在国际范围内加快推进，可能在中国成为主流车联网通信系统。随着时间推移，LTE—V技术由于可以向5G平滑过渡，其发展对DSRC造成越来越大压力。 7.信息安全汽车信息安全是随着车辆网联化比例的提升，而得到越来越高的关注。为了防范黑客入侵非法获取数据，葚至远程控制车辆等潜在的威胁，必须高度重视信息安全防范，出台汽车信息安全标准与评价体系。 8.测试评价技术智能汽车的测试评价方法与传统汽车有很大差异，需要专门的评价体系和测试场地。 1）需要对自动驾驶系统的基本要素功能进行考核 图像识别能力 雷达对障碍物的探测能力 网络联通能力 执行器的控制性能 信息安全防护性能 2）需要对自动驾驶理解我国相关交通法规的能力进行考核 对信号灯的识别 对交警手势的识别 对限速限高等交通标识物的识别 3）需要专门整理出自动驾驶汽车会遇见的典型场景工况，对自动驾驶系统进行各场景下的专门测试 4）让自动驾驶汽车在划分好的测试区域内面对真实的随机出现的交通情况做出自主决策与控制，检测其是否能够依法依规安全行驶 二、产业的重大变革1.数据成为未来竞争的核心要素 2.互联网造车 产业变革与技术发展为互联网企业提供了突破传统车企壁垒的绝佳机遇 同时传统汽车企业着能依托优势资源把握智能网联的发展趋势，也将有巨大的发展机遇，摆脱低利润的制造企业的困境 3.硬件和软件都将是未来汽车的核心竞争力 三、变革与挑战1.变革长期而言,基于人工智能技术,实现无人驾驶的出租车将会在汽车领域产生颠覆性的变革。 1）颠覆出租车市场 有效载客时间高于传统出租车80%，成本降低至出租车的12% 2）改善城市空间布局 节省40%的停车空间 3）优化城市交通 可平均减少30%交通拥堵时间 4）颠覆私家车市场 相对自有购买车辆,无人驾驶出租车存在更高的经济性、便利性优势 2.挑战但自动驾驶的未来发展,目前来看还需要突破法律法规、伦理与基础设施的三大挑战。 1）法律法规：适用于自动驾驶的交通安全法及其实施条例、相关机动车行驶的技术标准缺失,其制定还面临诸多不确定性 2）伦理：在行人与驾驶者的安全之间,人工智能的道德判定应该怎样预设存在争议 3）基础设施：未来自动驾驶普及需要5G通讯技术的建设与商业化；直接决定自动驾驶系统的安全与可靠性的高精地图,其行业处于发展初期，目前仍还面临诸多挑战 四、总结 给出本文的思维导图，回忆一下~]]></content>
      <categories>
        <category>智能网联汽车</category>
      </categories>
      <tags>
        <tag>未来发展趋势</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智能网联汽车——网联化]]></title>
    <url>%2F2019%2F12%2F16%2F%E6%99%BA%E8%83%BD%E7%BD%91%E8%81%94%E6%B1%BD%E8%BD%A6%E2%80%94%E2%80%94%E7%BD%91%E8%81%94%E5%8C%96%2F</url>
    <content type="text"><![CDATA[智能网联汽车由两部分组成——智能化和网联化，在上一篇推送《智能网联汽车——智能化》已经详细介绍了智能化，今天将详细介绍网联化。网联化其实就是车联网，今天将主要介绍什么是车联网，车联网后有什么功能，通过什么样的手段（无线通信、短程通信、5G通信）实现车联网。 一、车联网 V2X引言： 类似电脑、手机联网后共享网络的信息资源，汽车联网后可与汽车、道路交互信息，实现资源共享。 场景： 炎热的夏天，从办公室下班回家如果能提前打开车内的空调，上车后就很舒适。 车辆将要通过路口，如果车辆能及时接收到信号灯的状态就可以提前作出通过路口或减速的预判，有效减少闯红灯的现象。 前方车辆在急刹车时，如果能及时将信息告知后方车辆，可以避免发生连环追尾事故。 所有上述场景的实现都依赖于汽车和外界的连接，也就是Vehicle to everything，即V2X。 1.车联网概念：利用先进传感技术、网络技术、计算技术、控制技术、智能技术对交通进行全面的感知，对每辆汽车进行交通全程的控制，对每条道路进行交通的全时空控制，实现道路的零堵塞、零伤亡、极限通行能力的专门控制网络。 分类：车联网主要围绕车通信问题展开，可以根据车通信距离的远近分为车内网、车际网和车云网。 根据上述车联网的通信方式，中国汽车工程协会对车联网给出了另一种阐释：车联网是以车内网、车际网和车云网为基础，按照约定的通信协议和数据交互标准，在车与车、车与人、车与环境之间进行通信和信息传输，实现车辆智能化控制、智能动态服务和智能交通管理的一体化网络。 2. V2X2.1车际网专用中短距离通信技术,实现车车/车路协同,包括DSRC、LTE-V、5G； 时延极短,可靠性高,需要支撑主动安全应用。 2.2车云网提供车与云端的连接,目前用于 Telematics的通信,包括3G/4G/5G；​ 覆盖范围广,能够与 Internet连接，时延较大,不适合紧急安全应用。 2.3车内网车与内部传感器的有线连接, CAN BUS、高速以太网；​ 车机与手机等设备的无线连接,包括蓝牙、WiFi、NFC。 二、无线通信技术无线通信技术是智能网联汽车实现的基础，直接决定了信息交互的实时性和有效性。 无线通信是利用电磁波信号可以在空间当中自由辐射和传播，进行信息交互的一种通信方式，可以用来传输数据、图像、音频、视频等等。 1.组成 发射设备：将原始的信号源转换成适合在给定介质上（电磁波）传输的信号。 调制器将低频信号加载到高频载波信号上，频率变换器进一步将信号变换成发射电波所需要的频率（比如短波、微波等等），经过功率放大器放大后经过天线发射出去实现传输。 接收设备：将收到的信号还原成原来的信号送至接收端。 接收设备将接收天线接收的射频载波信号经过频率放大以及频率转换及解调器解调后将原来的信号还原出来。 2.短距离无线通信2.1蓝牙蓝牙技术能够有效简化移动通信终端设备之间的通信，也能够简化设备和英特网之间的通信，使得数据传输变得更加迅速高效，也为无线通信拓宽了道路。 组成： 特点： ①全球范围使用（工作在2.4GHzISM频段，绝大多数国家ISM频段范围是2.4~2.4835GHz，并且使用该频段是不需要向各国的管理部门申请许可证，是可以直接使用的） ②通信距离0.1~10m（功率达到100mw时，距离可以达到100m） ③可同时传输语音和数据（采用的是链路交换和分组技术，支持异步数据信道，三路语音信道以及异步数据与同步语音同时传输的信道） ④可建立临时性的对等连接（可以根据蓝牙设备在网络设备中的角色分为主设备和从设备，主设备是主网连接主动发起请求的蓝牙设备。几个蓝牙设备连接成皮网时，只有一个主设备，其他都是从设备） ⑤抗干扰能力强（采用跳频的方式来扩展频谱） ⑥模块体积小（便于集成） ⑦功耗低（激活模式为正常工作，呼吸模式、保持模式、休眠模式为节能而使用的三种低功耗模式） ⑧接口标准开放（蓝牙技术联盟为了推广蓝牙技术的运用，将蓝牙技术协议全部公开，全世界任何范围内组织、个人都可以进行蓝牙的产品开发） ⑨成本低（各大供应商研发自己的蓝牙芯片） 2.2ZigBee以IEEE802.15.4标准为基础发展起来的一种短距离无线通信技术。 组成：支持三种网络拓扑结构。 特点： ①低功耗（传输速率比较低，发射功率仅为1mw，而且采用的是休眠模式） ②低成本（由于大幅度简化协议） ③低速率（仅有20-250kbit/s） ④短距离(在10-100m之间，在增加发射功率后，距离可增加到1-3000m) ⑤短延时（响应速度比较快，一般休眠激活时延只有15ms，节点连接进入网络连接只需要30ms，活动信道接入只需要15ms） ⑥高容量（可以采用三种网络结构） ⑦高安全（采用三级安全模式，可以灵活确定他的安全属性） ⑧高可靠（采用碰撞避免策略） ⑨免执照频段（主要应用在数字家庭领域、工业领域以及智能交通领域） 2.3WiFi在1997年，WiFi的IEEE802.11标准问世，1999年成立了WiFi联盟，之后为了满足不断出现的实际需求又相继推出了802.11a、802.11b、802.11g、802.11n等多个标准。 特点： ①覆盖范围大（覆盖半径可以达到数百米，而且可以解决高速移动时数据的纠错问题和误码问题） ②传输速率快 ③健康安全（IEEE802.11标准规定发射功率不得超过100mw，实际发射功率为60-70mw，所以辐射非常小） ④无需布线 ⑤组建容易 2.4RFIDRFID是20世纪90年代兴起的一种自动识别技术，也称为电子标签。可以通过无线电信号识别特定目标并读写相关的数据，而且不需要识别系统与特定目标之间建立机械或者光学的接触，即这是一种非接触式的自动识别技术。 组成： 标签：由耦合元件和芯片组成，每个电子标签都具有唯一的电子编码附着在物体上标识目标对象，每个标签都有全球唯一的ID号，就是UID（用户身份证明）。这个ID在制作标签芯片时会存放在ROOM中，无法修改。 读卡器：读取或写入标签信息的设备。一般情况下会将收集到的数据信息传送到后台系统，再由后台系统处理数据信息。 天线：在标签和读卡器之间传递射频信号，读卡器发送的射频信号通过天线以电磁波的形式辐射到空间，当电子标签的标签进入该空间时，接收电磁波的能量，但是只能接收很小的一部分。 特点： ①读取方便快捷（数据的读取无需光源，可以通过外包装来进行。有效识别距离也更大，采用自带的电池主动标签时，有效距离可以达到30m以上） ②识别速度快（标签一进入磁场，读卡器就可以及时读取其中的信息而且能够同时处理多个标签，实现批量的识别） ③数据容量大 ④穿透性和无屏障阅读（在被覆盖的情况下，RFID可以穿越纸张、木材、塑料等非金属或非透明的材质，并且能进行穿透性的通信） ⑤使用寿命长,应用范围广（应用在粉尘、油污等高污染环境和放射性环境，而且封闭式包装使得RFID标签寿命大大超过印刷的条形码） ⑥标签数据可动态更改（利用编程器可向标签写入数据，从而赋予RFID标签交互式便携文件的功能。而且写入的时间相比打印条形码更少） ⑦安全性好（不仅可以嵌入附着在不同类型的产品上，而且可以为标签数据的读写设置密码保护） ⑧动态实时通信（标签以50-100次/s的频率和读卡器进行通信，所以只要RFID所附着的物体出现在读卡器有效识别范围内就可以对其所在位置进行动态追踪和监控） 应用： 汽车无钥匙进入系统 校园卡/员工证 ETC 3.远距离无线通信3.1移动通信我们的手机卡不管使用的是中国移动、中国联通还是中国电信其实都属于移动通信。移动通信技术是指通信的双方至少有一方是在运动中实现通信的方式，包括移动台与固定台之间、移动台与移动台之间、移动台与用户之间的通信技术。 组成： 主流4G网络结构： 全IP网络可以使不同的有线和无线接入技术实现互联、融合。全IP网络的无线接入点有无线局域网，AD Hoc（移动自组织网）网，有线接入点有公共电话交换网络（PSTN）和综合业务数字网络（ISDN）。移动通信的2G/2.5G和3G/B3G通过特定的网关进入IP核心网。Internet则通过路由器与IP核心网相连。 特点（和固定通信相比）： ①移动性（保证物体在移动状态下通信） ②电波传播环境复杂多变（移动物体在各种环境中运动，电磁波在传播时会产生反射、折射、绕射、多普勒效应等现象，产生多径干扰、信号传播、延迟等效应，另外移动台相对于基地台远近的变化会引起接收信号场强的变化，也就是说存在远近效应） ③噪声和干扰严重（在城市环境中存在着汽车噪声，各种工业噪声还有移动用户之间的互调干扰、同频干扰等等） ④系统和网络结构复杂（移动通信是一个多用户通信网络，必须使用户之间互不干扰，能协调一致的工作。此外，移动通信系统还需要与市话网、卫星通信网、数据网等互联） ⑤用户终端设备要求高 ⑥要求有效的管理和控制（系统中用户端是可以移动的，为了确保与指定的用户通信，移动通信系统必须具备很强的管理和控制功能） 3.2微波通信微波通信使用的是波长在0.1mm在1mm之间的电磁波，对应的频率范围是在0.3~3000GHz。 组成： 天馈系统：用来发射接收或者转接微波信号的设备。 发信机：用来将基带信号转变成大功率的射频信号。 收信机：用来将基带信号的射频信号转变成基带信号。 多路复用设备：把多个用户的电信号构成共用一个传输信号的基带信号。 用户终端设备：把各种信息变成为电信号。 特点： ①快速安装（微波通信占地面积小） ②抵御自然灾害和人为破坏能力强（微波通信链路是空间介质） ③受地理条件制约小（微波通信链路是空间介质） ④设备体积小、功耗低（微波传输设备大量采用集成电路，而且数字信号在传播过程中抵抗干扰能力强） 3.3卫星通信卫星通信是指利用人造地球卫星作为中继站转发无线电信号，在两个或者多个地面站（在地球表面的无线电通信站）之间进行通信。卫星通信是在地面微波通信和空间技术基础上发展起来的，通信卫星相当于是一个离地面很高的微波中继站。 组成： 卫星端：在空中起到中继站的作用，把地面站发射过来的电磁波放大后再返回送到另一个地面站。 地面端：是卫星系统和地面公众网的接口，地面用户也可以通过地面端出入卫星系统，形成链路。 用户端：各种用户的终端，比如手机。 特点： ①通信距离远（卫星离地面约35000km，视区可以达到地球表面的42%，最大通信距离有18000km，并且中间无须再进入中继站） ②通信容量大,业务种类多,线路稳定（卫星通信采用的是微波频段，可供使用的频带资源较宽，一般都在数百兆Hz以上，适用于多种业务传输；卫星的电波在大气层以外的宇宙空间中传输，电波传播比较稳定） ③覆盖面积大,便于实现多址连接（通信卫星所覆盖的地面站都可以使用该卫星进行通信，） ④卫星通信机动灵活（地面基站的建立可以不受地理条件的约束，可以建在边远地区、岛屿、甚至汽车、轮船、飞机上） ⑤自发自收监测（只要地面站收发端处于同一通信卫星覆盖范围内，自己向对方发送的信号自己也能接收，从而监视本站所发信息是否正确、传输以及通信质量优劣） ⑥卫星的发射和控制技术比较复杂 ⑦较大的传播时延（以静止卫星通信系统为例，地面站之间的单程传播时延约为0.27s，往返传输时延大约在0.54s） 三、专用短程通信1.DSRC概念：DSRC是专门用于道路环境的车辆与车辆、车辆与基础设施、基础设施与基础设施间,通信距离有限的无线通信方式,是智能网联汽车系统最重要的通信方式之一。 参考架构： 车载媒体单元（OBU）的媒体访问单元控制层和物理层负责处理车辆与车辆之间、车辆与路测设施之间专用短程无线通信连接的建立，维护信息传输。应用层和网络层负责把各种服务和应用信息传递到路测基础设施和车载单元上并通过车载子系统与用户进行交互。 结构中的管理与安全功能覆盖了专用短距离通信的整个框架。 技术要求： 总体功能要求（包括无线通信能力和网络通信能力） 媒体访问控制层技术要求 网络层技术要求 应用层技术要求 信道分配： 中心频段是在5.9GHz，总共有75MHz的频带宽度，前面的5MHz被预留，后面的70MHz被平均分成7个信道，有一个控制信道，其他六个都是服务信道。172和184是用于公共安全专用信道，比如说和生命财产相关的应用就在这两个信道当中使用。 汽车辅助驾驶：汽车辅助驾驶和道路基础设施状态的警告，道路基础设施的警告包括车辆事故、道路工程警告、交通条件警告、基础设施状态异常警告 交通运输安全：紧急救援请求及响应，紧急车辆调度与优先通行，超载超线管理，交通弱势群体保护 交通管理：交通法规的告知，交通执法、信号优先、停车场管理 导航及交通信息服务：路线实时的指引和导航，流量监控，建议行程，信息点通知 电子收费：道路、桥梁和隧道通行费、停车费 运输管理：运政稽查、特种运输检测、车队管理、场站区管理，车辆软件数据的匹配和更新、车辆的数据校准、节奏感知信息更新以及发送 2.LET-VLTE-V类似于基站发射4G信号,比前面那种靠WiFi的更稳定,让车与车之间的沟通更便利。被认为是实现车联网的重要基石，基于4.5G网络以LTE蜂窝网络作为V2X的基础，面向未来5G的重点研究方向。也是车联网的专有协议，面向车联网应用场景，实现车与车、车与路测设施、车与人、车与网络的互联和数据传输，也就是V2X。 为了应对车辆主动安全、行车效率、车载娱乐等多场景不同的需求，LTE-V采用的是广域蜂窝式和短程直通式的通信，前者是基于现有蜂窝技术的扩展，主要承载着传统的车联网业务，后者引入LTE—D2D，由于LTE-D具备了能够寻找500公尺以内数以千计设备的能力，因此能让两个以上最接近LTE-D设备在网内通信。 DSRC与LTE-V之争： DSRC已被美国交通部确认为V2V的标准，经过十年的研发与测试已经定型，在2015年9月美国交通部拨出4200万美元在美国三个地方开展安全测试；通用汽车在2016年上市的凯迪拉克CTS上就装备了V2X；欧盟的协同式智能交通系统和日本的V2X也都是基于DSRC技术。 由于5.9G的DSRC在中国会有潜在的干扰问题，所以中国需要一个不同的V2X解决方案。LTE-V的最大好处就在于能够重复使用现有的蜂窝基础建设与频谱，运营商不需要布建专用的路测设备和提供专用的频谱。由于DSRC基本上是一种WiFi技术，所以LTE可以比DSRC提供更好的服务质量，并且DSRC面临部署的问题，性能无法被保证。 四、5G通信1.特点 网速更快。作为新型移动通信网络技术，5G的传输速度可以达到几十Gb/s,在2GHz波段下的传输速度可以高达1Gb/s，也就是说下载一部电影只需要几秒钟就可以完成了。 兼容性好。5G通信以原有的通信技术为基础形成无线网路技术平台，涉及到NFC和蓝牙等无线技术。兼容性好，使人们在网络支付中更加安全。 2. 5G与自动驾驶 1）5G高可靠性、低延时的特点正是自动驾驶所需。现有的感知技术比如雷达、摄像头实际上只给车提供了”看“的能力，没有办法跟车实现实时的互动。有了5G的交互式感知，车就会对外界做出一个输出，不光能探测到状态，还可以作出一些反馈。 2）自动驾驶的协同里面有很多场景，比如自动超车、协作式避碰、车辆编队都对可靠性和延时性提出要求，都需要5G的保证。 3）可以说5G提供了交互式感知，还弥补了传感器受到距离和环境的约束，同时还促进了从单车智能到协作式智能的演化。 4）从自动驾驶运营的角度来说，5G的到来也提供了一些新的可能。比如说车辆在大多数情况下完成行驶测试任务，遇到自动驾驶车辆无法自主处理的场景，L3级以上的自动驾驶系统可以做出判断并通知位于控制中心的驾驶员远程介入。远程驾驶员还可以操控多辆无人驾驶车辆等等，由此可见，5G可以协助对城市固定路线车辆实现部分智能的云控制。对于园区、港口的无人驾驶车辆实现基于云的运营优化以及特定条件下的远程控制。 六、总结 给出本文的思维导图，回忆一下~]]></content>
      <categories>
        <category>智能网联汽车</category>
      </categories>
      <tags>
        <tag>网联化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智能网联汽车——智能化]]></title>
    <url>%2F2019%2F12%2F13%2F%E6%99%BA%E8%83%BD%E7%BD%91%E8%81%94%E6%B1%BD%E8%BD%A6%E2%80%94%E2%80%94%E6%99%BA%E8%83%BD%E5%8C%96%2F</url>
    <content type="text"><![CDATA[一、无人驾驶的系统组成 算法端包括面向传感、感知和决策等关键步骤的算法，从传感器原始数据中提取有意义的信息以了解周遭的环境情况，并且根据环境变化做出决策； 客户端包括机器人操作系统以及硬件平台，融合多种算法以满足实时性与可靠性的要求； 云端包括数据存储、仿真、高精度地图以及深度学习训练模型，为无人车提供离线计算以及存储的功能，通过云平台我们能够测试新的算法，更新高精度地图，并且训练更加有效地识别跟踪和决策模型。 二、无人驾驶的感知1.常用的传感器1.1回顾前面在智能网联汽车——传感器与驾驶辅助一文已经详细介绍了各类传感器，现在再简单回忆一下无人驾驶车辆的传感器种类， 并对比一下主要传感器的优缺点。 1.2无人车传感器的布设在实际的具备自动驾驶功能的车辆上，同时安装有激光雷达、毫米波雷达、视觉等传感器，并且数量可能不止一个。 1）谷歌：无论是在白天还是夜晚，都能实行360°监控，视野面积可达3个足球场。 激光雷达系统：高频率的短程激光雷达;高分辨率的中程激光雷达;长距离激光雷达。 视觉传感器系统:配备了8个镜头,具有360°视野,在长距离、日光和低亮度的情况下也能很好地工作。 2）奥迪：2017年7月,奥迪在其全球品牌峰会上,正式对外发布了第五代奥迪A8,这款车型搭载了奥迪的L3高度自动驾驶功能,且是全球首款搭载L3高度自动驾驶系统的量产车型。据介绍,全新奥迪A8的高度自动驾驶功能,拥有“拥堵路段自动驾驶”、“泊车自动驾驶”和“车库自动驾驶”等功能系统。 通过所有组件收集大量冗余信息,可以让监测更准确。即使一个信息源或者多个信息源的数据缺失,汽车可以照常自动行驶。 3）德尔福：2017GES上,德尔福( DELPH)与Mob i leye共同展示中央传感定位与规划(CSLP)自动驾驶解决方案,并在复杂路段(包含信号不佳的隧道)跑上10km。该方案总共包括:7个视觉传感器、8个毫米波雷达、5个激光雷达,改善了车辆在隧道或信号不佳的路段的定位能力,即便汽车在丧失GPS信号与云端地图信号的糟糕环境下,CSLP自动驾驶系统依旧能确保10cm以内的定位精度。 同时我们应该注意到对于低级别的自动驾驶车辆，即具备驾驶辅助功能的车辆，其传感器的布局及数量也不尽相同。 4）0级无人驾驶：又称为“非自动驾驶”,由人类驾驶员时刻完全地控制汽车的所有底层结构,包括加减速、转向等。 0级无人驾驶车辆可能会在前后方布置毫米波雷达、视觉传感器、超声波传感器等传感器,主要用于防碰撞提示、帮助驾驶员观察环境。 5）1级无人驾驶：又称为“辅助驾驶”,在特定驾驶模式下由辅助驾驶系统根据环境信息控制转向或加减速中的一种,而其它驾驶任务由人类驾驶员完成。 1级无人驾驶车辆会在前后方布置毫米波雷达、视觉传感器、超声波传感器等传感器,在內部布置视觉传感器,以获取大范围内行驶路径上的障碍物信息,实现车道保持、紧急自动刹车、驾驶员疲劳探测等功能。 6）2级无人驾驶：又称为“部分自动化”,在特定驾驶模式下由自动驾驶系统根据环境信息同时控制转向和加减速,而其它驾驶任务由人类驾驶员完成; 2级无人驾驶车辆会在前后方和四周布置毫米波雷达、视觉传感器、超声波传感器等传感器,在内部布置视觉传感器,以获取大范围内行驶路径上以及车辆周围的障碍物信息,实现自适应巡航、自动泊车、自动换道等功能。 7）3~5级无人驾驶：车辆的传感器配置大致相同,主要区别在于对数据的处理以及驾驶行为决策的成熟度。 2.环境感知技术 2.1车道线传统的方法就是使用基于特征检测的霍夫变换从黑白的图像当中去检测直线或者线段。基本流程如下： 读图,或者读视频 选取关键兴趣区域,减少计算量 取图,灰度化 高斯平滑,减少边缘干扰 利用cany算子,进行边缘检测 集中到边缘检测的兴趣区域,进一步减少运算量 利用霍夫变换变换,进行直线检测 将检测出来的直线与原图进行合成 这种方法存在一些弊端： 没有学习过程,对变化没有适应性(虽然减少了样本标注的过程) 兴趣区域(ROI)固定,没有适应性,取决于摄像机、车辆相对于道路的位置 各种国像处理过程的参数、阈值等需要提前设定,调整不灵活。 处理速度慢,4.5-6帧/秒,正常读取速录为30帧/秒。 故现在更多的是使用基于深度学习的目标检测方法，关于深度学习及其使用将在后面的《深度学习与无人驾驶》推送中专门介绍。 下图就是使用基于深度学习的方法检测出的车道线。 其他的车辆、红绿灯、交通标志都可以使用基于特征检测的方法以及基于深度学习的方法。 3.定位3.1GPS组成： 缺点： 在高楼中由于高楼遮挡容易丢失信号 传播受大气电离层的反射，云层的反射和折射影响，会导致定位误差 更新频率（10Hz）不高 3.2IMU概念：IMU( Inertial Measurement Unit,惯性测量单元)是测量物体的角速度和加速度的装置,进而可以推算物体的姿态、速度和位移。 优缺点： 无信号丢失等问题。 全自主式、全天候、不受外界环境的干扰影响。 频率比较高。 但存在误差积累，在长时间距离内并不能保证位置更新的准确性 3.3 GPS/IMU定位融合原因： GPS是一种相对精准的定位传感器,但更新频率低,并不能满足实时计算的要求。 惯性传感器的定位误差会随着运行时间增长,但由于其是高频传感器,在短时间内可以提供稳定的实时位置更新。 通过融合这两种传感器的优点,各取所长,就可以得到比较实时与精准的定位。 实现： 卡尔曼滤波： 1) 功能:从一组有限的、包含噪声的对物体位置的观测序列预测出物体的位置坐标及速度。 2) 架构: 预测阶段:基于上一时间点的位置预测当前时间点的位置。 更新阶段:通过当前对物体位置的观测去纠正位置预测、从而更新物体的位置 融合优势： 当GPS信号受到遮挡、高强度干扰或当卫星系统接收机出现故障时,INS可以独立地进行导航定位。 提高定位频率。 GPS信息可以用来修正INS的输出信息,控制其误差随时间的积累 3.4 RTK GPS/IMU定位融合为了降低天气、云层对GPS信号的影响，出现了其他GPS技术，如差分GPS。这种技术通过在一个精确的已知位置（基准站）上安装上安装GPS检测接收机，计算得到基准站与GPS卫星的距离，然后根据误差修正结果，从而提高了定位精度。 差分GPS分为位置差分和距离差分，距离差分又分为伪距差分和载波相位差分。RTK（Real-Time Kinematic，实时动态）技术即载波相位差分，可使定位精度达到cm级别，这也是很多无人驾驶公司采用RTK技术定位的原因，但由于硬件成本极高，采用RTK定位技术实现大规模量产商用的可行性不高，并且仍然受城市建筑物影响，楼群越密集越高，定位误差越大。 3.5基于激光雷达的SLAM定位SLAM（即时定位与地图构建）：机器人在未知环境中从一个未知位置开始移动，在移动过程中根据位置估计和地图进行自身定位，同时在自身定位基础上构建增量式地图，实现机器人的自主定位和导航。扫地机器人应用的原理就是SLAM。 激光SLAM：3D激光雷达采集到的物体信息呈现出一系列分散的、具有准确角度和距离信息的点，被称为点云。通常，激光SLAM系统通过对不同时刻两片点云的匹配与比对来计算激光雷达相对运动的距离和姿态的改变，从而完成对机器人自身的定位。 优缺点： 稳定 数据量少 定位及地图创建精度高 但传感器昂贵，量产商用可行性低 3.6基于视觉的SLAM定位优缺点： 获取数据成本低 数据量丰富 建图精度略低 受光线、环境影响较大 结合之前的介绍，视觉传感器分为单目和双目，故也有单目、双目SLAM之分。从可靠性和鲁棒性来说，双目要比单目SLAM更好一些。 一般来说，视觉SLAM都结合IMU等传感器使用，以更大程度地提高建图精度和姿态估计精度。 近几年随着深度学习、人工智能技术的发展，在SLAM领域也有一些结合AI、深度学习、目标检测、语义分割等技术的SLAM技术出现，如语义SLAM等。通过这些方法可以从图像中获得更丰富的语义信息，这些语义信息可以辅助推断几何信息，如已知物体的尺寸就是一个重要的几何线索。 4.高精度地图SLAM主要适用于机器人等领域。在诸如无人清洁车、低速园区无人摆渡车、低速无人快递车等低速场景中十分常见、由于其庞大的计算开销、时延、数据存储等问题，以及无人车对实时控制、安全的高性能要求，导致其目前不适合应用在大范围面积、高速自动驾驶场景中。高速自动驾驶在地图定位方面使用的是高精度地图技术。 高精度地图面向无人驾驶环境采集生成地图数据,根据无人驾驶需求建立道路环境模型,在精确定位、基于车道横型的碰撞避让、降碍物检测和避让、智能调速、转向和引导方面都可以发挥重要作用。它实际上是融合了激光雷达点云数据、GPS信号、语义矢量地图（车道线、红绿灯、交通信号标志等）综合信息的一种定位技术。 高精度地图结构： 第一层是激光点云层，用来实现在线的点云匹配与定位 第二层是视觉地图，主要是视觉感知的一些焦点特征 第三层是矢量地图，其中包含的主要是一些道路的属性特征 第四层是GPS或者说卫星定位层 第五层是动态地图，用来反映动态的交通状况 5.多传感器融合技术简单地说,传感器融合就是将多个传感器获取的数据、信息集中在一起综合分析以便更加准确可靠地描述外界环境,从而提高系统决策的正确性。 核心：在于高精的时间以及空间的同步，时间上能够到10的负6次方，空间上能够到100m外3~5cm的误差。 要求： 硬件设备数量要足够,各种类型的传感器都配备,从而保证信息获取充分且冗余; 融合算法要足够优化,因为多传感器的使用会使需要处理的信息量大增,这其中甚至有相互矛盾的信息,如何保证系统快速地处理数据,过滤无用、错误信息,从而保证系统最终做出及时正确的决策十分关键。 目前多传感器融合的理论方法有贝叶斯准则法、卡尔曼滤波法、模糊集理论法、人工神经网络法等。 两种实现方式： 实例：定位中的多传感器融合 可以使用粒子滤波的方法关联已知地图和激光雷达测量的过程，粒子滤波可以在10cm的精度内达到实时定位的效果，在城市的复杂环境中尤为有效。 三、无人驾驶的决策1.目标物体行为预测定义：根据感知和定位的结果，实现目标物体（比如前方有一辆车）轨迹的预测和行为的分析 实现： 我们要实现一个动态的短时间内目标物体（前方车辆）的轨迹预测（蒙特卡洛采样、卡尔曼滤波） 根据目标物体一些状态或者行为的分析，实现一个中期的目标的轨迹预测 考虑红绿灯、标志牌等入射的一些信息，做一些规则的约束，对我们的目标物体行为预测和轨迹规划的一些曲线进行约束 2.自车行为决策定义：决策本车地行为（跟车还是换道） 实现： 基于规则的方法，列举有限驾驶状态，可保证行车安全 基于深度学习的方法，可学习优秀驾驶员的驾驶模式（输入端是我们一些方向盘的转角，一些踏板的开度，包含传感器的一些信息，生成学习模型） 基于深度学习及规则约束的智能驾驶混合决策与路径规划系统 3.自车路径规划定义：本车决定好是换道，就需要规划出一条比较平滑的路径 场景： 因为这里是让本车在前方有车的情况下换道，故实现的是局部避障。 实现：附加不同的因素来生成路径。 4.自车行为规划定义：在规划好的路径上来实现车辆的控制加上车辆的速度。 和普通车辆的控制并没有什么不同，两者都是基于一定的预设轨迹，考虑车辆当前姿态和此预设轨迹的误差并进行不断的跟踪反馈控制 四、无人驾驶的控制（以新能源车为例）车辆基于一定的预设轨迹（路径规划），考虑车辆当前姿态和此预设轨迹的误差并进行不断的跟踪反馈控制车辆的方向盘转角、速度大小等，形成闭环。即所谓的对于车辆加减速的纵向控制和对于车辆方向盘转角的横向控制，其执行机构分别为方向盘、加速踏板和刹车踏板，对应的实现机制就是线控驱动、线控制动、线控转向。 4.1线控驱动电子节气门：取消了踏板和节气门之间的机械结构，而是通过加速踏板位置传感器去检测油门踏板的位移，这个位移就代表了驾驶员的驾驶意图。把该信号传递给ECU，ECU根据其他传感器反馈回来的信息进行分析和计算得到最佳的节气门开度，然后再驱动节气门控制电机，节气门位置传感器检测节气门的实际开度，再把该信号反馈给ECU去实现整个节气门开度的闭环控制。 线控驱动实现： VCU的主要功能是实现扭矩需求的计算以及实现扭矩分配。因此只需要VCU开放速度控制接囗就能实现线控驱动。 4.2线控制动ABS： 轮速传感器：用于检测车轮的速度，这个速度信号会输入ABS ECU。 ABS ECU：接收轮速信号及其他信号，计算车轮的滑移率、车轮的加速度、减速度等信号，判断车轮是否有抱死的趋势从而输出控制指令给液压控制单元 ABS HCU（液压控制单元）：相当于执行器，接收电子控制单元的命令，执行压力调节的任务 线控制动的实现（以ABS为例） 加装一套执行机构和控制系统，控制系统可以和线控驱动的控制系统结合起来，比较目标车速和实际车速进行驱动油门踏板或者驱动制动踏板。（适用情况:ABS系统控制接口无法获得） 对ABS ECU进行接管控制，比如ABS ECU接收期望制动压力信号，直接驱动HCU。（适用于ABS的控制接口开放的情况。） 4.3线控转向汽车线控转向系统由方向盘总成、转向执行总成和主控制器(ECU)三个主要部分以及自动防故障系统、电源等辅助系统组成。 方向盘总成包括了方向盘、方向盘转角传感器、力矩传感器、方向盘回正力矩电机；将驾驶员的转向意图通过测量方向盘转角转换成数字信号，然后传递给主控制器，同时接受主控制器送来的力矩信号从而产生方向盘回正力矩提供给驾驶员相应的路感信息。 主控制器会对采集的信号进行分析处理判明汽车的运动状态，然后向方向盘回正力矩电机和转向电机发送指令控制两个电机的工作，保证在各种工况下都具有理想的车辆响应，从而减轻驾驶员的负担，同时控制器还可以对驾驶员的操作指令进行判别，判断在当前状态下驾驶员操作是否合理。 自动防故障系统：包括一些列的监控和实施算法，能够针对不同的故障形式和等级作出相应的处理从而实现最大限度的保持汽车正常行驶。 电源系统：承担着控制器、两个执行马达以及其他车用电机的供电任务。其中仅仅是前轮转角执行马达的功率就有500-800W，加上汽车上的其他电子设备，电源的负担就已经想相当沉重了。所以要保证电网在大负荷下稳定工作，电源的性能就显得十分重要了。 优点： 轻易实现主动转向的功能 获得比EPS更快的响应速度 滤掉路面上的激震信号 消除了撞车事故中转向柱后移引起伤害驾驶员的可能性 去掉了转向系功能模块间的机械连接，布置方式灵活,可以获得更大的驾驶员腿部空间。 五、小结 给出本文的思维导图，回忆一下~]]></content>
      <categories>
        <category>智能网联汽车</category>
      </categories>
      <tags>
        <tag>智能化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智能网联汽车——传感器与驾驶辅助]]></title>
    <url>%2F2019%2F12%2F11%2F%E6%99%BA%E8%83%BD%E7%BD%91%E8%81%94%E6%B1%BD%E8%BD%A6%E2%80%94%E2%80%94%E4%BC%A0%E6%84%9F%E5%99%A8%E4%B8%8E%E9%A9%BE%E9%A9%B6%E8%BE%85%E5%8A%A9%2F</url>
    <content type="text"><![CDATA[一、传感器1.汽车传感器特点①适应性强，耐恶劣环境 ②抗干扰能力强 ③稳定性和可靠性高 ④性价比高，大批量生产 2.种类2.1视觉传感器组成： 广义的视觉传感器主要由光源、镜头、感光传感器、模数转换器、图像处理器、图像存储器等组成。 狭义的视觉传感器是指感光传感器,它的作用是将镜头所成的图像转变为数字或模拟信号输出,是视觉检测的核心部件,主要有CCD感光传感器和CMOS感光传感器。 上图为CCD和CMOS的区别，目前车载摄像头使用的比较多的是CMOS感光传感器。 优缺点： 它的范围广、数据多，分辨率比较高，能够感知颜色。 使用环境严格，受光照条件影响较大。 分类： 特点：摄像机以一定的角度和位置安装在车辆上,为了摄像机所生成的图像像素坐标系中的点坐标与摄像机环境坐标系中的物点坐标之间的转换关系，需要进行摄像机标定。 应用： 2.2激光雷达原理：向外发射激光束，根据激光遇到障碍物后的折返时间、强弱程度等，计算目标与自己的相对距离、方位、运动状态及表面光学特性。激光光束可以准确测量视场中物体轮廓边沿与设备间的相对距离，这些轮廓信息组成所谓的“点云”并绘制出3D环境地图，精度可达到厘米级别。 组成： 优缺点： 因为用的是激光束定位，所以定位非常准确，可以达到cm级别的精度 全天候工作，不受光照条件的限制。 在浓雾天气下，激光可能会打到空气中的一些漂浮物质，漂浮物的直径如果比较大，可能会影响到激光雷达的准确度 价格较贵，谷歌的无人驾驶车辆使用的Velodyne64线激光雷达，价格高达7万美元以上。 应用：目前行业内的普遍观点认为，要实现Level3及以上级别的无人驾驶，必须配备有更高精度的激光雷达传感器。 供应商： Velodyne:在参加了两届 DARPA无人驾驶汽车挑战赛后,2007年开始专注研究激光雷达,用一款 Velodyne64线进入360°高性能激光雷达领域。 Ibeo：Ibeo是一家成立于1998年的公司,2000年被传感器制造商 Sick AG收购。2000年至2008年研发了激光扫描技术、并且开始了若干自动驾驶项目的尝试,2009年公司脱离 Sick AG独立,2010年和法雷奥合作开始量产可用于汽车的产品 Scala。 Quanergy：在2017年的CES上也推了自己的新产品:S2.号称是世界上第一软固态激光雷达,内部无任何转动机构,参数是8线,探测范围为10厘米-150米。 2.3毫米波雷达特点：工作频率通常选在30-300GHz频域（即波长为1-10mm）。结构简单，体积小，可以同时得到目标的相对距离和相对速度以及相对角速度信息。 优缺点： 可测距离远，与红外激光设备相比较，具有对烟尘雨雾良好的穿透、传播特性，不受雨雪等恶劣天气的影响，抗环境变化能力也比较强 存在定位不准确的问题，通常毫米波雷达对镜像的定位是比较准确的，但是它的侧向定位并不准确，并且会有很多的误判，分辨率低。 测量原理: 一般分为脉冲方式和调频连续波方式两种。 脉冲方式测量原理简单,但由于受技术、元器件等方面影响,实际应用中间很难实现。采用脉冲方式的毫米波雷达需在短时间内发射大功率信号脉冲,结构复杂,成本高。 大多数车载毫米波雷达都采用调频连续波的方式,其雷达结构简单，体积小，可以同时得到目标的相对距离和相对速度。 频段分类： 应用： 供应商： 2.4超声波传感器超声波雷达是一种运用超声波定位的雷达，由于检测距离比较短，通常把它布置在车身的两侧以及后端。 应用： 2.5车载传感器网络由安装在移动车辆上的无线传感器节点自组织形成的网络称为车载传感器网络,它可以实现V2V、V2I之间的通信。在车载网络中,车辆都安装了无线的车载单元(OBU),车辆通过这种设备采用短距离无线通信技术与其他车辆通信,也可以与路侧单元(RSU)通信。 二、驾驶辅助系统目前短期内实现完全的自动驾驶难度较大，就连自动驾驶技术最成熟的谷歌也没有做到量产级别。所以关注通往L4级及以上的L2/L3驾驶辅助更具有现实意义。 1.并线辅助系统（盲区检测系统） 概念：由于汽车后视镜本身存在视觉盲区,导致驾驶员无法及时准确地获知盲区内车辆的动向,因此车辆并线剐蹭或碰撞便成为常见的一种交通事故。因此人们想到了并线辅助装置，其原理很简单与我们常见的倒车雷达类似。当车辆在行驶中、在本车后方出现其他车辆时，系统将自动点亮该方向的车外后视镜上的指示标志以作提醒。 原理： 应用： 现在并线辅助的主要技术大致上分为影像及雷达两种，而后者又可分为24GHz及77GHz二种短波雷达频谱技术。 影像+雷达：本田品牌部分车型采用影像+雷达技术，在车侧后视镜安装摄像头和雷达，以影像方式监控车侧后方来车。 雷达：目前大部分车型采用雷达技术，将雷达感测器安装于车侧或后保险杠，可发出微波侦测车侧或车尾之来车。 2.车道偏离预警系统车道偏离预警系统是一种通过报警或振动的方式辅助驾驶员减少汽车因车道偏离而发生交通事故的系统。 原理： 当车道偏离预警系统开启时,系统利用安装在汽车上的图像采集单元获取车辆前方的道路图像,控制单元对图像分析处理,从而获得汽车在当前车道中的位置参数,车辆状态传感器会及时收集车速车辆转向状态等参数,控制单元的决策算法判定是否偏离。如果发生偏离，会触发相应的人机交互单元。 3.车道保持辅助系统车道保持辅助系统是在车道偏离预警系统的基础上对转向和制动系统协调控制,使汽车保持在预定的车道上行驶,减轻驾驶员负担,防止驾驶失误的系统。 利用视觉传感器采集道路图像,利用转速传感器采集车速信号,利用转向盘转角传感器采集转向信号,然后对车道两侧边界线进行识别,通过比较车线和车辆行驶方向,判断是否偏离行驶车道。如果发生偏离，会会触发转向盘操纵模块使车辆始终位于车道线内。 4.夜视系统夜视系统（Night Vision Device，NVD）是一种源自军事用途的汽车驾驶辅助系统。在这个辅助系统的帮助下，驾驶者在夜间或弱光线的驾驶过程中将获得更高的预见能力，它能够针对潜在危险向驾驶者提供更加全面准确的信息或发出早期警告。 4.1主动近红外照射该系统并不依赖热源，而是通过设备向外发射红外光束，照射目标，并将识别后的数据以图像的形式传递给驾驶者。此技术成本较低、效果较好，目前车辆上大多使用该技术实现夜视功能。 4.2微光夜视技术 该技术主要功能是利用夜间市区的普通可见光以及仅开启普通前大灯的前提，通过设备放大射入光线实现夜视能力。如果遇到强光源照射，可能会导致设备失灵，所以需要其它技术来弥补此短板。 4.3红外热成像技术也被称为红外线成像技术，将人们肉眼看不见的红外线转化成为可见光。因为绝对0度以上的物体都要辐射能量，不同温度的物体散发的热量不同，人类、动物和行驶的车辆要比周围环境散发的热量多。夜视系统就能收集这些信息，然后转变成可视的图像，把本来在夜间看不清的物体清楚的呈现在眼前，增加夜间行车的安全性。 优缺点： 解决夜间驾驶视野受限问题 解决夜间会车眩光问题 提升恶劣天气(大雾、雾霾、沙尘天气)下的驾驶视野 价格在三种方案中属最贵 发展： 成熟的应用热成像技术的产品，会集成目标检测算法，即具备行人、车辆防碰撞预警的功能，大大提高夜间行车的安全性。 5.疲劳预警系统 驾驶员疲劳预警系统是指驾驶员精神状态下滑或进入浅层睡眠时,系统会依据驾驶员精神状态指数分别给出语音提示、振动提醒、电脉冲警示等。 组成： 检测方法： ①基于驾驶员生理信号（客观性强，准确率高，但是与检测仪器有强相关系，这些检测方法基本都是接触性的检测，都会干扰到驾驶员的正常操作，影响行车的安全，此外不同人存在生理差异） 脑电信号检测 心电信号检测 肌电信号检测 脉搏信号检测 呼吸信号检测 ②基于驾驶员生理反应特征（表征疲劳的特征直观明显并且可以非接触策略，但是检测识别的算法是比较复杂的，且检测结果受光线变化以及个体生理状况的影响） 眼睛特征检测 视线方向检测 嘴部状态检测 头部位置检测 ③基于汽车行驶状态 基于方向盘的疲劳检测 汽车行驶速度检测 车道偏离检测 ④基于多特征信息融合检测方法 依据信息融合技术,将基于生理特征、驾驶行为和汽车行驶状态相结合是理想的检测方法,大大降低了采用单一方法造成的误警或漏警现象。 6.自适应巡航系统 6.1定速巡航 在驾车行驶过程中，驾驶员可以启动定速巡航，之后不需再踩油门，车辆既可按照一定的速度前进。定速巡航控制区域一般在方向盘后方或者集成在多功能方向盘上。开启定速巡航后，驾驶员通过定速巡航的手动调整装置，对车速进行小幅度调整而无需踩油门。当需要减速时，踩下刹车踏板即可自动解除定速巡航，驾驶员可再按钮重新以先前设定的速度恢复定速巡航。 ​ 原理：简单地说就是由巡航控制组件读取车速传感器发来的脉冲信号与设定的速度进行比较，从而发出指令由伺服器机械的来调整节气门开度的增大或减小，以使车辆始终保持所设定的速度。电子式多功能定速巡航系统摒除了拉线式定速巡航器的机械控制部分，完全采用精准电子控制，使控制更精确，避免了机械故障的风险。 在平缓的道路上，使用定速巡航可以保持车辆匀速行驶，减少耗油量；在长途驾驶时，定速巡航装置可以把驾驶员的脚从油门踏板上解放出来，从而减少疲劳程度；在有限速的路段，驾驶员可以运用定速巡航控制车速，不再看速度表，把注意力放在路面上，从而可以促进安全。 6.2自适应巡航自适应巡航控制系统（Adaptive Cruise Control，ACC），是一种智能化的自动控制系统，在巡航控制技术的基础上发展而来。除了可依照驾驶者所设定速度行驶外，还可以实现保持预设跟车距离以及随着车距变化自动加速与减速的功能。 总的来讲，自适应巡航系统由传感器、数字信号处理器以及控制模块三大部分组成。目前市场上常见的传感器有雷达传感器、红外光束以及视频摄像头等几种。信号处理器负责将传感器接收到的信息进行数字处理，最后由控制模块处理收集到的信息进行控制。系统判断需要减速时，最终由ABS系统对车轮实施制动或者变速箱采用降挡的办法，将车速降低。 一般自适应巡航需要车辆达到一定速度之后才可以启动，速度过低会自动解除。 6.3全速自适应巡航全速自适应巡航系统是由自适应巡航系统发展而来，相比自适应巡航，全速自适应巡航的工作范围更大，目前博世新一代系统可以在0-150km/h之间工作，而自适应巡航通常在40-150km/h。 7.自动泊车辅助系统概述：自动泊车辅助系统是利用车载传感器探测有效泊车空间并辅助控制车辆完成泊车操作的一种先进驾驶辅助系统。 组成： 分类：垂直停车、侧方停车和斜式停车，常见的是前两种，斜式停车只在部分车型中配备。另外部分高端车型除了可以自动将车辆停入车位，还可以实现自动驶离车位，进一步减轻了驾驶员工作，提高了便利性。 实现： 1）超声波传感器探测车位 ​ 利用车辆自带的超声波传感器，探测出适合的停车空间，然后车辆会自动接管方向盘来控制方向，将车辆停入车位。 2）摄像头识别车位 ​ 车辆能通过摄像头自动检索停车场的停车位置，并在空闲的停车位旁边自动开始驻车辅助操作。按下按钮后，方向盘将自行运作，只需按照提示切换档位，系统就能恰到好处地引导车辆进入停车位。 发展——自主泊车：对技术的要求要高上一个等级，是自动驾驶的一个核心分支，就如在2019年百度AI开发者大会上李彦宏宣称的解决“最后一公里”的停车问题，需要汽车按照规划好的路线自动行驶并完成泊车。 实现自主泊车需要更多的传感器、更复杂的感知、定位、规划等技术，在一些解决方案中，还需要车联网、停车场标准化建设以及数据的协同和互通。 三、总结 给出本文的思维导图，回忆一下~]]></content>
      <categories>
        <category>智能网联汽车</category>
      </categories>
      <tags>
        <tag>传感器</tag>
        <tag>驾驶辅助</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智能网联汽车——概述]]></title>
    <url>%2F2019%2F12%2F10%2F%E6%99%BA%E8%83%BD%E7%BD%91%E8%81%94%E6%B1%BD%E8%BD%A6%E2%80%94%E2%80%94%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[一、定义与内涵一般我们说“自动驾驶”或者“无人驾驶”，更普适的说法是“智能车”（智能汽车）。其实”智能汽车“与下图看到的“智能网联汽车”是类似的概念，一般不会刻意强调他们的区别。简单来说，智能汽车也就代表了智能网联汽车。 1.智能汽车智能汽车的”智能“有两种模式： 自主式智能汽车（Autonomous Vehicle）：指依靠自车所搭载的各类传感器对车辆周围环境进行感知，依靠车载控制器进行决策和控制并交由底层执行，实现自动驾驶。 网联式智能汽车（Connected Vehicle）：就是车辆通过V2X通信的方式获取外界的环境信息并帮助车辆进行决策与控制。 这两种智能的模式都在各自往前发展，同时也在融合，其融合的结果就是智能网联汽车。 并且当”智能“和“网联”一词一起出现，“智能”一般做狭义理解，即”自主式智能“；当”智能“单独出现，一般做广义理解，即涵盖了”自主式智能“和“网联式智能”。 所以才有上文的智能汽车也就代表了智能网联汽车一说。 2.车联网以车内网、车际网和车云网为基础，按照约定的通信协议和数据交换标准实现车与X（人、车、路、云等系统）之间进行无线通讯和信息交换的大系统网络，是能够实现智能交通管理、智能动态信息服务和车辆智能化控制的一体化网络。 3.智能网联汽车到现在你应该很清楚智能网联汽车由智能和网联两部分组成，现在就让我们给智能网联汽车下一个明确的定义吧。 定义：搭载先进的车载传感器、控制器、执行器的车辆，并融合了现代的通信与网络技术。实现了车与X（人、车、路、云等系统）之间进行智能化的信息交换、共享，具备复杂的环境感知、智能决策、协同控制等功能，可综合实现安全、高效、舒适、节能行驶，并最终实现替代人来操作的新一代汽车。 二、分级一辆汽车从需要驾驶员全神贯注驾驶到不需要驾驶员可以自行驾驶，可以想象不是一蹴而就的，会经历几个阶段。 在车辆的智能化分级中，工业界目前采用最多的标准是由国际汽车工程师协会（SAE）制定的。 在L0级时，车辆没有辅助系统，驾驶员需要全神贯注，手眼并用。 在L1级时，车辆有横向或者纵向辅助系统，但驾驶员仍需要集中注意力，手眼并用。 在L2级时，车辆有横向和纵向辅助系统，驾驶员仍需要观察环境，但可以临时解放手和眼。 在L3级时，车辆在紧急情况下会发出请求驾驶员接管，驾驶员全程需要有接管意识，驾驶员解放手和眼。 在L4级时，车辆即使在紧急情况下（可以自己处理）也不会发出请求驾驶员接管，驾驶员不需要有接管意识，可以解放手和大脑。 在L5级时，车辆可以实现完全自动驾驶，车辆不需要驾驶员。 此外，还可以使用智能化和网联化两个维度对智能网联汽车进行分级。 特别地，中国是从智能化和网联化这两个维度进行分级的。 智能化： 网联化： 三、必要性此处，或许有人生出这样的疑问。好好地车辆怎么就不需要驾驶员了，是否普及无人驾驶之后驾驶员就丧失了驾驶乐趣。当然，无人驾驶是有其现实意义的。 道路安全:交通事故率可降低到目前的1%。 出行效率:车联网技术可提高道路流量10%,C-ACC（协同式自适应巡航控制）占市场份额90%时,交通效率会提高80%。 能源节省:协同式交通系统可提高燃油经济性2040%,高速公路编队行驶可降低油耗1015%。 经济效益:以美国为例,自动驾驶汽车占市场份额90%时,每年将带来2000亿美元以上的经济效益,同时将带动机械、电子、通信、互联网等相关产业的快速发展。 国防意义:无人驾驶战斗车辆。 生活方式：减轻驾驶负担,车辆共享,便捷出行,降低驾驶者的门槛 四、技术体系既然无人驾驶如此“厉害”，那么实现这一无人驾驶系统，势必包含很广的技术范畴，同时这些技术必定形成技术体系。 1.车辆/设施关键技术具体包括如下： 2.信息交互关键技术具体包括如下： 五、小结 给出本文的思维导图，回忆一下~]]></content>
      <categories>
        <category>智能网联汽车</category>
      </categories>
      <tags>
        <tag>智能化</tag>
        <tag>网联化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu环境下安装GPU版TensorFlow-显卡RTX2060]]></title>
    <url>%2F2019%2F11%2F29%2FUbuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%AE%89%E8%A3%85GPU%E7%89%88TensorFlow-%E6%98%BE%E5%8D%A1RTX2060%2F</url>
    <content type="text"><![CDATA[在上一篇文章《Linux-Windows10双系统安装》中已经成功安装了双系统，本篇将在此基础上继续搭建深度学习环境。 一、背景系统：Ubuntu16.04 GPU: RTX 2060 tensorflow版本与cuda和cudnn的对应关系： https://tensorflow.google.cn/install/source 先前在Windows里面搭建深度学习环境时选择的就是tensorflow-gpu==1.12.0，可以进行模型的训练。故此处选择的cuda和cudnn如下。 二、安装CUDA1）去官网点击打开链接下载CUDA，需要注册一个账号（上外网登陆下载比较快） 注意：下载runfile（local）文件 2）下载完成以后，找到CUDA9.0所在位置（我的是放在下载目录下），终端输入：. 12sudo chmod +x cuda_9.0.176_384.81_linux.runsudo ./cuda_9.0.176_384.81_linux.run 安装过程中会有几个选项需要确认： 注意：不要安装cuda自带的显卡驱动（第一步不要安装，因为在上一篇文章已经安装完），其他的直接选y，路径输入新的路径. 3）配置环境 安装完成以后，需要把cuda路径添加到当前用户的配置文件里： 1sudo vim ~/.bashrc 将安装路径添加到文件末尾： 12export PATH=/usr/local/cuda-9.0/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64:$LD_LIBRARY_PATH 然后执行如下命令使路径生效： 1. ~/.bashrc 4）验证 终端输入： 1nvcc -V 可以看到cuda的版本信息： 接着尝试运行一下cuda中自带的例子： 123cd /usr/local/cuda-9.0/samples/1_Utilities/deviceQuerysudo make./deviceQuery 可以看到输出成功： 三、安装cuDNN官网下载cuda对应版本的cudnn点击打开链接。 注意：下载cuDNN v7.0.5 Library for Linux 下载完成以后将其解压到Cuda的目录当中，依次执行如下命令： 1234tar -xzvf cudnn-9.0-linux-x64-v7.tgzsudo cp cuda/include/cudnn.h /usr/local/cuda/includesudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn* 四、安装 Tensorflow1)切换python版本 Ubuntu16.04 LTS环境下有两个版本的Python，分别是Python3.5和Python2.7。Ubuntu默认的是python 2.7版本，我们需要切换默认版本为Python3.5。 如下就是设置python3.5的路径，并设置优先级为200，默认优先级最高的优先使用 1sudo update-alternatives --install /usr/bin/python python /usr/bin/python3.5 200 去官网复制get-pip.py脚本文件:点击打开链接。 复制好以后，输入如下指令，安装pip 12cd ~/下载/sudo python get-pip.py 终端键入 pip -V 查看，是否正确指向python3.5包的路径并且结尾是否是python3.5，无误后就可以安装其他包什么的了。 3)利用pip安装Tensorflow： 1sudo pip install tensorflow-gpu==1.12.0 4)如果上一步因为网速等原因下载失败，就直接从官方库里下载release包（.whl文件）进行安装。不过需要先从下面选择自己对应的系统与环境。 下载好安装包以后，进入到对应下载目录下直接安装即可： 12cd ~/下载/sudo pip install tensorflow_gpu-1.12.0-cp35-cp35m-manylinux1_x86_64.whl 五. 测试1）进入python，键入以下代码 1234import tensorflow as tfhello = tf.constant('Hello Tensorflow')sess = tf.Session()print(sess.run(hello)) 如下图所示，则表明Tensorflow安装成功 注：红框里的两行代码参见博客点击打开链接 2) 查看tensorflow版本 在python环境中输入： 123import tensorflow as tftf.__version__ #查看版本tf.__path__ #查看路径 以上。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>GPU</tag>
        <tag>Linux</tag>
        <tag>RTX2060</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-Windows10双系统安装]]></title>
    <url>%2F2019%2F11%2F27%2FLinux-Windows10%E5%8F%8C%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[在Windows10系统上搭建完深度学习环境用于无人驾驶中的目标检测后，想在Linux系统上再尝试一下。由于VMware虚拟机安装的Linux系统不支持物理硬件，所以需要一步到位安装一个双系统。本文介绍如何安装双系统以及装完系统后的输入法和英伟达显卡驱动配置。 1.安装双系统详细操作参考https://blog.csdn.net/fanxueya1322/article/details/90205143 详细操作参考https://blog.csdn.net/lzq_103/article/details/84197486 1.1 U盘启动盘的制作将Ubuntu系统ISO 格式镜像文件借助Rufus工具加载进U盘，从而将U盘制作成启动盘。 1.2 分配磁盘空间留出磁盘空间用于安装ubuntu 系统。 这里有一个巨坑，容易误操作，把主分区转成了动态分区。动态分区的箭头处颜色为绿色，然后不支持磁盘空间分配，即没有分区的磁盘概念。就算留出100G的存储空间，在下一步（1.3）分区时，系统是检测不到这100G存储的。所以在出现系统提示要不要转为动态磁盘时，一定要选择否. 如果已经转为动态磁盘，经过多种方法的尝试，最终使用AOMEI Dynamic Disk Manager软件可以将动态磁盘转为基本磁盘。 1.3 分区 目录 建议大小 格式 描述 / 150G-200G ext4 根目录 /tmp 5G左右 ext4 系统的临时文件，一般系统重启不会被保存。 /boot 1G左右 ext4 系统引导起始位置，建议：应该大于400MB或1GB Linux的内核及引导系统程序所需要的文件，比如 vmlinuz initrd.img文件都位于这个目录中。在一般情况下，GRUB 或 LILO 系统引导管理器也位于这个目录；启动撞在文件存放位置，如kernels，initrd，grub。 /home 尽量大些 ext4 用户工作目录；个人配置文件，如个人环境变量等；所有账号分配一个工作目录。 swap 物理内存的两倍 交换空间 交换空间：交换分区相当于Windows中的“虚拟内存”，如果内存低的话（1-4G），物理内存的两倍，高点的话（8-16G）要么等于物理内存，要么物理内存+2g左右 1.4 安装分区完成准备安装。 1.5 启动项修改安装好双系统后你的电脑开机时可能默认还是选择 windows 启动，可以通过EasyBCD工具对启动顺序进行修改。 2.安装搜狗输入法参考自https://blog.csdn.net/leijiezhang/article/details/53707181 Linux自带的输入法不是太好用，而且正常在Windows系统里面使用的是搜狗输入法，并且搜狗输入法发行了Linux版本，故没有任何理由不在Linux系统上安装一个 搜狗输入法。 1）下载搜狗输入法（注意32位和64位） https://pinyin.sogou.com/linux/?r=pinyin 2）按键Ctr+Alt+T打开终端，输入以下命令切换到下载文件夹: 1cd ~/下载/ 3) 查看是否下载完成 1ls 4) 安装输入法 1sudo dpkg -i sogoupinyin_2.3.1.0112_amd64.deb 1234567891011121314#会报错dpkg: 处理软件包 sogoupinyin (--install)时出错： 依赖关系问题 - 仍未被配置正在处理用于 mime-support (3.60ubuntu1) 的触发器 ...正在处理用于 libglib2.0-0:i386 (2.56.1-2ubuntu1) 的触发器 ...覆盖文件 /usr/glib-2.0/schemas/50_sogoupinyin.gschema.override 中指定的方案 org.gnome.settings-daemon.plugins.xsettings 中没有键 Gtk/IMModule；忽略对此键的覆盖。正在处理用于 libglib2.0-0:amd64 (2.56.1-2ubuntu1) 的触发器 ...覆盖文件 /usr/glib-2.0/schemas/50_sogoupinyin.gschema.override 中指定的方案 org.gnome.settings-daemon.plugins.xsettings 中没有键 Gtk/IMModule；忽略对此键的覆盖。正在处理用于 gnome-menus (3.13.3-11ubuntu1) 的触发器 ...正在处理用于 desktop-file-utils (0.23-1ubuntu3) 的触发器 ...正在处理用于 shared-mime-info (1.9-2) 的触发器 ...正在处理用于 hicolor-icon-theme (0.17-2) 的触发器 ...在处理时有错误发生： sogoupinyin 5）解决依赖问题 1sudo apt-get -f install 6)再次安装 1sudo dpkg -i sogoupinyin_2.3.1.0112_amd64.deb 7)从桌面右上角系统设置里面点击“语言支持”，从跳出来的第一个对话框选择“安装”按钮，然后就开始安装，期间会提示输入密码。 8）重启 1reboot 9）点击右上角键盘标志，选择“配置当前输入法”，然后点击“+”，添加合适的输入法，我这里最终选择的是“键盘-英语（美国）+搜狗拼音“。这样，就和在Windows10上使用搜狗输入法时一样，按”Shift键“就可以切换中文和英文输入法。 3.安装英伟达显卡驱动参考自https://blog.csdn.net/DongZhuoHui/article/details/97913269 一方便是由于配置深度学习环境的需要，另一方面是由于没有显卡驱动的话，整个界面的分辨率有限，影响观感。所以就算不配置深度学习环境，安装显卡驱动也是必要的。 1）安装环境 操作系统：Ubuntu 16.04 LTS 显卡：NVIDIA GeForce RTX 2060 2）在正式入坑深度学习环境搭建之前，先要确定两点。 tensorflow版本与cuda和cudnn的对应关系： https://tensorflow.google.cn/install/source 先前在Windows里面搭建深度学习环境时选择的就是tensorflow-gpu==1.12.0，可以进行模型的训练。故此处选择的cuda和cudnn如下。 CUDA要求的linux下的Driver Version： https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html 根据CUDA为9.0，故只要驱动版本大于等于384.81就可以了，下面介绍安装最新的驱动版本为440.36完全满足要求。 3) 打开NVIDIA驱动官网，选择最新的一个驱动程序，下载 注意： 版本选择Linux，不要下载Windows10 64bits 4）禁用nouveau驱动 打开文件 1sudo vim /etc/modprobe.d/blacklist.conf 在文件末尾添加以下几行命令 blacklist nouveau blacklist rivafb blacklist rivatv blacklist nvidiafb options nouveau modeset=05) 更新Linux系统内核 1sudo update-initramfs -u 6) 重启并检查nouveau驱动是否成功被禁 12rebootlsmod | grep nouveau 重启电脑后，打开终端输入命令以查看nouveau驱动是否成功被禁，命令无返回则是成功禁用。 7) 安装驱动.run文件 12cd ~/下载/sudo sh NVIDIA-Linux-x86_64-440.36.run -no-x-check -no-nouveau-check -no-opengl-files 参数解释： –no-x-check：表示安装驱动时不检查X服务（图形接口服务），如果没有关闭图形界面则必须加上，否则反之。 –no-nouveau-check：表示安装驱动时不检查nouveau驱动，这也是非必需的，因为我们已经在前面步骤中禁用驱动。 –no-opengl-files：表示只安装驱动文件，不安装OpenGL文件。这个参数不可省略，否则会导致登陆界面死循环，英语一般称为”login loop”或者”stuck in login”。 8) 安装进行时的选项 选项 选择 The distribution-provided pre-install script failed! Are you sure you want to continue? Continue Unable to find a suitable destination to install 32-bit compatibility libraries. Your system may not be set up for 32-bit compatibility. 32-bit compatibility files will not be installed; if you wish to install them, re-run the installation and set a valid directory with the –compat32-libdir option. OK Would you like to run the nvidia-xconfigutility to automatically update your x configuration so that the NVIDIA x driver will be used when you restart x? Any pre-existing x confile will be backed up. Yes Your X configuration file has been successfully updated. Installation of the NVIDIA Accelerated Graphics Driver for Linux-x86_64 (version: 430.40) is now complete. OK 9)在命令行中输入 1nvidia-smi 如出现以下画面，即证明已正确安装一半。 10）重启电脑 1reboot 11) 接着在命令行中输入 1nvidia-settings 出现以下界面，代表显卡安装完全正确。 然后整个电脑桌面会变得非常清晰，图标也会相应的变小。可以进行相应的图标大小设置达到使用习惯即可。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Linux系统</tag>
        <tag>双系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读取天气信息，并通过QQ邮箱每天给好友定时发送]]></title>
    <url>%2F2019%2F10%2F27%2F%E8%AF%BB%E5%8F%96%E5%A4%A9%E6%B0%94%E4%BF%A1%E6%81%AF%EF%BC%8C%E5%B9%B6%E9%80%9A%E8%BF%87QQ%E9%82%AE%E7%AE%B1%E6%AF%8F%E5%A4%A9%E7%BB%99%E5%A5%BD%E5%8F%8B%E5%AE%9A%E6%97%B6%E5%8F%91%E9%80%81%2F</url>
    <content type="text"><![CDATA[1.结果假设你的好友和你不在一个城市，他不怎么经常关注天气情况而你又想对他表示关心，那么这篇推送就非常适合你。先来看下结果吧，你的好友会根据你的设置每天在某一时间收到一封你发给他的邮件，邮件内容如下： 类似的根据不同的API接口，还可以每天定时发送一封关于金山词霸每日一句、睡前小故事的邮件等等。 2.前提第一条，你得有一个集成开发环境来运行程序，由于我使用的是Python语言，所以需要类似PyCharm或者Spyder的开发环境。 第二条，你需要看到我这篇文章，很明显这一条你已经做到了（手动滑稽）。 3.正文我对实现上述功能的理解大致可以总结为这么一句话：我要向谁通过什么接口在每天什么时间发送什么信息，并且得到什么适当的反馈，证明已经发送成功。所以程序大致可以分为以下几个步骤。 3.1 我你需要知道自己的QQ号，这个没什么问题吧。 3.2谁这里有人会困惑，我到底该发给谁呢，想什么呢，当然是发给你的好基友啊，不然还发给你喜欢的对象（手动滑稽） 3.3接口这里使用的是QQ邮箱的接口，有个比较重要的参数是QQ邮箱的授权码。为此，我们得先获取该授权码。 1）登录自己的QQ号，并进入QQ邮箱界面。 2）点击设置，然后在邮箱设置里面选择账户。 3）然后向下拖动，点击生成授权码，按照相应的提示做就可以了。 4）如果没有温馨提示一栏，就点击开启POP3/SMTP服务。 5)授权码是一串小写英文字符串。 3.4定时顾名思义，就是在每天的某一时刻发送邮件，只需要更改时间参数即可。 3.5信息这里的信息指得就是天气信息。这里有两点要注意： 1）你需要获得城市编号，这个可以通过中国天气网官网（http://www.weather.com.cn/），然后搜索城市名称获得。 2）替换掉参数。 3）需要获得具体关于天气的信息。打开中国天气网后，按F12即可进入开发者工具，然后找到对应的关键词。 4）程序参数来源。 5）在return部分，除了截图中的内容外，可以自由发挥，加一些俏皮的内容。 3.6反馈当我们想知道对方是否收到邮件时，除了问对方外，还可以自己做一些设置。 这里，我设置的是当程序运行时，会有start打印出来，当发送成功时，会打印发送完成。 4.改进由于本程序是在本地运行的，所以得一直使程序在运行状态才行。后面可以考虑将程序运行在云服务器上。]]></content>
      <categories>
        <category>小项目</category>
      </categories>
      <tags>
        <tag>天气</tag>
        <tag>QQ邮箱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Hexo和GitHub免费搭建个人博客网站]]></title>
    <url>%2F2019%2F10%2F09%2F%E5%9F%BA%E4%BA%8EHexo%E5%92%8CGitHub%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99%2F</url>
    <content type="text"><![CDATA[基于Hexo和GitHub免费搭建个人博客网站1.引言阮一峰大神说喜欢写博客的人，会经历三个阶段。 第一阶段，刚接触博客觉得很新鲜，试着选择一个免费空间来写。比如CSDN、博客园。 第二阶段，发现免费空间限制太多，就自己购买域名和空间，搭建独立博客。比如阿里云、腾讯云。 第三阶段，觉得独立博客的管理太麻烦，最好在保留控制权的前提下，让别人来管，自己只负责写文章。比如基于Hexo和GitHub免费搭建个人博客网站。 Github提供了Pages功能，只要将写好的Markdown文章提交到Github上托管，即可生成独立博客，而且提供几乎不限流量的存储空间，一切都是免费的。一旦搭建好，则只需要负责写文章就行了，不需要定期维护。 Hexo 是一个基于Node.js的静态博客框架。Hexo使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 之前介绍过写Markdown文章的神器Typora，今天就让我们来关注具体搭建个人博客网站。 本文是针对Windows10系统，其他诸如Mac系统还有Linux系统请参考百度相关内容。 2.Hexo的搭建步骤2.1 安装Git BashGit Bash用来管理你的Hexo博客文章，并将文章上传到GitHub平台的工具。 1）具体的安装过程就是一直点击next，直到出现install，点击install，安装完成后点击finish，然后在任意文件夹下右键打开Git Bsah Here输入命令git --version，查看有没有安装成功。 注意：右键点击Git Bash Here 如果出现下图，则说明安装路径中有中文（软件安装）。 2.2 安装Node.jsHexo是基于Node.js编写的，所以需要安装该软件。从官网下载，然后安装即可，直接选择LTS（长期支持版）版本的。 1）安装完成后，打开管理员命令行输入命令node -v以及npm -v，查看有没有安装成功。 2.3 安装Hexo1）创建一个文件夹Blog，然后进入该文件夹右键Git Bash Here打开。输入命令npm install -g hexo-cli，至此就安装完成了。 2）安装完成后，还需要初始化hexo，即执行如下命令： 12hexo initnpm install 3）新建完成后，在Blog文件夹下会出现许多新的文件，目录如下： node_modules：是依赖包 scaffolds：命令生成Markdown文章等的模板 source：用命令创建的各种Markdown文章 themes：主题文件夹，Hexo 会根据主题来生成静态页面。 _config.yml：站点配置文件，您可以在此配置大部分的参数。 db.json：source解析所得到的 package.json：项目所需模块项目的配置信息 4）输入如下命令，然后在浏览器中输入localhost:4000(本地查看)即可查看用hexo生成的博客了。 12hexo ghexo s 使用ctrl+c可以把服务关掉。 5）至此，Blog文件夹下会多出两个目录 .deploy_git public 注意：出现bash： npm： command not found错误。 网上试了各种办法，最后重启一下电脑就没有问题了。 3.Hexo关联到GitHub静态页面已经有了，我们还需要将其部署到GitHub上。一来我们搭建博客想要别人可以（以一个链接）访问到我们的博客，二来我们需要有个托管平台，这样我们就可以关注博客内容本身而不是麻烦的管理。 3.1 GitHub创建个人仓库1）首先注册一个GitHub账号 2）然后点击New，新建仓库 按照下图所示操作后，点击Create repository即创建成功。 3.2生成SSH添加到GitHub1）在git bash输入如下命令 12git config --global user.name "yourname"git config --global user.email "youremail" 这里的yourname输入你的GitHub用户名，youremail输入你GitHub的邮箱。这样GitHub才能知道你是不是对应它的账户。 2)然后创建SSH,一路回车 1ssh-keygen -t rsa -C "youremail" 这个时候它会告诉你已经生成了.ssh的文件夹。在你的电脑中找到这个文件夹后可以发现有两个文件（一般位于C盘）。 3)然后在GitHub的setting中，找到SSH keys的设置选项，点击New SSH key把id_rsa.pub里面的信息复制进去。 SSH，简单来讲，就是一个秘钥，其中，id_rsa是你这台电脑的私人秘钥，不能给别人看的，id_rsa.pub是公共秘钥，可以随便给别人看。把这个公钥放在GitHub上，这样当你链接GitHub自己的账户时，它就会根据公钥匹配你的私钥，当能够相互匹配时，才能够顺利的通过git上传你的文件到GitHub上。 3.3 将Hexo部署到GitHub1）用Notepad++打开站点配置文件 _config.yml。搜索关键词deploy。将YourgithubName对应的修改为你的GitHub用户名。 1234deploy: type: git repo: https://github.com/YourgithubName/YourgithubName.github.io.git branch: master 2）这个时候需要先安装deploy-git ，也就是部署的命令,这样你才能用命令部署到GitHub。 1npm install hexo-deployer-git --save 3） 1hexo n bolgname 然后就会发现在Blog/source/_posts目录下出现一个名为blogname的Markdown文章，用Typora软件打开，文件最上方以 --- 分隔的区域，有用于指定个别文件的变量的Front-matter。我是用的个参数具体包括：title，categories，tags，date。 用Typora打开模板文件Blog/scaffolds/post.md文件，可以进行相应的设置。 4) 然后使用两个命令即可将你写好的文章（位于Blog/source/_posts文件夹下）部署到GitHub。 123hexo g # 生成静态文章hexo d # 部署文章hexo s # 可以先从本地（localhost:4000）预览效果，在部署文章到GitHub上 注意： 1）输入hexo d命令后需要你输入username和password。 得到下图就说明部署成功了，过一会儿就可以在http://yourname.github.io 这个网站看到你的博客了。 2）在上面第三步中提到Front-matter，其中除了我使用的几个参数外，还有其他参数。 3）另外，在上面第三步中提到Blog/scaffolds目录，其实该目录下一共有三个文件，刚刚只提到了post.md，还有另外两个文件。 其实这三个文件是对应三种不同的布局（layout）。 123451.hexo n [layout] &lt;tittle&gt; # 之前在创建文章时，layout默认是post，名为title的文章在Blog/source/_posts文件夹下2.hexo n draft &lt;tittle&gt; # draft是草稿的意思，名为title的文章在Blog/source/_drafts文件夹下，hexo s --draft # 草稿写好文章后，可以在本地开启预览hexo p draft &lt;tittle&gt; # 将名为title的文章发送到Blog/source/_posts文件夹下3.hexo new page board # 系统会自动在source文件夹下创建一个board文件夹，以及board文件夹中的index.md，这样你访问的board对应的链接就是http://xxx.xxx/board 4.Hexo的基本页面配置4.1 hexo基本配置用Notepad++打开站点配置文件 _config.yml，我们可以在 _config.yml 中修改大部分的配置。 1）网站 参数 描述 title 网站标题 subtitle 网站副标题 description 网站描述 author 您的名字 language 网站使用的语言 timezone 网站时区，Hexo 默认使用您电脑的时区。 2）网址 参数 描述 默认值 url 网址 root 网站根目录 permalink 文章的永久链接格式 :year/:month/:day/:title/ permalink_defaults 永久链接中各部分的默认值 url：网站域名 permalink：分享某篇文章后在对方的网页中显示的域名。 3）其他设置默认就可以了，更多基本设置可以参考Hexo官网配置内容。 4.2 更换主题1）系统默认给的主题是landscape，并且在hexo官网还提供了270个主题，可以先预览选择一个主题。 比如我个人喜欢next主题，那么怎么应用该主题呢。 2）点击右上角从GitHub上面下载下来，然后再把整个文件夹放到Blog/themes目录下。 3）然后将Blog目录下_config.yml文件中的主题改为next。 4）进入Blog/themes/next目录下，里面也有一个配置文件_config.yml，这个配置文件是修改你整个主题的配置文件。 menu（菜单栏） 文件搜索关键词menu。 search(搜索框) 1npm install hexo-generator-searchdb --save # 安装插件 修改站点配置文件（位于Blog/_config.yml）中添加内容 12345search: path: search.xml field: post format: html limit: 10000 avatar（头像） 文件搜索关键词avatar。将准备好的头像放置在Blog/themes/next/source/images目录下。 social（社交） 文件搜索关键词social。 进度条 搜索关键词b2t，将图中两条均改为true。即可得到位于左侧栏的top按钮，可以实时显示阅读进度。 5.Hexo的高端页面配置（比较灵活）hexo添加各种功能，主要包括 搜索的SEO 阅读量统计 访问量统计 评论系统等 可以自由选择添加，具体的设置可以参考这篇文章。 6.其他本文是基于Node.js的静态博客框架Hexo，其他的博客各位如果感兴趣可以参考网络资源进行相应的博客搭建。下面列出了常见的静态博客。 6.1静态博客 jekyllrb.com（基于ruby） gohugo.org(基于go) vuepress.vuejs.org（基于vuejs） solo.b3log.org(基于java)]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[无人驾驶环境感知--基于深度学习的车道线检测]]></title>
    <url>%2F2019%2F09%2F17%2F%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6%E7%8E%AF%E5%A2%83%E6%84%9F%E7%9F%A5--%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%BD%A6%E9%81%93%E7%BA%BF%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[前言前面提到了使用基于霍夫变换的车道线检测，这种方法虽然操作简单，不需要对样本进行标注，但是相应的也有其局限性。 没有学习过程，对环境变化没有适应性 兴趣区域固定，且取决于摄像机、车辆相对于道路的位置 各种图像处理过程的参数、阈值等需要提前设定，调整不灵活 处理速度慢，4.5-6帧/秒，正常读取速度为30帧/秒 故本文介绍处理准确率更高，对环境适应性更好的基于深度学习的车道线检测。 正文1.数据集功能：用于标注，喂给设计好的模型进行训练。 1）从12个视频（包括一天不同时间、不同天气、不同交通状况和弯曲道路）中选取21054个图像 17.4%是夜晚清晰场景，16.4%是早上雨天场景，66.2%是下午阴天场景 26.5%是直线或者近乎直线道路，30.2%是混合或者略弯的道路，43.3%是相当弯曲的道路（道路还包括不同的区域，比如在修路段和十字路口） 2）滤去模糊和遮挡的图像，最终从中选取了14235个图像 3）在从10个中选取1个（视频相邻帧过于近似），最终含有1420个图像 4）从Udacity ‘s Advanced Lane Lines project取一些图，加上一些其它处理，最终有1978个实际图像 5）最终使用的图像：旋转处理后，扩展为6382个图像，水平翻转后，最终12764个图像 6）原始图像尺寸为80乘160乘3，对应的图片和标注结果如下图。 2.深度学习模型功能：根据标注的数据结合设计好的模型进行训练，一般训练会花费很多时间，所以为了方便实际工程使用会在训练后生成.h5文件（基于Keras框架）。 代码一：导入相关库123456789101112import numpy as npimport picklefrom sklearn.utils import shufflefrom sklearn.model_selection import train_test_split# 导入Keras的一些库及函数from keras.models import Sequentialfrom keras.layers import Activation, Dropout, UpSampling2Dfrom keras.layers import Conv2DTranspose, Conv2D, MaxPooling2Dfrom keras.layers.normalization import BatchNormalizationfrom keras.preprocessing.image import ImageDataGeneratorfrom keras import regularizers 代码二：数据预处理123456789101112131415161718192021# Load training imagestrain_images = pickle.load(open("full_CNN_train.p", "rb" ))# Load image labelslabels = pickle.load(open("full_CNN_labels.p", "rb" ))# Make into arrays as the neural network wants thesetrain_images = np.array(train_images)labels = np.array(labels)# 标准化标签 - 训练图像在神经网络的入口标准化labels = labels / 255# Shuffle images along with their labels, then split into training/validation setstrain_images, labels = shuffle(train_images, labels)# Test size may be 10% or 20%X_train, X_test, y_train, y_test = train_test_split(train_images, labels, test_size=0.1) print( 'X_train.shape: ' + str(X_train.shape))print( 'X_test.shape: ' + str(X_test.shape))print( 'Y_train.shape: ' + str(Y_train.shape))print( 'Y_test.shape: ' + str(Y_test.shape)) 注意： 1）在机器学习中，我们常常需要把训练好的模型存储起来，这样在进行决策时直接将模型读出，而不需要重新训练模型，这样就大大节约了时间。Python提供的pickle模块就很好地解决了这个问题，它可以序列化对象并保存到磁盘中，并在需要的时候读取出来，任何对象都可以执行序列化操作。 Pickle模块中最常用的函数为：pickle.load(file) 函数的功能：将file中的对象序列化读出。 2）train_test_split()是sklearn.model_selection中的分离器函数，用于将数组或矩阵划分为训练集和测试集，函数样式为：X_train, X_test, y_train, y_test = train_test_split(train_images, labels, test_size, random_state，shuffle) test_size：浮点数，在0 ~ 1之间，表示样本占比（test_size = 0.3，则样本数据中有30%的数据作为测试数据，记入X_test，其余70%数据记入X_train，同时适用于样本标签）；整数，表示样本数据中有多少数据记入X_test中，其余数据记入X_train random_state：随机数种子，种子不同，每次采的样本不一样；种子相同，采的样本不变（random_state不取，采样数据不同，但random_state等于某个值，采样数据相同，取0的时候也相同，这可以自己编程尝试下，不过想改变数值也可以设置random_state = int(time.time())） shuffle：洗牌模式，shuffle = False，不打乱样本数据顺序；shuffle = True，打乱样本数据顺序 3）打印： 1234X_train.shape: (11487, 80, 160, 3)X_test.shape: (1277, 80, 160, 3)Y_train.shape: (11487, 80, 160, 1)Y_test.shape: (1277, 80, 160, 1) 代码三：训练时的一些超参数12345# Batch size, epochs and pool size below are all paramaters to fiddle with for optimizationbatch_size = 128epochs = 10pool_size = (2, 2)input_shape = X_train.shape[1:] # (80, 160, 3) 代码四：深度学习模型1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677### 神经网络架构 ###model = Sequential()# Normalizes incoming inputs. First layer needs the input shape to workmodel.add(BatchNormalization(input_shape=input_shape))# Below layers were re-named for easier reading of model summary; this not necessary# Conv Layer 1model.add(Conv2D(8, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv1'))# Conv Layer 2model.add(Conv2D(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv2'))# Pooling 1model.add(MaxPooling2D(pool_size=pool_size))# Conv Layer 3model.add(Conv2D(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv3'))model.add(Dropout(0.2))# Conv Layer 4model.add(Conv2D(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv4'))model.add(Dropout(0.2))# Conv Layer 5model.add(Conv2D(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv5'))model.add(Dropout(0.2))# Pooling 2model.add(MaxPooling2D(pool_size=pool_size))# Conv Layer 6model.add(Conv2D(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv6'))model.add(Dropout(0.2))# Conv Layer 7model.add(Conv2D(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv7'))model.add(Dropout(0.2))# Pooling 3model.add(MaxPooling2D(pool_size=pool_size))# Upsample 1model.add(UpSampling2D(size=pool_size))# Deconv 1model.add(Conv2DTranspose(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv1'))model.add(Dropout(0.2))# Deconv 2model.add(Conv2DTranspose(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv2'))model.add(Dropout(0.2))# Upsample 2model.add(UpSampling2D(size=pool_size))# Deconv 3model.add(Conv2DTranspose(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv3'))model.add(Dropout(0.2))# Deconv 4model.add(Conv2DTranspose(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv4'))model.add(Dropout(0.2))# Deconv 5model.add(Conv2DTranspose(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv5'))model.add(Dropout(0.2))# Upsample 3model.add(UpSampling2D(size=pool_size))# Deconv 6model.add(Conv2DTranspose(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv6'))# Final layer - only including one channel so 1 filtermodel.add(Conv2DTranspose(1, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Final'))### End of network ### 注意： 1）神经网络架构可视化 2）Keras 有两种不同的建模方式： Sequential models: 这种方法用于实现一些简单的模型。你只需要向一些存在的模型中添加层就行了。本程序采用的是该建模方式。 12from keras.models import Sequentialmodels=Sequential() Functional API: Keras的API是非常强大的，你可以利用这些API来构造更加复杂的模型，比如多输出模型，有向无环图等等。 1from keras.models import Model 代码五：训练模型12345678910111213141516171819# 使用ImageDataGenerator帮助模型使用少量数据datagen = ImageDataGenerator(channel_shift_range=0.2)#channel_shift_range：浮点数，随机通道偏移的幅度。datagen.fit(X_train)# 编译（指定优化器和损失函数）model.compile(optimizer='Adam', loss='mean_squared_error')# 利用批量生成器让模型对数据进行拟合model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size), steps_per_epoch=len(X_train)/batch_size,epochs=epochs, verbose=1, validation_data=(X_test, y_test))# 当训练完成时冻结层model.trainable = Falsemodel.compile(optimizer='Adam', loss='mean_squared_error')# 保存模型model.save('full_CNN_model.h5')# Show summary of modelmodel.summary() 注意： 1）ImageDataGenerator()是keras.preprocessing.image模块中的图片生成器，同时也可以在batch中对数据进行增强，扩充数据集大小，增强模型的泛化能力。比如进行旋转，变形，归一化等等。· 2）利用model.fit_generator让模型对数据进行拟合，它在数据生成上的效果和fit相同。 它的第一个参数应该是一个Python生成器，可以不停地生成输入和目标组成的批量，比如datagen.flow（按batch_size大小从x,y生成增强数据）。 因为数据是不断生成的，所以Keras模型要知道每一轮需要从生成器中抽取多少个样本。这是steps-per-epoch参数的作用：从生成器中抽取steps_per_epoch个批量后（即运行了steps_per_epoch次梯度下降），拟合过程将进入下一个轮次。本例中，每个批量包含128个样本，所以读取完所有11487个样本需要90个批量。 使用fit_generator时，你可以传人一个validation_data参数，其作用和在fit方法中类似。值得注意的是，这个参数可以是一个数据生成器，但也可以是Numpy数组组成的元组。如果validation_data传人一个生成器，那么这个生成器应该能够不停地生成验证数据批量，因此你还需要指定validation_steps参数，说明需要从验证生成器中抽取多少个批次用于评估。 3）在Keras中，冻结网络的方法是将其trainable的属性设为False。如果在编译之后修改了权重的trainable属性，那么应该重新编译模型，否则这些修改将被忽略。 3.车道线检测功能：将模型训练得到的.h5文件和待检测视频作为输入，得到车道线检测后的视频文件结果。 代码一：导入库123456789import numpy as npimport cv2from scipy.misc import imresizefrom moviepy.editor import VideoFileClip#from IPython.display import HTMLfrom keras.models import load_model# 载入预训练模型model = load_model('full_CNN_model.h5') 代码二：检测算法1234567891011121314151617181920212223242526272829303132333435363738394041# Class to average lanes withclass Lanes(): def __init__(self): self.recent_fit = [] # 包含所有的绘制好预测车道线的RGB图像 self.avg_fit = [] # def road_lines(image): """ 获取道路图像，调整图像大小，从模型中预测的车道将被绘制为绿色， 重新创建一个包含绘制车道线的RGB图像，并与原来的道路图像相融合。 """ # Get image ready for feeding into model small_img = imresize(image, (80, 160, 3)) small_img = np.array(small_img) small_img = small_img[None,:,:,:] #(1, 80, 160, 3) # Make prediction with neural network (非标准化) prediction = model.predict(small_img)[0] * 255 # Add lane prediction to list for averaging lanes.recent_fit.append(prediction) # Only using last five for average if len(lanes.recent_fit) &gt; 5: lanes.recent_fit = lanes.recent_fit[1:] # Calculate average detection lanes.avg_fit = np.mean(np.array([i for i in lanes.recent_fit]),axis = 0) # Generate fake R &amp; B color dimensions, stack with G blanks = np.zeros_like(lanes.avg_fit).astype(np.uint8) lane_drawn = np.dstack((blanks, lanes.avg_fit, blanks)) # 将图片尺寸调成和原始视频图像同样大小的尺寸 lane_image = imresize(lane_drawn, (1080,1920, 3)) # 将绘制的车道线和原始道路图像相融合 result = cv2.addWeighted(image, 1, lane_image, 1, 0) return result# 实例化Lanes类lanes = Lanes() 注意： 1）dstack 表示将数组在第三维进行堆叠（即第三层方括号），可将 arr 第三层括号里面的东西视为一个整体，即： 123456789101112131415161718arr1 = [[[块1], [块2], [块3]],[[块4], [块5], [块6]],[[块7], [块8], [块9]]]arr2 = [[[块11], [块12], [块13]],[[块14], [块15], [块16]],[[块17], [块18], [块19]]]# 然后 dstack((arr1, arr2)) 的结果如下：[[[块1 + 块11],[块2 + 块12],[块3 + 块13]][[块4 + 块14],[块5 + 块15],[块6 + 块16]][[块7 + 块17],[块8 + 块18],[块9 + 块19]]] 2）cv2.addWeighted( )函数说明 1cv2.addWeighted(src1, alpha, src2, beta, gamma[, dst[, dtype]]) → dst dst = src1 * alpha + src2 * beta + gamma 其中： src1 – 第一张图片 alpha – 第一张图片的权重 src2 – 与第一张大小和通道数相同的图片 beta – 第二张图片的权重 dst – 输出，python中可以直接将dst放在前面作为输出 gamma – 加到每个总和上的标量，相当于调亮度 3）原视频图像的大小可以这样查看：右键-&gt;属性-&gt;详细信息-&gt;帧宽度、帧高度 代码三：检测视频流中的车道线并保存成另外的视频123456789101112# 输出视频vid_output = 'output_freeway_clip.mp4'# 输入视频#clip1 = VideoFileClip("project_video.mp4")clip1 = VideoFileClip("freeway_clip.mp4")# 将视频分帧并且对每一帧图像进行车道线检测 vid_clip = clip1.fl_image(road_lines)# 将检测好的视频帧图像再合并成视频 vid_clip.write_videofile(vid_output, audio=False) 4.原图与实际检测效果1）原图 2）实际检测]]></content>
      <categories>
        <category>无人驾驶环境感知</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Python</tag>
        <tag>车道线</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令及vim编辑器的使用]]></title>
    <url>%2F2019%2F09%2F16%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%BB%A5%E5%8F%8Avim%E7%BC%96%E8%BE%91%E5%99%A8%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言本文提到的的命令都是在Linux终端下使用的，也就是所谓的黑窗口。正常打开Linux系统是有界面的，也可以通过鼠标像在Windows上操作那样进行点击图标，新建文件等操作。所以想使用终端得另外打开，Ctrl + Alt + T和Ctrl + D是两个常用的打开和关闭终端的快捷键方式。 1．Linux常用命令1.1 使用频率较高的命令1ifconfig #查看IP地址，可以利用该地址让Linux主机被远程控 12345678#通过sudo（“超级用户执行”）命令，你可以临时以root用户身份运行其他命令。这是大多数用户运行root权限命令的最佳方式，因为这样既不用维护root帐户，也不需要知道root用户密码。只要输入自己的用户密码，就能获得临时的root权限。chen@chen-virtual-machine:~$ sudo apt-get update #更新软件列表。[sudo] chen 的密码：xxx #正常输入密码的时候是不会显示的#其他类似的命令还有sudo apt-get install xxx #安装XXX软件。sudo apt-get upgrade #更新已安装软件。sudo apt-get remove xxx #删除XXX软件。 1234567##永久获得root用户身份chen@chen-virtual-machine:~$ su - #chen是用户名;注意':'后面'$';命令是‘SU -’密码： xxx #正常输入密码的时候是不会显示的root@chen-virtual-machine:~# #root是指此时已经获得了root权限；注意':'后面'#' 1.2 Linux文件与目录管理我们知道Linux的目录结构为树状结构，最顶级的目录为根目录 /。 上图是根目录下的一些目录，其中最常用的是home目录。 1.2.1目录和路径1）相对路径和绝对路径 绝对路径：路径的写法，一定由根目录 / 写起，例如： /home/chen/test1 这个目录。 123#以绝对路径进入test1目录root@chen-virtual-machine:~# cd /home/chen/test2/test1 root@chen-virtual-machine:/home/chen/test2/test1# 相对路径：路径的写法，不是由 根目录/ 写起，例如由/home/chen/test1 要到 /home/chen/test2 底下时，可以写成： cd ../test2 。 123root@chen-virtual-machine:/home/chen# cd test1 #首先进入test1目录root@chen-virtual-machine:/home/chen/test1# cd ../test2 #其次进入test2目录root@chen-virtual-machine:/home/chen/test2# 2）目录的相关操作cd：切换目录 123456789101112131415161.root@chen-virtual-machine:~# cd ~chenroot@chen-virtual-machine:/home/chen# #代表进入chen这个使用者的家目录，也就是/home/chenroot@chen-virtual-machine:/home/chen# lschen test1 模板 文档 桌面examples.desktop test2 视频 下载slambook 公共的 图片 音乐#列出目录下的文件2.root@chen-virtual-machine:/home/chen# cd ~3.root@chen-virtual-machine:/home/chen# cd#两个命令都表示回到自己家目录，也就是/root这个目录root@chen-virtual-machine:~# lschen1 Tom#列出目录下的文件4.cd ..表示进入当前目录的上一层目录5.cd - 表示进入当前目录的的前一个工作目录 pwd：显示当前目录cd 12345678root@chen-virtual-machine:/# cd /var/mailroot@chen-virtual-machine:/var/mail# pwd/var/mail#列出目前的工作目录root@chen-virtual-machine:/var/mail# pwd -P/var/spool/mail#/var/mail/是链接文件，链接到/var/spool/mail，加上-P的选项后，不会显示链接文件(快捷方式)的路径，而是显示正确的完整路径#P为大写 mkdir：建立一个新的目录 12345678root@chen-virtual-machine:/home/chen# mkdir test3#创建一名为test3的新目录root@chen-virtual-machine:/home/chen# mkdir test4/test1/test2mkdir: 无法创建目录"test4/test1/test2": 没有那个文件或目录#本来没有test4/test1这些目录，所以无法创建test2这个目录root@chen-virtual-machine:/home/chen# mkdir -p test4/test1/test2#加上-p的选项后，先创建test4/test1这些目录，然后再创建test2这个目录#p为小写 rmdir：删除一个空目录 12345678root@chen-virtual-machine:/home/chen# rmdir test3#删除一名为test3的目录root@chen-virtual-machine:/home/chen# rmdir test4rmdir: 删除 'test4' 失败: 目录非空root@chen-virtual-machine:/home/chen# rmdir -p test4/test1/test2#加上-p的选项后，可以直接删除test4目录下的所有空目录，非空目录删不掉root@chen-virtual-machine:/home/chen# rm -r test4#也可以使用rm -r 删除test4下的所有内容，可以非空 1.2.2 文件与目录管理1）ls1234567891011121314151617181920212223242526272829303132331.root@chen-virtual-machine:/home# cd chen2.root@chen-virtual-machine:/home/chen# cd ~chen3.root@chen-virtual-machine:/home/chen# lschen slambook test2 模板 图片 下载 桌面examples.desktop test1 公共的 视频 文档 音乐4.root@chen-virtual-machine:/home/chen# ls -d.#仅列出目录本身，而不是列出目录内的文件数据5.root@chen-virtual-machine:/home/chen# ls -l总用量 60drwxrwxr-x 2 chen chen 4096 9月 10 16:36 chen-rw-r--r-- 1 chen chen 8980 4月 20 15:05 examples.desktopdrwxrwxr-x 3 chen chen 4096 5月 23 12:52 slambookdrwxr-xr-x 2 root root 4096 9月 10 14:33 test1drwxr-xr-x 4 root root 4096 9月 10 14:39 test2drwxr-xr-x 2 chen chen 4096 4月 20 15:23 公共的drwxr-xr-x 2 chen chen 4096 4月 20 15:23 模板drwxr-xr-x 2 chen chen 4096 4月 20 15:23 视频drwxr-xr-x 2 chen chen 4096 4月 20 15:23 图片drwxr-xr-x 2 chen chen 4096 4月 20 15:23 文档drwxr-xr-x 2 chen chen 4096 4月 20 15:23 下载drwxr-xr-x 2 chen chen 4096 4月 20 15:23 音乐drwxr-xr-x 3 chen chen 4096 5月 23 10:11 桌面#详细信息显示，包含文件的属性与权限等数据6.root@chen-virtual-machine:/home/chen# ls -a. .cache .local test1 视频.. chen .mozilla test2 图片.apport-ignore.xml .config .profile .thunderbird 文档.bash_history examples.desktop slambook .viminfo 下载.bash_logout .gnupg .subversion 公共的 音乐.bashrc .ICEauthority .sudo_as_admin_successful 模板 桌面#全部的文件，连同隐藏文件（开头为.的文件）一起列出来 2）cp:复制文件或目录123456789101112131415161718192021221.root@chen-virtual-machine:/home/chen# cp /home/chen/test/a.py /home/chen/test1#将a.py文件拷贝到test1目录下2.root@chen-virtual-machine:/home/chen# cp -i /home/chen/test/a.py /home/chen/test1cp：是否覆盖'/home/chen/test1/a.py'？ yroot@chen-virtual-machine:/home/chen##加上-i选项后，则在覆盖前会询问使用者是否确定，可以按下n或y来二次确认3.root@chen-virtual-machine:/home/chen/test# cd ~chen/test1root@chen-virtual-machine:/home/chen/test1# cp /home/chen/test/b.py .#先进入想要将文件拷贝到的目录，然后再将其他目录的文件拷贝进来，注意最后的小点。root@chen-virtual-machine:/home/chen/test1# ls -l /home/chen/test/b.py b.py-rw-r--r-- 1 root root 0 9月 12 21:07 b.py-rw-r--r-- 1 root root 0 9月 12 21:06 /home/chen/test/b.py#这个时候原文件和拷贝文件的属性、权限可能会有差异。4.root@chen-virtual-machine:/home/chen/test1# cp -a /home/chen/test/b.py .root@chen-virtual-machine:/home/chen/test1# ls -l /home/chen/test/b.py b.py-rw-r--r-- 1 root root 0 9月 12 21:06 b.py-rw-r--r-- 1 root root 0 9月 12 21:06 /home/chen/test/b.py#加上-a选项，即将文件的所有特性都复制过来了。5.root@chen-virtual-machine:/home/chen/test1# cp -r /home/chen/test /home/chen/test1root@chen-virtual-machine:/home/chen/test1# lstest#加上-r选项，可以复制目录。但是文件与目录的权限可能会改变，随意一般还会加上-a选项，尤其是在备份的情况下 3) rm:删除文件或目录1234567891011121.root@chen-virtual-machine:/home/chen/test1/test# rm a.py#直接删除a.py文件root@chen-virtual-machine:/home/chen/test1/test# lsb.py2.root@chen-virtual-machine:/home/chen/test1/test# rm -i b.pyrm：是否删除普通空文件 'b.py'？ y#加上-a选项就会主动询问，避免你删除到错误的文件名3.root@chen-virtual-machine:/home/chen/test1/test# cd ..root@chen-virtual-machine:/home/chen/test1# lstestroot@chen-virtual-machine:/home/chen/test1# rm -r test#加上-r选项就可以删除目录 4) mv：移动文件或目录123456789101112131、root@chen-virtual-machine:/home/chen/test1# mv /home/chen/test/a.py /home/chen/test1root@chen-virtual-machine:/home/chen/test1# lsa.py#直接移动2、root@chen-virtual-machine:/home/chen/test1# mv a.py b.pyroot@chen-virtual-machine:/home/chen/test1# lsb.py#重命名3.root@chen-virtual-machine:/home/chen/test# mv -i /home/chen/test/a.py /home/chen/test1root@chen-virtual-machine:/home/chen/test# cd ~chen/test1root@chen-virtual-machine:/home/chen/test1# lsa.py b.py#加上-i选项就可以询问是否覆盖已经存在的目标文件 1.2.3Linux 文件内容查看Linux系统中使用以下命令来查看文件的内容： 1）cat 由第一行开始显示文件内容 123cat -A #可以将文件的内容完整的显示出来（包含如换行和[Tab]之类的特殊字符）cat -b #列出行号，仅针对非空白行号显示，空白行不标行号cat -n#打印出行号，连同空白也会有行号，与-b的选项不同 2）tac 从最后一行开始显示，可以看出 tac 是 cat 的倒着写 3） nl 显示的时候，同时输出行号 1234567891011121314151.nl -b #指定行号指定的方式，主要有两种：nl -b a #表示不论是否为空行，也同样列出行号(类似 cat -n)；nl -b t #如果有空行，空的那一行不要列出行号(默认值)；2.nl -n rz #行号在自己栏位的最右方显示，且加 0 ；root@chen-virtual-machine:/home/chen/test1# nl -b a -n rz b.py000001 print("hello.world:")000002000003 print（“I love python”）#自动补零，默认6位3.nl -w #行号栏位的占用的字符数root@chen-virtual-machine:/home/chen/test1# nl -b a -n rz -w 3 b.py001 print("hello.world:")002003 print（“I love python”）#变成仅有三位数 4）more 一页一页地显示文件内容 1234567891011121314151617181920212223242526272829301.root@chen-virtual-machine:/home/chen/slambook/slambook/ch10/ceres_custombundle# more ceresBundle.cpp#打开.cpp文件#include &lt;iostream&gt;#include &lt;fstream&gt;#include "ceres/ceres.h"#include "SnavelyReprojectionError.h"#include "common/BALProblem.h"#include "common/BundleParams.h"using namespace ceres;void SetLinearSolver(ceres::Solver::Options* options, const BundleParams&amp; params)&#123; CHECK(ceres::StringToLinearSolverType(params.linear_solver, &amp;options-&gt;linear_solver_type)); CHECK(ceres::StringToSparseLinearAlgebraLibraryType(params.sparse_linear_algebra_library, &amp;options-&gt;sparse_linear_algebra_library_type)); CHECK(ceres::StringToDenseLinearAlgebraLibraryType(params.dense_linear_algebra_library, &amp;options-&gt;dense_linear_algebra_library_type)); options-&gt;num_linear_solver_threads = params.num_threads;&#125;。。。。（中间省略）。。。。--更多--(19%) &lt;== 光标会在这里等待你的命令2.空白键 (space)：代表向下翻一页；3.Enter：代表向下翻『一行』；4./字串：代表在这个显示的内容当中，向下搜寻『字串』这个关键字；5.q：代表立刻离开 more ，不再显示该文件内容。 5）less 与 more 类似，但是比 more 更好的是，它可以往前翻页 less + 文件名 空白键：向下翻动一页； [pagedown]：向下翻动一页； [pageup]：向上翻动一页； /字串：向下搜寻『字串』的功能； ?字串：向上搜寻『字串』的功能； 6） head 只看头几行 12341.head -n number 文件#显示文件的前面number行2.head -n -number 文件#显示前面所有行，但不包括后面number行 7）tail 只看尾巴几行 12341.tail -n number 文件#显示文件的最后的number行2.tail -f 文件#表示持续侦测后面所接的文件（可能有新的数据一直写入），要等到按下[ctrl]-c才会结束tail的侦测 以上都是针对现有存在文件，假如要创建一个新的空文件就需要用到touch命令 1touch 文件 1.3 文件与文件系统的压缩1.3.1解压缩目录：tartar可以将多个目录或文件打包成一个大文件，同时可以通过gzi、bzip2、xz的支持，将该文件同时进行压缩。 1234567891.压缩：tar –zcvf filename.tar.gz dirname解压：tar –zxvf filename.tar.gz#通过gzip的支持进行压缩/解压缩：此时文件名最好是*.tar.gz2.压缩：tar –jcvf filename.tar.bz2 dirname解压：tar –jxvf filename.tar.bz2#通过bzip2的支持进行压缩/解压缩：此时文件名最好是*.tar.bz23.压缩：tar –Jcvf filename.tar.xz dirname解压：tar –Jxvf filename.tar.xz#通过xz的支持进行压缩/解压缩：此时文件名最好是*.tar.xz 更多有关linux命令的教程：http://www.runoob.com/linux/linux-command-manual.html 2．Vim编辑器及其配置除了查看文件内容外，我们还需要对文件中的内容进行编辑。Linux自带的编辑器有nano和vi，但vi编辑器使用起来很不方便，我们需要先下载vim编辑器，它是vi编辑器的升级版，更人性化些。 2.1vim的安装和使用1)首先更新索引源： 1sudo apt-get update 2)安装vim编辑器: 1sudo apt-get install vim 3）打开文件、保存、关闭文件（vim命令模式下使用） vi filename #打开filename文件，此时是命令模式 w #保存文件 q #退出编辑器，如果文件已修改请使用下面的命令 q! #退出编辑器，且不保存 wq # 退出编辑器，且保存文件 4）插入文本或行(vim命令模式下使用，执行下面命令后将进入插入模式，按ESC键可退出插入模式) a #在当前光标位置的右边添加文本 i #在当前光标位置的左边添加文本 A #在当前行的末尾位置添加文本 I #在当前行的开始处添加文本(非空字符的行首) O #在当前行的上面新建一行 o #在当前行的下面新建一行 R #替换(覆盖)当前光标位置及后面的若干文本 J #合并光标所在行及下一行为一行(依然在命令模式) 5) 设置行号(vim命令模式下使用) set nu #显示行号 set nonu #取消显示行号 6)注意vim命令模式下，想要输入命令，得先输入“：”。 2.2 vim编辑器显示高亮未配置vim时文档的显示无高亮，无行号，体验极差，为了增加高亮，改善体验。 1）我们可以先用SecureFx将《三个工具实现PC端远程连接、桌面共享和文件传输》一文百度文分享链接中的vimconfig.tar.gz传送到/home/用户名/目录下 2）在命令行模式下输入tar xvf vimconfig.tar.gz 解压压缩包 3）进入vimconfig目录中运行config.sh脚本 4）可能会报错，我们需要输入命令sudo /home/mrchen/.vim /home/mrchen/.vimrc,然后再运行./config.sh,然后再运行apt-get install ctags。 上面命令就是在/home/用户名/目录下新建.vim文件和.vimrc文件。 然后在加载ctags包。 5）最后，disconnet然后重新连接登陆就可以了，然后再用vim打开文本文件，即可打开新世界。]]></content>
      <categories>
        <category>树莓派</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[七个Windows工具提升你的办公、开发效率]]></title>
    <url>%2F2019%2F08%2F31%2F%E4%B8%83%E4%B8%AAWindows%E5%B7%A5%E5%85%B7%E6%8F%90%E5%8D%87%E4%BD%A0%E7%9A%84%E5%8A%9E%E5%85%AC%E3%80%81%E5%BC%80%E5%8F%91%E6%95%88%E7%8E%87%2F</url>
    <content type="text"><![CDATA[七个Windows工具提升你的办公、开发效率 工欲善其事，必先利其器。 好的办公工具不仅能提升个人的工作效率，更能提升个人工作的幸福感。这里介绍七款工具，全面助力你的办公、开发。 1.Clover 痛点：我们在不同文件夹之间进行操作时，一两个文件窗口还好。随着工作量的增加，陆陆续续会跳出来许多窗口，这时候我们很有可能就会忘记我们要找的文件对应哪个窗口。又得遍历一遍，效率低下。 在这里介绍Clover工具。 推荐理由：Clover 是 Windows Explorer 资源管理器的一个扩展，为其增加类似谷歌 Chrome 浏览器的多标签页功能。这样我们就可以类比浏览器把不同文件夹当成不同网页使用。 基本使用： Ctrl+T新开页面 Ctrl+W关闭页面 Ctrl+Tab切换页面 Ctrl+D添加当前路径到书签栏，下次可以直接点击打开。 2.Listary 痛点：我们在查找某个文件时可能要翻山越岭经过若干文件目录，最后找到一个.docx文档；随着装的软件越来越多，电脑桌面图标也越来越多，直到整个桌面都是快捷方式。 在这里介绍Listary工具。 推荐理由：Listary 是一项革命性的搜索工具 ，它使得寻找文件和应用程序启动速度极快。 基本使用： 在几乎任何界面键盘输入两次 ctrl 键 ， 输入文件夹/文件/程序名，上下选择+回车即可打开文件夹/文件/程序，还您一个干净整洁的桌面。同时，它还支持模糊搜索，比如输入七个，就可以搜索出本文文档。 在刚刚上下选择某一文件/文件夹/程序名后，只要按下方向键右键。你会立即看到文件的右键菜单弹出。然后就可以对应的提示做出操作。 右边有三个扩展的功能键，分别是收藏夹显示之前收藏过的快捷方式；历史纪录显示查询过的快捷方式，命令是一些扩展功能，比如一键打开CMD命令行窗口等。 网站关键词+内容可以实现快速搜索，比如想在百度上搜索“如何编写MarkDown文档”，在搜索栏中输入bd + 如何编写MarkDown文档，即可。 这里有一些软件自带的关键词。比如，刚刚使用的bd。 3.Snipaste Windows自带截图、QQ有截图功能、网页也有截图。可是，可是，Snipaste这款工具截图真的太好用了。 推荐理由：Snipaste 是一个简单但强大的截图工具，也可以让你将截图贴回到屏幕上！下载并打开 Snipaste，按下 F1 来开始截图，再按 F3，截图就在桌面置顶显示了。就这么简单！ 基本使用： 强大的截图，它可以自动检测截图窗口大小，比如可以截出完美的矩形框。 把图片贴到屏幕上，把图片作为窗口置顶显示。 方便地标注图像， 支持矩形、椭圆、 线条、 箭头 、 铅笔 、 马克笔 、 文字，还有马赛克、 高斯模糊、橡皮擦。 自定义设置，右击Snipaste选择首选项，即可进行丰富的设置。 4.Rolan 推荐理由：这款软件可以将文件夹/文件/应用程序拖动到窗口进行分类，最主要的是他可以贴边隐藏，随时呼出。 这款软件和Listary有异曲同工之妙。和Listary搭配使用，基本上能在最快的时间内找到你需要的文件夹/文件/应用程序并打开；同时最大程度上减少桌面图标数量（我自己本人桌面只有一个回收站(｡･ω･｡)）。 5.uTools 推荐理由:uTools 是一款极简、插件化、跨平台、现代化的桌面增强效率软件，它有两个最大的特点，第一个就是快速呼出，按下键盘快捷键(默认 Alt+空格)，即可呼出输入框。第二个就是丰富的插件，作者将 uTools 设计成“一切皆插件”，所有的功能都由插件来提供。下面就让我们来看看丰富的插件吧。 通用 在线翻译，为中英文翻译，简单快捷。 todo，可以快速呼出的待办事项清单，随时记录大小事务。 图片 图床 ​ 一般在用MarkDown写文章的时候，对于图片是从本地传的文档在另一电脑上图片就加载不出来。这时候需要把图片放到第三方，别人打开文档就可以看到加载的图片，相当于从第三方下载的图片，这个第三方就被称为图床。 ​ 如下图，点击上传标志即可上传。 ![Snipaste_2019-05-20_21-40-32.png](http://yanxuan.nosdn.127.net/af02b4bfbe10356e14b16a0380e7eed4.png) 图片转文字，是OCR拍照识字没错了。 开发 编码小助手 无论是开发者还是学习编程的爱好者，都经常遇到需要对各类字符串进行编码的情况。uTools 的编码小助手插件包含了 时间戳转换、uuid、hash加密、base64、进制转换等功能，可以大大提高大家写代码的效率。 使用：ALT + 空格呼出软件。在插件中心下载需要的插件，在已安装打开插件，点击红色方框处即可使用。 前面介绍的Listart、Snipaste、Rolan、uTools强烈建议设置为开机自启动，这样的话就可以直接通过快捷键呼出软件啦，而不需要慢慢找到图标再打开软件。 6.Bandizip Windows版 推荐理由：这是一款压缩包软件，和其他压缩包不同的是它支持压缩预览。只要点击右键就可以实现压缩文件预览。 7.PotPlayerMini 推荐理由： 它无需安装，双击可执行文件即可运行播放印音频/视频，启动速度快。 内置强大的解码器（普通用户无需安装第三方解码器即可播放主流格式的视频文件，支持BD和MKV大视频文件的播放。），播放过程稳定，对付高清大片没有任何问题。 对字幕的支持也很不错，再加上给力的各种皮肤以及相应的Direct3 D9皮肤机制让其在同一款皮肤下的播放形式有了更多一层的表现力，可塑性极强。 介绍了炒鸡好用的软件，那么问题来了，怎么样能以最快的速度下载安装好这些软件呢？ 公众号后台回复“办公效率工具”即可获得百度云链接。 然后就可以开始你的效率人生啦，时间就是金钱！]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>效率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[五个Windows工具提升你的学习、科研效率]]></title>
    <url>%2F2019%2F08%2F26%2F%E4%BA%94%E4%B8%AAWindows%E5%B7%A5%E5%85%B7%E6%8F%90%E5%8D%87%E4%BD%A0%E7%9A%84%E5%AD%A6%E4%B9%A0%E3%80%81%E7%A7%91%E7%A0%94%E6%95%88%E7%8E%87%2F</url>
    <content type="text"><![CDATA[五个Windows工具提升你的学习、科研效率 工欲善其事，必先利其器。 1.引言我在之前的文章《七个Windows工具提升你的办公、开发效率》推荐了七个Windows工具来助力办公、开发工作。（以下内容会用到之前文章中推荐的工具，以下简称文章为《七个》）事实上，除了办公、开发，我们还需要适时地充电学习来应对新知识不断涌现的时代。这里主要针对两种学习场景，一种是看视频做笔记，另一种是看论文学习。 2.视频笔记2.1视频截图（任何视频格式）推荐：使用PotPlayer（《七个》文章推荐）播放视频，截图有个快捷键：ctrl+c，即可将当前视频画面（不含字幕）复制到剪贴板，然后再拷贝到Word等编辑器；也可以在播放视频的时候用Snipaste（《七个》文章推荐）截取部分画面（比如大段的文字或者字幕），然后复制到uTools（《七个》文章推荐）软件当中的OCR插件中提取文字，然后再将提取的文字复制到笔记上。 2.2视频字幕（.mkv视频格式）痛点：2.1提到了可以提取字幕的方法，但是如果需要提取一节视频的所有字幕生成文件（一般是.srt）；或者对于一些“生肉”教学视频，需要批量封装字幕。这里推荐两个工具，可以批量封装和批量提取字幕。 推荐：Mkvtoolnix-mmg和Mkvtoolnix-MKVExtractGUI2 使用：具体使用和注意事项可以参考使用mkvtoolnix批量封装和批量提取字幕。 3.论文学习3.1论文下载痛点：在学校可以很轻松的凭借校园网下载大量的学术论文，一旦离开学校想下载论文就需要付费下载，也就是“知识就是金钱”。有人提出可以让在校的同学帮忙下载，但是毕竟不是长久之计，这里提供几种方法可以免费下载论文。 推荐一：谷粉学术 使用： 1）打开官网，输入想要查询的关键词，如“deep learning”，然后点击谷粉学术。 2）然后如果出现[pdf]字样，可以直接点击。 3）然后如果出现[sci hub 下载] 字样，可以直接点击。 4）然后在跳转的页面进一步操作，即可下载到pdf学术文档。 注意： 1）该网站针对国外英文文献有大量的下载资源链接，但是对国内的很多文献找不到下载。如输入”深度学习“搜索字样。所以在查找国内文献无法找到下载链接的情况下建议使用推荐二。 2）其他的如使用PubMed号、DOI号搜索下载论文可以参考这篇文章。 推荐二：浙江图书馆 使用：我是在同事的推荐下知道浙江图书馆的，具体的注册和使用方法可以参考这篇文章。 注意：在使用知网时会提示“当前并发用户数已满”。知网是有并发用户数限制的，当用户已满的时候就会出现这样的情况，一般是使用数据库的人比较多，可以过一会再尝试；另外许多论文是好几个数据库都收的，知网忙的时候不妨转用万方、维普等数据库。 3.2论文管理——Zotero痛点：阅读大量的paper是磕盐工作者一项基本的工作，尤其是在做一项课题研究的初期。有些论文我们在读过一遍时就会将其丢在一堆文档中，当我们想再次查阅时就会变得特别不方便；或者当我们自己要写一篇论文时，可能要打开数十个网页来导出参考文献，然后再将内容复制到Word中编辑。 推荐理由：可以前期就对文献做了细致的管理和标记，方便后面查阅；可以轻松插入引用上标，一键导入所有参考文献。 基本使用： 1）首先是从官网下载安装该软件。（百度云提供下载） 2）打开软件界面，鼠标右键点击我的文库-&gt;新建分类 3）如果是英文文献，就直接将下载好的英文论文PDF文档拖入标题一栏，然后会自动填充信息，还可以针对文献做笔记。 4）如果是中文文献，首先导出.blb文件后用记事本打开并复制里面的全部内容 然后在文件中选择从剪切板导入 然后将中文论文PDF文件鼠标拖至刚导入文件成为其子文件 5）标题一栏中的PDF文档是可以直接打开看的；还有搜索功能，即假如标题界面一共有30篇论文，可以输入关键字搜索。 6）然后就是自己编写论文，需要引用文献。首先是选择合适的样式，这里推荐“Chinese Std GB/T 7714-2005(numeric,Chinese)”，比较符合国人写论文的习惯。首先从这个网站将该样式下载下来，下载文件为.csl格式。 7）然后加载样式。打开Word-&gt;选择Zotero-&gt;Document Preference-&gt;Zotero - 文档首选项(管理样式)-&gt;Zotero 首选项(获取更多样式) -&gt;点击加号 加载.csl文件 8）然后插入上角标。鼠标光标置于要插入上角标处-&gt; Zotero-&gt; Add/Edit Citation -&gt; 选择样式 光标处出现{Citition字样}，并选择经典视图 选择对应文献，点击OK后就会出现上标。然后依次添加其他上标。 9）最后一键导入所有参考文献内容。鼠标光标置于要导入参考文献处-&gt; Zotero-&gt; Add/Edit Biliography 10)最终效果图 11) 数据存储位置设置 因为随着使用时间增长，文献库会日渐庞大，因此不建议直接使用默认的数据存储位置（C盘）。这个也可以在【编辑-首选项-高级-文件和文件夹】栏目下进行修改，选择数据存储位置-自定义，然后选择目标目录即可。之后会提示你需要手动将Zotero原数据库下的文件移动到新目录下，这个很方便，可以直接点击打开数据文件夹，然后将里面的内容全部移动到目标文件夹下就可以了。 介绍了炒鸡好用的软件，那么问题来了，怎么样能以最快的速度下载安装好这些软件呢？ 公众号后台回复“科研效率工具”即可获得百度云链接。 然后就可以开始你的效率人生啦，时间就是金钱！]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>效率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cmake使用教程]]></title>
    <url>%2F2019%2F05%2F23%2FCmake%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"></content>
      <categories>
        <category>工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基于Typora的Markdown编辑器使用教程]]></title>
    <url>%2F2019%2F05%2F12%2FTypora%EF%BC%9A%E6%9E%81%E7%AE%80Markdown%E7%BC%96%E8%BE%91%E5%99%A8%2F</url>
    <content type="text"><![CDATA[1.MarkdownMarkdown以简单的语法深受写作朋友的喜爱，尤其是对程序代码输入的支持更加方便了程序员写技术博客。此外它让写作回归到内容本身，而不是以富文本（word、秀米等）为主流的对排版的专注。 Markdown可以实现一篇文章，跨平台使用。即在Markdown编辑器里面写好后，可以直接一键复制到简书，知乎，CSDN，Github，有道云笔记等平台。如果是需要复制到微信公众号里的话，就需要借助MarkDown转微信公众号工具。 2.Typora主要功能介绍上文提到Markdown编辑器，我个人使用的是Typora，这是一款所见即所得的工具，并且它还支持将写好的Markdown文章轻松转换成PDF和HTML文件等其他格式，转成PDF文件时会自动按照标题生成书签和目录。 如果需要的话，可到官网下载Typora这款软件。 2.1 字体效果​ 强调:在要强调内容前后分别加两个“*”号。 下划线:快捷键Ctrl+u。 斜体：内容前后分别加一个“*”号。 代码：先转化成英文输入法，再把内容前后分别加上一个“`”号。 import cv2 删除内容（切换完成状态）：先转化成英文输入法，再把内容前后分别加上两个“~”号。 高亮：内容前后分别加两个“=”。 文字跳转链接：先转化成英文输入法，输入中括号+小括号，中括号里面填文字内容，小括号里面填网页链接。 2.2 段落效果 有序列表：输入数字“1”+“.”+空格 ， 自动开始有序列表。 无序列表：输入“+”或“-”或“*”+空格，自动开始无序列表。 段落引用：Ctrl + Shift +Q。 标题：Ctrl+0~6即可实现对应的段落到六级标题。 2.3 插入 代码块：先转化成英文输入法，再把内容前后分别加上三个“`”号。 针对不同编程语言配备不同的代码高亮和横向滚动条，还可以自己设置背景颜色。 123456import matplotlib.pyplot as pltimport matplotlib.image as mpingimport numpy as npimport cv2from moviepy.editor import VideoFileClipfrom pylab import axis 表格:Ctrl+T。 在弹出的对话框中选择行列数，自动生成列表。还可以很方便地对表格进行编辑。 图像：直接将本地或者网页图片拖到光标处即可。 水平分割线：输入三个或三个以上“-”（“*”），再按回车键，即出现一条分割线。 内容目录：[toc] + enter 2.4数学公式首先在文件-&gt;偏好设置里将Markdown扩展语法打钩。 2.4.1公式输入神器——Mathpix Snip输入数学公式应该是一件令人头疼的事，特别是大段的数学公式。在这里推荐Mathpix Snip ，可以从它的官网下载。说它是神器，主要是我们只需要截个图，公式会自动转化为 LaTex 表达式，然后再进行复制就可以，大大提高了效率。 使用：启动软件后，使用快捷键CTRL+ALT+M即可开始截图，图源可以来自文档、视频、手写体。 2.4.2内联公式复制第二行代码，即$代码$，点copy即可，光标点到输入公式的地方，直接粘贴。 2.4.3公式块复制第三行代码，即$$代码$$，点copy即可，光标点到输入公式的地方，直接粘贴。 2.4.4延伸如果想深入学习关于数学公式的Markdown表达，可以参考这篇博客。 2.5HTML页面 嵌入Bilibili视频 123456789&lt;iframe height=450 width=800 src="//player.bilibili.com/player.html?aid=32640707&amp;cid=57118032&amp;page=1" scrolling="no" border="0" frameborder="0" framespacing="0" allowfullscreen="true"&gt; &lt;/iframe&gt; 以上文本可以从B站视频分享中找到。 嵌入腾讯视频 123456&lt;iframe height=450 width=800 src="https://v.qq.com/txp/iframe/player.html?vid=j00301d62s4" frameborder=0 allowfullscreen&gt; &lt;/iframe&gt; 以上文本可以从腾讯视频分享中找到。 嵌入音频(T＿T) 1&lt;iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&amp;id=864711417&amp;auto=1&amp;height=66"&gt;&lt;/iframe&gt; 以上文本可以从网页版网易云音乐生成外链播放器获取。 2.6其他 插入表情：ctrl + shift + b插入颜文字表情（搜狗输入法），插入emoji表情（使用微软输入法） 主题，在官网选择自己喜欢的主题，下载下来，通常是个压缩包，解压后放入 Typora 的主题文件夹（文件-&gt;偏好设置-&gt;主题-&gt;打开主题文件夹），重启 Typora 就可以使用了。 3.总结 轻量级极简免费功能强大的Markdown编辑器。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Typora</tag>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树莓派入门之开机配置、远程操作]]></title>
    <url>%2F2019%2F05%2F11%2F%E6%A0%91%E8%8E%93%E6%B4%BE%E4%BD%BF%E7%94%A8%E4%B9%8B%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85%E5%92%8C%E5%9F%BA%E6%9C%AC%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1.引言 树莓派是为学习计算机编程教育设计的一种只有信用卡大小的微型电脑，其系统基于Linux。随着Windows 10 IoT的发布，我们也将可以用上运行Windows的树莓派。 树莓派的系统是安装在SD卡里的，只需插上SD卡、接通显示器和键盘鼠标，就能执行如电子表格、文字处理、玩游戏、播放高清视频等PC的基本功能。 个人选择树莓派，主要想用来打通Linux的基本编程知识以及嵌入式硬件知识，另外相应的做一些简易创客项目。 2.系统安装 2.1 安装包（百度云提供下载链接）首先是下载所需要的树莓派系统镜像和安装烧写镜像的工具，当然也可以从官网下载最新的Raspbian树莓派系统。 此外，还有本文树莓派登录控制提到的其他软件。 2.2 树莓派系统镜像烧写1）解压下载的树莓派系统文件，得到img镜像文件； 2）将SD使用卡托或者读卡器后，连上电脑； 3）直接点击安装Win32DiskImager-0.9.5-install.exe并运行； 4）在软件中选择img文件，“Device”下选择SD的盘符，然后选择“Write”； 5）然后就开始安装系统了，根据你的SD速度，安装过程有快有慢。 6）安装结束后会弹出完成对话框，说明安装就完成了，如果不成功，请关闭防火墙一类的软件，重新插入SD进行安装。 2.3 Windows下备份（还原）树莓派树莓派支持许多操作系统，针对不同需求在不同系统上做相应的配置，我们可以通过购买若干张SD卡。当然我们也可以实现一张卡的重复使用，也就是说我们可以把原先系统的内容完整拷贝到Windows下，下次再用我们就再将原先系统内容烧录到SD卡即可。同样，还是上文提到的Win32DiskImager-0.9.5-install.exe可以实现该功能。 先新建一个空白的.img后缀的文件，然后选择直接read就可以备份系统了，到时再重装就可以恢复了。 3.基本设置3.1 换源树莓派默认的下载源是国外的源，每次下载时速度都比较慢，因此我们可以从国内已经下载了这些资源的人那里获取，比如说，清华大学开源软件镜像站。 1）给Raspbian的包管理器apt-get换源在树莓派的命令行界面输入： 1sudo vim /etc/apt/sources.list 在第一行开头加一个#注销其内容，然后把下面的内容拷贝到最后一行之后，如图中的效果： 12deb http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpideb-src http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi 再输入以下命令更新到清华大学镜像源最新的软件列表。 1sudo apt-get update 以上步骤实现了Respbian的包管理器apt-get换源到清华大学软件镜像站，并更新了软件列表，今后在树莓派命令行中执行sudo apt-get install 软件名时便会自动从清华大学开源软件镜像站高速下载。 2）给Python的第三方模块安装工具pip换源在树莓派的命令行中依次输入运行以下三个命令 123sudo mkdir ~/.pip #root目录下建立.pip文件夹cd .pip sudo nano pip.conf 在打开的文件中输入以下内容，如图中的效果： 12345678[global]timeout = 10index-url = http://mirrors.aliyun.com/pypi/simple/extra-index-url= http://pypi.douban.com/simple/[install]trusted-host= mirrors.aliyun.com pypi.douban.com 3.2树莓派启动烧写完后把SD卡直接插入树莓派接通电源即可运行，此时我们需要连接显示器。 打开如图： 3.3 配置WIFI基本操作：接上鼠标，选择左上角树莓派标志-&gt;首选项-&gt;Raspberry Pi Configuration-&gt;Localisation-&gt;WiFi Country设置Country为CN China。 重启后，在右上角选择WIFI，输入密码即可连接。 拓展：设置树莓派开机后自动连接无线网络（从第二次开始无需显示器）。 1）在联网状态下（首次需要连接显示器），打开终端，安装vim编辑器。 sudo apt-get install vim 2）切换到root用户（部分文件我们是没有权限的，这时需要获取root用户的权限，可以通过sudo来临时获取最高权限，或者切换到root用户，） 3）然后进入目录/etc/wpa_supplicant/wpa_suppliacant.conf.进行编辑。下图是编辑完成后以界面文件形式打开。 ssid是无线网络名称，psk是密码，key_mgmt是加密方式（可省略），priority是优先级。 可以设多个WiFi，赋予不同的优先级，数字越大，优先级越高。 我设两个，第一个是手机热点，第二个是路由器WiFi，一个断网会自动切换到另一个。 3.4 本地化设置进入左上角树莓派标志-&gt;首选项-&gt;Raspberry Pi Configuration-&gt;Localisation进行相应的设置。 1）系统语言设置 2）系统时区时间设置 3）系统键盘布局设置 当然，也可以在终端完成这一切设置，详细可见这篇博客。 4.树莓派的登录控制4.1 SecureCRT1）官方的树莓派系统没有开启SSH服务，需要我们人为的开启SSH服务，我们需要在HDMI显示器上的命令行终端上输入sudo raspi-config进入到树莓派系统配置界面。 2）打开命令行终端，输入ifconfig查看我们的ip地址。 3）在电脑端打开软件SecureCRT和SecureFX并连接ip地址。这样就可以实现远程控制树莓派了。 4.2 VNC Viewer1）首先确认树莓派打开VNC，打开树莓派命令行界面输入命令，进入树莓派配置界面。 1sudo raspi-config 第五行：Interfacing Options开启功能 第三行VNC：VNC远程桌面登陆。 2）安装好VNC Viewer，确保树莓派和主机连在同一个网络下。 3）打开VNC Viewer，输入IP地址，回车即可。 4）跳出授权界面，正常填写树莓派用户名和密码即可。 5）点击ok即可进入界面。 6）另外还需设置一下分辨率，输入命令 1sudo vim /boot/config.txt 找到如下两行，将其设置为自己电脑的分辨率即可。比如我的电脑是1920x1080。 12framebuffer_width=1920framebuffer_width=1080 7）重启就可以像自己笔记本一样正常显示了。 4.3 TeamViewer关于树莓派登录控制提到的三个软件更详细的使用方法可以参考《三个工具实现PC端远程连接、桌面共享和文件传输》。 5.树莓派配置并使用USB摄像头。将USB摄像头外接在树莓派，打开终端，输入：lsusb 终端继续输入sudo apt-get install fswebcam 下载完成后，终端输入fswebcam --no-banner -r 640*480 camera.jpg可以在/home/pi目录下生成一个当前摄像头拍摄到的实时照片。以此证明USB摄像头工作正常。 介绍了和树莓派搭配使用的软件，那么问题来了，怎么样能以最快的速度下载安装好这些软件呢？ 公众号后台回复“树莓派基本工具”即可获得百度云链接。 对基本编程感兴趣的童鞋不能错过树莓派幺！]]></content>
      <categories>
        <category>树莓派</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Win10环境下安装GPU版TensorFlow-显卡RTX2060.md]]></title>
    <url>%2F2019%2F04%2F21%2FWin10%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%AE%89%E8%A3%85GPU%E7%89%88TensorFlow-%E6%98%BE%E5%8D%A1RTX2060%2F</url>
    <content type="text"><![CDATA[前言 目前由于工作需求需要跑跑深度学习数据集。这里介绍两种配置的tensorflow-gpu环境的搭建。 正文 环境一 win10_64 tensorflow1.4 cuda8.0 cudnn6.0 GTX1050 python 3.6 参考win7_64+tensorflow1.4+cuda8.0+cudnn6.0+GTX1050安装 Win10下Tensorflow(GPU版)安装趟坑实录 参照这两个连接可以完美安装tensorflow-gpu。 环境二 RTX2060 win10 Tensorflow GPU CUDA 9.2 CUDNN7.2 python 3.6 安装1.Anaconda3下载与安装下载Anaconda3-5.2.0-Windows-x86_64并安装。 记得打勾！！两个都要勾上。 2. VS2017 下载与安装我这里下载的是Windows Community 2017版，下载完成后双击进行安装，安装C++的编译器。 其实不一定非要装vs，只是需要vc++框架即可，可在“控制面板\程序\程序和功能\卸载程序”里查看。 3.CUDA10.0下载与安装在电脑桌面鼠标右击依次打开”NVIDIA控制面板-&gt;”系统信息”-&gt;”组件”，查看适合本电脑的CUDA驱动版本。 在NVDIA官网下载对应版本的CUDA，这里我选择的是“CUDA Toolkit 10.0版本 ”，选项为： 4、CUDNN7.3.1下载与安装官网下载https://developer.nvidia.com/rdp/cudnn-archive 这里可能需要注册登录填个调查文件啥的。 这里我们选择cuDNN v7.3.1的版本 选择的依据是来源于github：https://github.com/fo40225/tensorflow-windows-wheel 由于我使用的tensorflow-gpu版本是1.12.0，python版本是3.6，CUDA版本是10.0。故选择cuDNN v7.3.1的版本。 到这里下载完成！完成后咱们开始解压，然后将相应的包，放到cuda相应包底下。!咱们只需要拿出这些文件夹里面文件放到想要cuda文件夹即可，我的cuda文件夹地址为：C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 5.创建并激活运行环境创建运行环境，输入指令： 1conda create -n tensorflow-gpu python=3.6 新建一个名字叫“tensorflow-gpu”，python版本为3.6的运行环境，此环境与Anaconda中其它环境隔离。然后输入“y“和回车后开始安装。 安装完成后会生成新的文件夹，可以此为依据查看自己是否创建成功。 激活并进入环境，使后续指令在激活的环境中生效，输入指令： 1conda activate tensorflow-gpu 注意：base和tensorflow-gpu是两个不同的运行环境，两个环境的安装包互不干涉。如果需要pip install 安装包，需要先切换到对应的运行环境下。(比如下文的pip install tensorflow-gpu安装包得先切换到对应运行环境下。) 升级pip到最新版，输入指令： 1python -m pip install --upgrade pip 6、tensorflow-gpu 安装github这个地址里面按照我下图的选择版本 下载，桌面弄个文件夹下，下载到文件夹内。然后在文件夹内按着shift键，右击空白地方，选择 在此处打开Powershell,然后在命令窗里输入： 1pip install tensorflow_gpu-1.12.0-cp37-cp37m-win_amd64.whl 使用conda list查看安装情况，看到了tensorflow-gpu说明已经安装完成。 7.测试 8.结果 已经能够识别出RTX2060显卡了，并成功输出！ 后记 enjoy yourself~]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>GPU</tag>
        <tag>RTX2060</tag>
        <tag>Win10</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VMware Tools安装教程]]></title>
    <url>%2F2019%2F04%2F21%2FVMware%20Tools%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言 使用虚拟机下安装linux系统的人都有这样的感受，感觉Linux系统界面太小，无论是打开浏览器还是终端，都感觉很压抑，并且无法很好地在Linux和Windows之间传送文件。前文给出了使用SecureCRT控制Linux终端以及传送文件的方法，本文将给出在Linux里实现界面文件轻易传送并且实现自适应调整大小的方法。 正文 1.环境 Windows10 VMware-workstation-full-14.0.0.24051 ubuntu-18.04.1-desktop-amd64 2.方法 首先是打开虚拟机，在菜单栏找到“虚拟机(M)”选项，并在其子菜单中选择“安装VMware Tools(T)…”(注意是要在虚拟机启动的状态下进行),弹出如下图所示的提示框，点击【是】。 点击左侧任务栏中的”files“，然后从上往下找到“VMwareTools”。点击进入会有一个“VMwareTools-10.1.6-5214329.tar.gz”的压缩包文件，将此文件提取（解压）到桌面。 注意：如果提取出现“Not enough free space to extract VMwareTools-10.1.6.-5214329”这样的报错。就需要把VMwareTools-10.1.6.-5214329.tar.gz复制到桌面再进行提取。 解压后会出现一个“vmware-tools-distrib”，进入并找到”vmware-install.pl“的脚本文件， 该文件就是安装vmware tool的脚本文件； “Ctrl+Alt+T”打开终端（命令行），进入到vmware-install.pl文件所在的目录下，运行命令执行该perl 脚本： 1sudo ./vmware-install.pl 按提示信息一步步走，也可全部按回车进入下一步，直到出现如下信息：“Enjoy——the VMware team”，至此VMwareTools终于安装完成了。 3.重启虚拟机 和ubuntu18.04系统接着还需要设置： 虚拟机界面 菜单栏找到”查看”-&gt;”自动调整大小”-&gt;”自动适应客户机大小”,选定它； 这样一来就可以实现了： 使用户可以在物理主机和虚拟机之间直接拖动文件。 避免了在物理机和虚拟机之间必须使用CTRL切换，我们不必使用键盘切换，直接便可退出，使得虚拟机真正成为了电脑的一部分。 使得界面充满整个VMware虚拟机，看着更舒适。 后记 enjoy yourself~]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>ubuntu</tag>
        <tag>VMware Tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三个工具实现PC端远程连接、桌面共享和文件传输]]></title>
    <url>%2F2019%2F04%2F17%2F%E4%B8%89%E4%B8%AA%E5%B7%A5%E5%85%B7%E5%AE%9E%E7%8E%B0PC%E7%AB%AF%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5%E3%80%81%E6%A1%8C%E9%9D%A2%E5%85%B1%E4%BA%AB%E5%92%8C%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%2F</url>
    <content type="text"><![CDATA[1.引言 一般情况下，我们在连接虚拟机的终端或者访问服务器或者连接另一台计算机需要不同程度的实现远程连接终端、桌面共享和文件传输。这里介绍不同程度的可以实现上述功能的3款软件。 2.SecureCRT SecureCRT 是一款可以实现远程连接终端和文件传输的软件。它发挥作用的前提是主机和远程主机必须在同一网段内。比如，下图是我用该软件来控制电脑的Linux虚拟机终端。 2.1 安装（百度云提供下载）1）双击安装程序进行安装（第一个文件是SecureCRT8.1.0 64位安装程序，第二个文件是SecureCRT注册机，第三个文件是SecureFX注册机。） 2）点击next 3）进入license agreement，选择 I accept…，点击next 4）点击next 5）这里我选择Custom（自定义安装），点击next 6）点击“change”更改自己的安装路径，然后点击next 7）这里我没勾选生成桌面快捷方式，点击next 8）点击install，开始安装，等待安装完成 9）安装完成后，先不要运行，点击finish 10）此时在开始菜单会看到若干图标，如下图 到这里，SecureCRT、SecureFX安装已经完成了。 2.2 破解1）解压注册机的压缩文件，将程序复制到SecureCRT的安装目录下（注意：电脑杀毒软件如果会把程序删除掉，需要先将杀毒软件关闭），如下图： 2）双击打开SecureFX keygen.exe，如下图，点击patch 3）点击path 选择 SecureCRT.exe进行替换，之后会继续弹出一个窗口，然后选择LicenseHelper.exe，之后会弹出替换成功的信息。选择完成后点击generate，生成注册码 4）接着回到开始菜单双击执行SecureCRT程序，选择文件数据存储位置，点击OK 5）接下来按照以下步骤即可 6）将注册机生成的信息按照提示复制粘贴过去 依次复制Name、Company、Serial number、License key、Issue date。 7）复制完后点击“完成” 8）接下来会让你填写登陆SecureCRT的账号密码，填写后每次打开SecureCRT需要登陆账号密码，这里我选择不填写 9）到这里SecureCRT破解已经完成了，接下来你就可以使用啦！ 10）SecureFX的破解方法同SecureCRT的破解类似。 2.3 使用1）点击开始菜单SecureCRT8.1快捷方式，输入Linux主机或远程主机的IP地址和用户名，点击“connect”连接，按照提示根据自己的需要选择，之后填写密码，点击“OK”即可连接使用！ 注意：一台Win10可以同时连接多个远程主机，如Session下面有三个IP地址就代表三个远程主机。 2）点击开始菜单SecureFX8.1快捷方式，选择IP地址即可。 然后就可以在Win10本地和Linux系统之间拖动传输文件，非常方便。 3）其实单独使用SecureCRT8.1就可以实现本地和Linux之间的文件互传，但总觉得没有拖动来的方便。有兴趣的可以往下读。 4）SecureCRT8.1实现本地和Linux之间的文件互传 SecureCRT 按下ALT+P就开启新的会话 进行ftp操作。 我们要想下载或上传某个目录下的文件，首先要cd 到该文件所在文件的目录下，然后使用 get（将远程目录中文件下载到本地目录）或put（将本地目录中文件上传到远程主机） +文件名的命令来下载或上传。 2.4 显示高亮2.4.1 界面显示高亮SecureCRT连接Linux后显示就是黑白，体验感极差。 我们可以做些基本设置，提升体验感，具体的设置可以参照这篇文章。 2.4.2 vim编辑器显示高亮未配置vim时文档的显示无高亮，无行号，体验极差，为了增加高亮，改善体验。 1）我们可以先用SecureFx将百度文分享链接中的vimconfig.tar.gz传送到/home/用户名/目录下 2）在命令行模式下输入tar xvf vimconfig.tar.gz 解压压缩包 3）进入vimconfig目录中运行config.sh脚本 4）可能会报错，我们需要输入命令sudo /home/mrchen/.vim /home/mrchen/.vimrc,然后再运行./config.sh,然后再运行apt-get install ctags。 上面命令就是在/home/用户名/目录下新建.vim文件和.vimrc文件。 然后在加载ctags包。 5）最后，disconnet然后重新连接登陆就可以了，然后再用vim打开文本文件，即可打开新世界。 2.5 问题再来详细谈谈第2.3步 使用的第一小步。 1）查看自己Linux的IP地址：“Ctrl+Alt+T”打开Linux系统的终端，输入ifconfig 2）可能出现的情况： 3）如果SecureCRT出现“The remote system refused the connection”，如果你遇到这个问题，说明你的Linux系统里面没有安装openssh-server ​ 查看Linux端的当前进程： ​ 显示只有一个进程。 ​ 安装openssh-server ​ 再次查看Linux这端的当前进程： ​ 有两个进程。 ​ 问题解决，再次尝试即可连接成功 3. VNC Viewer VNC Viewer 是一款可以实现远程桌面共享的软件。它发挥作用的前提是主机和远程主机必须在同一网段内。比如，下图是我用该软件来在本地共享树莓派桌面。 1）首先确认树莓派打开VNC，打开树莓派终端界面输入命令，进入树莓派配置界面。 1sudo raspi-config 第五行：Interfacing Options开启功能 第三行VNC：VNC远程桌面登陆。 2）安装好VNC Viewer，确保树莓派和主机连在同一个网络下。 3）打开VNC Viewer，输入IP地址，回车即可。 4）跳出授权界面，正常填写树莓派用户名和密码即可。 5）点击ok即可进入界面。 6）另外还需设置一下分辨率，终端输入命令 1sudo vim /boot/config.txt 找到如下两行，将其设置为自己电脑的分辨率即可。比如我的电脑是1920x1080。 12framebuffer_width=1920framebuffer_width=1080 7）重启就可以像自己笔记本一样正常显示了。 4. TeamViewer TeamViewer 是一款可以实现远程桌面共享与文件传输的软件。它发挥作用的前提是主机和远程主机不需要在同一网段内！比如，下图是我用该软件来在本地共享树莓派桌面。 PC端和树莓派的teamviewer版本要一致，不然不能连接哦！这里Windows电脑使用的是TeamViewer14，故在树莓派也要实用14版本。 1）下载Teamviewer 使用SecureFx将在官网下载好的teamviewer-host_14.2.8352_armhf.deb改名为teamviewer-host_armhf.deb拷贝到树莓派/home/pi目录下。 然后在终端执行下面两条命令。 123sudo dpkg -i teamviewer-host_armhf.debsudo apt-get -f install 2）安装GDebi，解决依赖问题 1sudo apt-get install gdebi 3）安装Teamviewer 1sudo gdebi teamviewer-host_armhf.deb 4）常用命令，建议第一次安装好TeamViewer就设置。 123456teamviewer help #查看帮助信息teamviewer info #查看本机IDsudo teamviewer passwd [你的密码] #设置本机密码sudo teamviewer --daemon start #启动TeamViewer服务sudo teamviewer --daemon enable #开启TeamViewer服务随机启动sudo reboot #重启即可连接 以上常用命令使每次接通树莓派电源，就自启动TeamViewer，然后通过Windows端控制，一劳永逸。 5）在PC端输入伙伴ID、连接、输入teamviewer密码 伙伴ID由刚刚的teamviewer info命令获取； 密码由刚刚的sudo teamviewer passwd [你的密码]命令自己设置的。 6）更改树莓派的分辨率 由于刚刚已经在VNC Viewer设置过，这里就不需要了。 介绍了炒鸡好用的软件，那么问题来了，怎么样能以最快的速度下载安装好这些软件呢？ 公众号后台回复“远程连接”即可获得百度云链接。 然后就可以开始你的远程连接工作啦，还不快去试试！]]></content>
      <categories>
        <category>树莓派</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Win10</tag>
        <tag>SecureCRT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图片标注工具LabelImg使用教程]]></title>
    <url>%2F2019%2F04%2F17%2F%E5%9B%BE%E7%89%87%E6%A0%87%E6%B3%A8%E5%B7%A5%E5%85%B7LabelImg%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言 我们知道，图片标注主要是用来创建自己的数据集，方便进行深度学习训练。本篇博客将推荐一款十分好用的图片标注工具LabelImg，重点介绍其安装以及使用的过程。 正文 这款工具是全图形界面，用Python和Qt写的，最牛的是其标注信息可以直接转化成为XML文件，与PASCAL VOC以及ImageNet用的XML是一样的。 1.安装1.1 下载源码并运行项目地址:LabelImg 下载源码压缩包，解压可得到名为labelImg-master的文件夹，进入该文件夹，在空白处使用“Shift+鼠标右键”，进入当前目录的命令行窗口（PowerShell），依次输入下面语句即可打开软件。 12pyrcc5 -o resources.py resources.qrcpython labelImg.py 结果： 注意：上述代码成功运行的条件是电脑安装anaconda以及安装好pyqt5库。 1.2 运行打包文件百度云备份：windows_v1.3.4 密码: r7yh 无需编译，直接打开就能用！ 将百度云文件保存并下载到本地，运行目录工具/labelImg.exe即可出现1.1下载源码并运行 所示结果。 2.使用1）点击Open Dir，然后点击刚刚保存在F盘的文件夹Images/001，图片路径就会出现在右下角位置。 2）点击Create RectBox，然后在图上将交通标志框选出来。然后输入类别（一共三类）： （1）禁止标志（Prohibitory）：红色、圆形； （2）强制性标志（Mandatory）：蓝色、圆形； （3）危险标志（Danger）：黄色、三角形； 输入对应类别英文，然后点击OK。注意，有的图片不止一个交通标志，都需要标记出来。 3）然后点击Save，默认保存.xml文件即可。 4）然后点击下一张，重复操作即可。 3.注意1）xml文件也是可以用文本文档打开的。 文件包里面还提供了一个GroundTruth.csv文件，可以验证自己的标注（上图划红线部分，只要两份数据相近即可）。 2）标注过程中可随时返回进行修改，后保存的文件会覆盖之前的。 3）有时候，下载的图片集不是jpg格式，这个时候就需要转换工具。文件夹工具提供了这样一个工具，将目录工具/convert.bat拷贝到图片集，双击即可实现将所有的png文件转为jpg文件。 后文 其他同类标注工具（Github）：1）Yolo_mark2）BBox-Label-Tool3）ImageLabel]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>LabelImg</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[无人驾驶环境感知--基于Keras的车辆实时检测]]></title>
    <url>%2F2019%2F03%2F28%2F%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6%E6%84%9F%E7%9F%A5--%E5%9F%BA%E4%BA%8EKeras%E7%9A%84%E8%BD%A6%E8%BE%86%E5%AE%9E%E6%97%B6%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[前言 ​ 车辆是行驶环境中的目标检测之一，其他的还有车道线检测、交通信号灯、交通标志检测。它是无人驾驶感知的重要一环。本文基于深度学习框架Keras完成数据集的训练与测试，达到识别车辆的效果。本文适合于对深度学习有一定了解，最好用过深度学习框架的童鞋。 正文 0.导入库1234567891011import numpy as npimport matplotlib.pyplot as pltimport cv2import globfrom moviepy.editor import VideoFileClipimport keras # use 2.0.8from keras.models import Sequentialfrom keras.layers.convolutional import Convolution2D, MaxPooling2Dfrom keras.layers.advanced_activations import LeakyReLUfrom keras.layers.core import Flatten, Dense, Activation, Reshapefrom utils import load_weights,Box,yolo_net_out_to_car_boxes, draw_box 1.Fast YOLO网络结构功能：原来YOLO整个网络结构包含了24个卷积层以及2个全连接层。由于车辆检测对实时性要求高，我们使用一种YOLO的简化版本：Fast YOLO，该模型使用简单的9层卷积替代了原来的24层卷积，它牺牲了一定的精度，处理速度更快，从YOLO的45fps提升到155fps。满足实时目标检测的需求。 （原YOLO网络结构） 1.1 修改图片通道顺序代码： 12#图片通道顺序修改为(channels,,height,width),保持和input_shape一致。keras.backend.set_image_dim_ordering('th') 1.2 自定义网络结构（Fast YOLO）代码： 12345678910111213141516171819202122232425262728293031#在这里，网络的输入变成了 448×448 ， 输出是一个 7×7×30 的张量。model = Sequential()model.add(Convolution2D(16, (3, 3),input_shape=(3,448,448),padding='same',strides=(1,1)))model.add(LeakyReLU(alpha=0.1))model.add(MaxPooling2D(pool_size=(2, 2)))model.add(Convolution2D(32, (3, 3) ,padding='same'))model.add(LeakyReLU(alpha=0.1))model.add(MaxPooling2D(pool_size=(2, 2),padding='valid'))model.add(Convolution2D(64, (3, 3) ,padding='same'))model.add(LeakyReLU(alpha=0.1))model.add(MaxPooling2D(pool_size=(2, 2),padding='valid'))model.add(Convolution2D(128, (3, 3) ,padding='same'))model.add(LeakyReLU(alpha=0.1))model.add(MaxPooling2D(pool_size=(2, 2),padding='valid'))model.add(Convolution2D(256, (3, 3) ,padding='same'))model.add(LeakyReLU(alpha=0.1))model.add(MaxPooling2D(pool_size=(2, 2),padding='valid'))model.add(Convolution2D(512, (3, 3) ,padding='same'))model.add(LeakyReLU(alpha=0.1))model.add(MaxPooling2D(pool_size=(2, 2),padding='valid'))model.add(Convolution2D(1024, (3, 3) ,padding='same'))model.add(LeakyReLU(alpha=0.1))model.add(Convolution2D(1024, (3, 3),padding='same'))model.add(LeakyReLU(alpha=0.1))model.add(Convolution2D(1024, (3, 3) ,padding='same'))model.add(LeakyReLU(alpha=0.1))model.add(Flatten())model.add(Dense(256))model.add(Dense(4096))model.add(LeakyReLU(alpha=0.1))model.add(Dense(1470)) 1.3模型可视化代码： 1model.summary() 打印： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465_________________________________________________________________Layer (type) Output Shape Param # =================================================================conv2d_318 (Conv2D) (None, 16, 448, 448) 448 _________________________________________________________________leaky_re_lu_353 (LeakyReLU) (None, 16, 448, 448) 0 _________________________________________________________________max_pooling2d_213 (MaxPoolin (None, 16, 224, 224) 0 _________________________________________________________________conv2d_319 (Conv2D) (None, 32, 224, 224) 4640 _________________________________________________________________leaky_re_lu_354 (LeakyReLU) (None, 32, 224, 224) 0 _________________________________________________________________max_pooling2d_214 (MaxPoolin (None, 32, 112, 112) 0 _________________________________________________________________conv2d_320 (Conv2D) (None, 64, 112, 112) 18496 _________________________________________________________________leaky_re_lu_355 (LeakyReLU) (None, 64, 112, 112) 0 _________________________________________________________________max_pooling2d_215 (MaxPoolin (None, 64, 56, 56) 0 _________________________________________________________________conv2d_321 (Conv2D) (None, 128, 56, 56) 73856 _________________________________________________________________leaky_re_lu_356 (LeakyReLU) (None, 128, 56, 56) 0 _________________________________________________________________max_pooling2d_216 (MaxPoolin (None, 128, 28, 28) 0 _________________________________________________________________conv2d_322 (Conv2D) (None, 256, 28, 28) 295168 _________________________________________________________________leaky_re_lu_357 (LeakyReLU) (None, 256, 28, 28) 0 _________________________________________________________________max_pooling2d_217 (MaxPoolin (None, 256, 14, 14) 0 _________________________________________________________________conv2d_323 (Conv2D) (None, 512, 14, 14) 1180160 _________________________________________________________________leaky_re_lu_358 (LeakyReLU) (None, 512, 14, 14) 0 _________________________________________________________________max_pooling2d_218 (MaxPoolin (None, 512, 7, 7) 0 _________________________________________________________________conv2d_324 (Conv2D) (None, 1024, 7, 7) 4719616 _________________________________________________________________leaky_re_lu_359 (LeakyReLU) (None, 1024, 7, 7) 0 _________________________________________________________________conv2d_325 (Conv2D) (None, 1024, 7, 7) 9438208 _________________________________________________________________leaky_re_lu_360 (LeakyReLU) (None, 1024, 7, 7) 0 _________________________________________________________________conv2d_326 (Conv2D) (None, 1024, 7, 7) 9438208 _________________________________________________________________leaky_re_lu_361 (LeakyReLU) (None, 1024, 7, 7) 0 _________________________________________________________________flatten_36 (Flatten) (None, 50176) 0 _________________________________________________________________dense_106 (Dense) (None, 256) 12845312 _________________________________________________________________dense_107 (Dense) (None, 4096) 1052672 _________________________________________________________________leaky_re_lu_362 (LeakyReLU) (None, 4096) 0 _________________________________________________________________dense_108 (Dense) (None, 1470) 6022590 =================================================================Total params: 45,089,374Trainable params: 45,089,374Non-trainable params: 0_________________________________________________________________ 2.加载模型数据功能：使用已经训练好的模型，将模型参数加载到keras模型中 代码： 1load_weights(model,'yolo-tiny.weights') 注意：原YOLO整个网络包含了24个卷积层以及2个全连接层。 3.将模型用于测试单张图片3.1主程序代码： 123456789101112131415161718192021222324252627282930313233343536373839404142#加载图片imagePath = 'test_images/test1.jpg'image = plt.imread(imagePath)print("image:" + str(image.shape))##图片预处理#裁剪图片image_crop = image[300:650,500:,:]#重置尺寸448*448resized = cv2.resize(image_crop,(448,448))print("resized:" + str(resized.shape))#转置#shape：(448, 448, 3)-&gt;(3, 448, 448)batch = np.transpose(resized,(2,0,1))print("np.transpose:" + str(batch.shape))#print(batch)#将像素点取值区间从[0~255]过渡到[-1~1]batch = 2*(batch/255.) - 1#print(batch)print("batch:" + str(batch.shape))#增加一个维度#[批量大小，通道, 高度，宽度] batch = np.expand_dims(batch, axis=0)print("np.expand_dims" + str(batch.shape))#输入到YOLO网络#输出预测为每一类的置信概率，一共1470类。out = model.predict(batch)print("out:" + str(out.shape))#从YOLO网络的输出中提取出车辆的检测结果#调用yolo_net_out_to_car_boxes（）函数进行测试boxes = yolo_net_out_to_car_boxes(out[0], threshold = 0.17)#可视化结果f,(ax1,ax2) = plt.subplots(1,2,figsize=(16,6))ax1.imshow(image)#调用draw_box（）函数画框图ax2.imshow(draw_box(boxes,plt.imread(imagePath),[[500,1280],[300,650]])) 打印： 123456image:(720, 1280, 3)resized:(448, 448, 3)np.transpose:(3, 448, 448)batch:(3, 448, 448)np.expand_dims(1, 3, 448, 448)out:(1, 1470) 注意： 批量梯度下降法（Batch Gradient Descent） ：在更新参数时都使用所有的样本来进行更新。 优点：全局最优解，能保证每一次更新权值，都能降低损失函数；易于并行实现。 缺点：当样本数目很多时，训练过程会很慢。 随机梯度下降法（Stochastic Gradient Descent）：在更新参数时都使用一个样本来进行更新。每一次跟新参数都用一个样本，更新很多次。如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将参数迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次，这种方式计算复杂度太高。 优点：训练速度快； 缺点：准确度下降，并不是全局最优；不易于并行实现。从迭代的次数上来看，随机梯度下降法迭代的次数较多，在解空间的搜索过程看起来很盲目。噪音很多，使得它并不是每次迭代都向着整体最优化方向。 小批量梯度下降法（Mini-batch Gradient Descen）：在更新每一参数时都使用一部分样本来进行更新。为了克服上面两种方法的缺点，又同时兼顾两种方法的优点。 三种方法使用的情况：如果样本量比较小，采用批量梯度下降算法。如果样本太大，或者在线算法，使用随机梯度下降算法。在实际的一般情况下，采用小批量梯度下降算法。 3.2yolo_net_out_to_car_boxes()函数代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#S:7*7个网格#C:每个网格含20个目标类别信息#B:每个网格含2个bounding box，每个bounding box含5个信息#1470 = 7*7*30 =7*7*10 + 7*7*20 = 7*7*2*5 + 7*7*20def yolo_net_out_to_car_boxes(net_out, threshold=0.2, sqrt=1.8, C=20, B=2, S=7): ##定义参数 class_num = 6 boxes = [] SS = S * S #网格单元数 prob_size = SS * C #类别信息尺寸 conf_size = SS * B #网格包含2个bounding box中的信息尺寸 #列表 probs = net_out[0: prob_size]#类别信息 confs = net_out[prob_size: (prob_size + conf_size)]#置信度 cords = net_out[(prob_size + conf_size):] #(x,y,w,h) #列表转矩阵 probs = probs.reshape([SS, C]) confs = confs.reshape([SS, B]) cords = cords.reshape([SS, B, 4]) ##测试 #对整个图像的每个网格都做这种操作，则可以得到 7×7×2=987×7×2=98 个bounding box， #这些bounding box既包含坐标等信息也包含类别信息。 for grid in range(SS):#遍历7*7的每个网格 for b in range(B):#遍历2个bounding box bx = Box()#将类Box实例化 bx.c = confs[grid, b]#置信度（confidence）：代表了所预测的box中含有object的置信度和这个box预测的有多准两重信息 bx.x = (cords[grid, b, 0] + grid % S) / S#中心坐标 (x,y),即我们要预测的目标的所在的矩形区域的中心的坐标值。 bx.y = (cords[grid, b, 1] + grid // S) / S bx.w = cords[grid, b, 2] ** sqrt#bounding box的宽和高 (w,h) bx.h = cords[grid, b, 3] ** sqrt p = probs[grid, :] * bx.c#类别置信度得分 #设置阈值，滤掉得分低的boxes if p[class_num] &gt;= threshold: bx.prob = p[class_num] boxes.append(bx) #进行NMS（Non-maximum suppression）:非最大抑制处理。 #首先基于物体检测分数产生检测框，分数最高的检测框M被选中， #其他与被选中检测框有明显重叠的检测框被抑制 #boxes = [(c1,x1,y1,w1,h1,prob1),(c2,x2,y2,w2,h2,prob2)...] boxes.sort(key=lambda b: b.prob, reverse=True)#按关键词prob从大到小排序 for i in range(len(boxes)): boxi = boxes[i] #bounding box if boxi.prob == 0: continue #检测分数为0，不会产生检测框，跳出本次循环 for j in range(i + 1, len(boxes)): boxj = boxes[j]#其他bounding box if box_iou(boxi, boxj) &gt;= .4:#boxi与boxj重叠部分大于阈值0.4 boxes[j].prob = 0.#将boxj移除 boxes = [b for b in boxes if b.prob &gt; 0.]#再次筛选（移除b.prob为负值的部分） return boxes 注意1： 网络的输出 7×7×30 负责这7*7个网格的回归预测。我们来看看这每个网格的30个输出构成： 每个网格都要预测2个bounding box，bounding box即我们用来圈出目标的矩形（也就是目标所在的一个矩形区域），一个bounding box包含如下信息： 中心坐标 (x,y) 即我们要预测的目标的所在的矩形区域的中心的坐标值。 (w,h) 即bounding box的宽和高 置信度（confidence）：代表了所预测的box中含有object的置信度和这个box预测的有多准两重信息 每个网格都要预测两个bounding box，即10个输出，此外，还有20个输出代表目标的类别，YOLO论文在训练时一共检测20类物体，所以一共有20个类别的输出,我们记做 C，合集每个网格的预测输出有30个数值。 注意2： 在测试阶段，每个网格预测的类别信息和bounding box预测的confidence相乘，就得到每个bounding box的class-specific confidence score。那么对整个图像的每个网格都做这种操作，则可以得到 7×7×2=98个bounding box，这些bounding box既包含坐标等信息也包含类别信息。 得到每个bounding box的class-specific confidence score以后，设置阈值，滤掉得分低的boxes，对保留的boxes进行NMS处理，就得到最终的检测结果。 NMS（Non-maximum suppression）:非最大抑制，它首先基于物体检测分数产生检测框，分数最高的检测框M被选中，其他与被选中检测框有明显重叠的检测框被抑制。在本例中，使用YOLO网络预测出一系列带分数的预选框，当选中最大分数的检测框M，它被从集合B中移出并放入最终检测结果集合D。于此同时，集合B中任何与检测框M的重叠部分大于重叠阈值Nt的检测框也将随之移除。 3.3 draw_box()函数1234567891011121314151617181920212223242526def draw_box(boxes, im, crop_dim): imgcv = im #数学推导 [xmin, xmax] = crop_dim[0] [ymin, ymax] = crop_dim[1] for b in boxes: h, w, _ = imgcv.shape left = int((b.x - b.w / 2.) * w) right = int((b.x + b.w / 2.) * w) top = int((b.y - b.h / 2.) * h) bot = int((b.y + b.h / 2.) * h) left = int(left * (xmax - xmin) / w + xmin) right = int(right * (xmax - xmin) / w + xmin) top = int(top * (ymax - ymin) / h + ymin) bot = int(bot * (ymax - ymin) / h + ymin) if left &lt; 0: left = 0 if right &gt; w - 1: right = w - 1 if top &lt; 0: top = 0 if bot &gt; h - 1: bot = h - 1 thick = int((h + w) // 150) #cv2.rectangle（图片，（左，左上），（右，右上），颜色，粗细） cv2.rectangle(imgcv, (left, top), (right, bot), (255, 0, 0), thick) return imgcv 4.将模型用于测试图片集代码： 12345678910111213#读取test_images里面所有的图片images = [plt.imread(file) for file in glob.glob('test_images/*.jpg')]batch = np.array([np.transpose(cv2.resize(image[300:650,500:,:],(448,448)),(2,0,1)) for image in images])batch = 2*(batch/255.) - 1print("batch:" + str(batch.shape))out = model.predict(batch)#可视化结果f,((ax1,ax2),(ax3,ax4),(ax5,ax6)) = plt.subplots(3,2,figsize=(11,10))for i,ax in zip(range(len(batch)),[ax1,ax2,ax3,ax4,ax5,ax6]): boxes = yolo_net_out_to_car_boxes(out[i], threshold = 0.17) ax.imshow(draw_box(boxes,images[i],[[500,1280],[300,650]])) 打印： 1batch:(6, 3, 448, 448) 注意：代码基本是步骤3的重复。 5.将模型用于视频123456789101112131415161718#处理方法同图片def frame_func(image): crop = image[300:650,500:,:] resized = cv2.resize(crop,(448,448)) batch = np.array([resized[:,:,0],resized[:,:,1],resized[:,:,2]]) batch = 2*(batch/255.) - 1 batch = np.expand_dims(batch, axis=0) out = model.predict(batch) boxes = yolo_net_out_to_car_boxes(out[0], threshold = 0.17) return draw_box(boxes,image,[[500,1280],[300,650]])vedio_in = VideoFileClip("video_in/test01.mp4")#读取文件夹video_in视频video_out = 'video_out/test01_out.mp4'#保存视频到文件夹video_inlane_clip = vedio_in.fl_image(frame_func) #将视频以图片帧的形式传递给frame_func()函数lane_clip.write_videofile(video_out, audio=False)#重新合成视频 打印： 注意： 视频本质上还是图片 代码基本是步骤3的重复 加入视频转换成图片以及图片合成视频步骤。 以上。]]></content>
      <categories>
        <category>无人驾驶环境感知</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Keras</tag>
        <tag>车辆</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[无人驾驶环境感知--基于HSV色彩空间的交通信号灯识别（0.97）]]></title>
    <url>%2F2019%2F03%2F26%2F%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6%E6%84%9F%E7%9F%A5--%E5%9F%BA%E4%BA%8EHSV%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%97%B4%E7%9A%84%E4%BA%A4%E9%80%9A%E4%BF%A1%E5%8F%B7%E7%81%AF%E8%AF%86%E5%88%AB%EF%BC%880.97%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前言 交通信号灯是行驶环境中的目标检测之一，其他的还有车道线检测、车辆检测、交通标志检测。它是无人驾驶感知的重要一环。本文是基于特征检测完成数据集的训练与测试，达到识别交通信号灯的目的。目前准确率为0.975。 正文 0.导入库1234567import cv2 import helpers import test_functionsimport randomimport numpy as npimport matplotlib.pyplot as pltimport matplotlib.image as mpimg 1.加载交通信号灯数据集代码： 12345678##1.加载交通信号灯数据集# 图像集目录IMAGE_DIR_TRAINING = "traffic_light_images/train"IMAGE_DIR_TEST = "traffic_light_images/test"#使用helpers.py文件中的load_dataset函数加载图像IMAGE_LIST = helpers.load_dataset(IMAGE_DIR_TRAINING)print("训练集照片总数：" + str(len(IMAGE_LIST))) 注意： load_dataset（）函数代码： 123456789101112131415161718192021222324252627def load_dataset(image_dir): im_list = [] image_types = [ "red", "yellow", "green"] print("红、黄、绿照片的数量分别为：") # 遍历每个颜色类型文件夹 for im_type in image_types: # 遍历每个image_type文件夹中的每个图像文件 #获取指定目录下（traffic_light_images/train/red(yellow)(green)）的所有图片 file_lists = glob.glob(os.path.join(image_dir, im_type, "*")) #分别打印红、黄、绿照片的数量 print(len(file_lists)) for file in file_lists: im = mpimg.imread(file)# 读取图片 # 检查图片是否存在以及是否被正确读取 if not im is None: #在列表中加入图片以及该图所属的类型 im_list.append((im, im_type)) return im_list 打印： 12345红、黄、绿照片的数量分别为：72335429训练集照片总数：1187 注意： 12im_list[500][0]#表示第500张图片im_list[500][1]#表示第500张图片的标签 2.将数据集可视化功能：由上述可知，红色共有723张，黄色共有35张，绿色共有429张。所以可选择500,750,1000做可视化。 代码： 1234567891011121314151617181920212223##2.将数据集可视化# 红：723；黄：35 绿：429；#显示图像#打印图片的尺寸和标签_,ax = plt.subplots(1,3,figsize=(5,2)) #新建red_image = IMAGE_LIST[500][0] #红灯ax[0].set_title(red_image.shape,fontsize=10) #标题--图片形状ax[0].annotate(IMAGE_LIST[500][1],xy=(2,5),color='blue',fontsize='10') #标记--蓝色标签ax[0].axis('off')#省去坐标轴ax[0].imshow(red_image)yellow_image = IMAGE_LIST[750][0]ax[1].set_title(yellow_image.shape,fontsize=10)ax[1].annotate(IMAGE_LIST[750][1],xy=(2,5),color='blue',fontsize='10')ax[1].axis('off')ax[1].imshow(yellow_image)green_image = IMAGE_LIST[1000][0]ax[2].set_title(green_image.shape,fontsize=10)ax[2].annotate(IMAGE_LIST[1000][1],xy=(2,5),color='blue',fontsize='10')ax[2].axis('off')ax[2].imshow(green_image)plt.show() 打印： 注意：在matplotlib中，整个图像为一个Figure对象。在Figure对象中可以包含一个或者多个Axes对象。每个Axes(ax)对象都是一个拥有自己坐标系统的绘图区域。上面代码中创建了三个ax对象并有自己的坐标，最后这三个对象可以保存成一张图片。 3.预处理数据代码: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546##3. 预处理数据##标准化输入及输出##输入#将每张图图片的大小resize成相同的大小，因为对于分类任务来说，我们需要在每张图片上应用相同的算法，因此标准化图像尤其重要def standardize_input(image): standard_im = cv2.resize(image,(32,32)) return standard_im##输出#这里我们的标签数据是类别数据：’red’,’yellow’,’green’，因此我们可以利用one_hot方法将类别数据转换成数值数据#红灯的数值数据应该是：[1,0,0]，黄灯应该是：[0，1，0]，绿灯应该是：[0，0，1]。这些标签被称为独热编码标签。def one_hot_encode(label): if label == "red": return [1,0,0] elif label == "yellow": return [0,1,0] else: return [0,0,1]#构建一个输入RGB图像列表：图像加类型数据#并输出标准化图像列表：图像和数值数据def standardize(image_list): standard_list = [] #遍历 for item in image_list: image = item[0] label = item[1] # 调用standardize_input函数 standardized_im = standardize_input(image) # 调用one_hot_encode函数 one_hot_label = one_hot_encode(label) #加入到新的standard_list列表中 standard_list.append((standardized_im, one_hot_label)) return standard_list#标准化所有的训练图片Standardized_Train_List = standardize(IMAGE_LIST) 4.将标准化数据可视化功能：将第1100张图片经过标准化和独热编码预处理。 代码： 123456789##4.将标准化数据可视化# 红：723；黄：35 绿：429；num = 1100standard_image = Standardized_Train_List[num][0]plt.title(standard_image.shape)plt.annotate('Label [red, yellow, green]:'+ str(Standardized_Train_List[num][1]),xy=(2,5),color='blue',fontsize='12')plt.axis('off')plt.imshow(standard_image)plt.show() 打印： 5.特征提取（全文重点）功能：这里主要是先将图片从RGB域变换到HSV域，然后处理图片使图片分别变成只有红色、绿色、黄色。然后再统计图片像素点，最后根据具备某特征像素点数比较大小确定结果。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778##5. 特征提取#HSV即色相、饱和度、明度（英语：Hue, Saturation, Value），又称HSB，其中B即英语：Brightness。#色相（H）是色彩的基本属性，就是平常所说的颜色名称，如红色、黄色等。#饱和度（S）是指色彩的纯度，越高色彩越纯，低则逐渐变灰，取0-100%的数值。#明度（V），亮度（L），取0-100%。#在这里我们将使用色彩空间、形状分析及特征构造def estimate_label(rgb_image,display): ''' 使用hsv和经验阈值来确定每个图像中的红色、绿色和黄色内容。 返回基于独热标签的的分类。 ''' hsv = cv2.cvtColor(rgb_image,cv2.COLOR_RGB2HSV)#REB域转HSV域 sum_saturation = np.sum(hsv[:,:,1])# 将明度值相加 area = 32*32 avg_saturation = sum_saturation / area #求平均的明度值 #设定阈值 sat_low = int(avg_saturation*1.3)#均值的1.3倍，工程经验 val_low = 140 ##过滤图像使图像只有绿色或黄色或白色(正常红绿灯为白色或者黄色) #在车道线检测里面提过，类似于PhotoShop里面的蒙版 #Green lower_green = np.array([70,sat_low,val_low]) upper_green = np.array([100,255,255]) green_mask = cv2.inRange(hsv,lower_green,upper_green) green_result = cv2.bitwise_and(rgb_image,rgb_image,mask = green_mask) #Yellow lower_yellow = np.array([10,sat_low,val_low]) upper_yellow = np.array([60,255,255]) yellow_mask = cv2.inRange(hsv,lower_yellow,upper_yellow) yellow_result = cv2.bitwise_and(rgb_image,rgb_image,mask=yellow_mask) # Red lower_red = np.array([150,sat_low,val_low]) upper_red = np.array([180,255,255]) red_mask = cv2.inRange(hsv,lower_red,upper_red) red_result = cv2.bitwise_and(rgb_image,rgb_image,mask = red_mask) #可视化图片于画布上 if display==True: _,ax = plt.subplots(1,5,figsize=(20,10)) ax[0].set_title('rgb image') ax[0].imshow(rgb_image) ax[1].set_title('red result') ax[1].imshow(red_result) ax[2].set_title('yellow result') ax[2].imshow(yellow_result) ax[3].set_title('green result') ax[3].imshow(green_result) ax[4].set_title('hsv image') ax[4].imshow(hsv) plt.show() sum_green = findNoneZero(green_result) sum_red = findNoneZero(red_result) sum_yellow = findNoneZero(yellow_result) #分析返回的像素点数 if sum_red &gt;= sum_yellow and sum_red&gt;=sum_green: return [1,0,0]#Red if sum_yellow&gt;=sum_green: return [0,1,0]#yellow return [0,0,1]#green#遍历一个一个像素点def findNoneZero(rgb_image): rows,cols,_= rgb_image.shape counter = 0 for row in range(rows): for col in range(cols): pixels = rgb_image[row,col] if sum(pixels)!=0: counter = counter+1 return counter 6.用照片测试提取的特征功能：这里用上文可视化过的三张图片来测试。 12345678##6.用照片测试提取的特征img_test = [(red_image,'red'),(yellow_image,'yellow'),(green_image,'green')]standardtest = standardize(img_test)for img in standardtest: predicted_label = estimate_label(img[0],display = True) print('Predict label :',predicted_label) #预测标签 print('True label:',img[1])#真实标签 打印: 12Predict label : [1, 0, 0]True label: [1, 0, 0] 12Predict label : [0, 1, 0]True label: [0, 1, 0] 12Predict label : [0, 0, 1]True label: [0, 0, 1] 7.测试数据集123456789101112131415161718192021##7.测试数据集#下载测试集TEST_IMAGE_LIST = helpers.load_dataset(IMAGE_DIR_TEST)print("测试集照片总数：" + str(len(TEST_IMAGE_LIST)))#标准化测试集STANDARDIZED_TEST_LIST = standardize(TEST_IMAGE_LIST)# shuffle() 方法将序列的所有元素随机排序。random.shuffle(STANDARDIZED_TEST_LIST) #调用get_misclassified_images()函数找出测试集中所有错误分类的图片MISCLASSIFIED = get_misclassified_images(STANDARDIZED_TEST_LIST,display=False) #计算准确率total = len(STANDARDIZED_TEST_LIST) #总的测试集数目num_correct = total - len(MISCLASSIFIED)#正确测试数目accuracy = num_correct/total #准确率 print('准确率: ' + str(accuracy))print("未正确分类图片数 = " + str(len(MISCLASSIFIED)) +' out of '+ str(total)) 打印： 1234561819107测试集照片总数：297准确率: 0.9797979797979798未正确分类图片数 = 6 out of 297 注意： 调用的get_misclassified_images()函数 12345678910111213141516171819#测试集准确率计算函数 def get_misclassified_images(test_images,display=False): misclassified_images_labels = [] for image in test_images: im = image[0] #测试图片 true_label = image[1] #真实图片标签 #预测图片标签 predicted_label = estimate_label(im,display=False) #比较真实和预测标签 if(predicted_label != true_label): #把错误标签添加到列表中 misclassified_images_labels.append((im, predicted_label, true_label)) return misclassified_images_labels 后文 在这个项目中，我们使用HSV色彩空间来识别交通灯，可以改善及提高的地方，以RCNN为代表的基于Region Proposal的深度学习目标检测算法以及以YOLO为代表的基于回归方法的深度学习目标检测算法。 即采用Faster-RCNN或SSD来实现交通灯的识别 然后将训练好的模型应用于图像和视频流。]]></content>
      <categories>
        <category>无人驾驶环境感知</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>交通信号灯</tag>
        <tag>特征检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于CMOS单目摄像头的人脸检测和移动物体检测]]></title>
    <url>%2F2019%2F03%2F25%2F%E5%9F%BA%E4%BA%8E%E5%8D%95%E7%9B%AE%E6%91%84%E5%83%8F%E5%A4%B4%E7%9A%84%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E5%92%8C%E7%A7%BB%E5%8A%A8%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[前言 生活中最常见的是电脑摄像头还有监视摄像头，以及购置的CMOS摄像头。本文正是在电脑和CMOS摄像头基础上实现人脸检测和移动物体检测。 正文 1. 人脸检测功能：打开摄像头，读取帧，检测帧中的人脸，扫描检测到的人脸中的眼睛，对人脸绘制蓝色的矩形框，对眼睛绘制绿色的矩形框，并实时追踪。（人丑没贴结果照片(T＿T)） 代码： 123456789101112131415161718192021222324252627282930313233343536373839import cv2def detect(): #获取Haar级联特征 face_cascade = cv2.CascadeClassifier('cascades/haarcascade_frontalface_default.xml')#获取Haar级联数据 eye_cascade = cv2.CascadeClassifier('cascades/haarcascade_eye.xml') camera = cv2.VideoCapture(0) #打开摄像头，如果外接摄像头，更改参数为1即可。 while cv2.waitKey(27) == -1 : #发生条件为“Esc”键没有被按下 ret, frame = camera.read() gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)#灰度图像 #faces = face_cascade.detectMultiScale(gray, scaleFactor, minNeighbors) #scaleFactor:人脸检测过程中每次迭代时图像的压缩率 #minNeighbors：每个人脸矩形保留近邻数目的最小值 #返回值为人脸矩形数组 faces = face_cascade.detectMultiScale(gray, 1.3, 5) #绘制人脸方框 #x，y为左上角坐标，w和h表示人脸矩形的宽度和高度，（255，0，0）表示蓝色，2表示粗细 for (x,y,w,h) in faces: img = cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2) #设定区域为人脸方框，后面在这一片区域中绘制眼睛方框 roi_gray = gray[y:y+h, x:x+w] eyes = eye_cascade.detectMultiScale(roi_gray, 1.03, 5, 0, (40,40)) for (ex,ey,ew,eh) in eyes: cv2.rectangle(img,(x+ex,y+ey),(x+ex+ew,y+ey+eh),(0,255,0),2) cv2.imshow("Face Detection", frame) #显示摄像头帧窗口 camera.release() #释放摄像头 cv2.destroyAllWindows() #释放窗口 if __name__ == "__main__": detect() 注意： 类Haar特征是一种用于实现实时人脸跟踪的特征。 Haar级联具有尺度不变性，换句话说，它在尺度变化上具有鲁棒性。OpenCV提供了尺度不变Haar级联的分类器和追踪器，并可将其保存成指定的文件格式。 OpenCV的Haar级联不具有旋转不变性。所以在检测时，要获得最好的效果，我们需要正对着摄像头。 如果没有效果，需要摘掉眼镜。 2. 移动物体检测功能：打开摄像头（视频流），读取帧，检测帧中的移动物体，并对移动的物体绘制方框，并实时追踪。（人丑没贴结果照片(T＿T)） 代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import cv2import numpy as npcamera = cv2.VideoCapture("vedio_in/traffic.flv")#加载视频流#camera = cv2.VideoCapture(0)#加载摄像头history = 20#设置20帧作为影响背景模型的帧 bs = cv2.createBackgroundSubtractorKNN(detectShadows = True) #初始化KNN背景分割器 bs.setHistory(history)frames = 0#帧计数器while cv2.waitKey(27) == -1: ret, frame = camera.read()#read（）解码并返回下一帧，ret判断视频帧是否成功读入，frame为实际读入的图像数组 #用BackgroundSubtractorKNN来构建背景模型的历史 fgmask = bs.apply(frame)#前景掩膜 # this is just to let the background subtractor build a bit of history if frames &lt; history: frames += 1 continue ##通过对前景掩膜采用膨胀和腐蚀的方法来识别斑点及周围边框 #二值化，将像素（127~255之间的像素都设为0） th = cv2.threshold(fgmask.copy(), 127, 255, cv2.THRESH_BINARY)[1] #cv2.getStructuringElement定义结构元素 #cv2.erode腐蚀 th = cv2.erode(th, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3)), iterations = 2) #cv2.dilate膨胀 dilated = cv2.dilate(th, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (8,3)), iterations = 2) #轮廓检测 #cv2.RETR_EXTERNAL得到最外面的轮廓，这对消除包含在其他轮廓中的轮廓很有用 #cv2.CHAIN_APPROX_SIMPLE压缩水平方向，垂直方向，对角线方向的元素，只保留该方向的终点坐标，例如一个矩形轮廓只需4个点来保存轮廓信息 #cv2.findContours()函数返回两个值，一个是轮廓本身，还有一个是每条轮廓对应的属性。 contours, hier = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) for c in contours: if cv2.contourArea(c) &gt; 500: #轮廓面积 (x,y,w,h) = cv2.boundingRect(c)#x，y是矩阵左上点的坐标，w，h是矩阵的宽和高 #第二个参数：（x，y）是矩阵的左上点坐标 #第三个参数：（x+w，y+h）是矩阵的右下点坐标 cv2.rectangle(frame, (x,y), (x+w, y+h), (0, 0, 255), 1) #cv2.imshow("mog", fgmask) #cv2.imshow("thresh", th) #cv2.imshow("diff", frame &amp; cv2.cvtColor(fgmask, cv2.COLOR_GRAY2BGR)) cv2.imshow("detection", frame) #显示摄像头帧窗口camera.release()#释放摄像头cv2.destroyAllWindows()#释放窗口 注意： OpenCV提供了一个称为BackgroundSubtractor类，在分割背景和前景时很方便。 BackgroundSubtractor是一个功能很完全的类，该类不仅能执行背景分割，而且能够通过机器学习的方法提高背景检测效果，并提供将分来结果保存到文件的功能。 本程序是基于KNN背景分割器。 本程序基于分割前景和背景的差异，所以摄像头捕捉不到面部眼睛、鼻子的动作。 另外，本程序仅限于摄像头静止的场景，无论是视频流（监控摄像头）还是实际摄像头（电脑）。 3.摄像头标定（延伸）相机标定（Camera Calibration） 通常是做计算机视觉的第一步，首先，为什么要做相机标定呢？因为我们通过相机镜头记录下的图像往往存在一定程度的失真，这种失真往往表现为 图像畸变。畸变分为两类： 径向畸变（radial distortion）:由于透镜的特性，光线容易在相机镜头的边缘出现较小或者较大幅度的弯曲，称之为径向畸变。这种畸变在普通廉价的镜头中表现更加明显，径向畸变主要包括桶形畸变和枕形畸变两种。 切向畸变（tangential distortion）：是由于透镜本身与相机传感器平面（成像平面）或图像平面不平行而产生的，这种情况多是由于透镜被粘贴到镜头模组上的安装偏差导致。 畸变（distortion） 是对直线投影（rectilinear projection）的一种偏移。简单来说直线投影是场景内的一条直线投影到图片上也保持为一条直线。那畸变简单来说就是一条直线投影到图片上不能保持为一条直线了，这是一种光学畸变（optical aberration） 标定相机通常使用棋盘图像。一般来说，使用相机在各个角度拍摄20张左右的棋盘图即可完成后面的标定工作。 3.1 求摄像头畸变系数,并持久化12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import numpy as npimport cv2import globimport matplotlib.pyplot as pltimport pickle#1.求畸变系数,并持久化def calibrate_camera(cal_images_path, cal_save_path): # 准备对象点，比如 (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0) objp = np.zeros((6*9, 3), np.float32) objp[:, :2] = np.mgrid[0:9, 0:6].T.reshape(-1, 2) # 数组存储所有图像中的对象点和图像点 objpoints = [] # 现实空间中的三维点 imgpoints = [] # 图像平面中的二维点 # 列出待校准图像 images = glob.glob(cal_images_path) draw_index = 0 #创建一张画布（方法三） fig1 = plt.figure(1, figsize=(16, 9)) # 浏览待校准图像列表并搜索棋盘角 for fname in images: img = cv2.imread(fname) image_size = (img.shape[1], img.shape[0]) #改变图片大小尺寸 gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)#转为灰度图像 # 找出棋盘图中的对角（即图片中黑白相对的点的坐标） ret, corners = cv2.findChessboardCorners(gray, (9, 6),None) # 如果找到，分别添加三维点和二维点 if ret == True: objpoints.append(objp) imgpoints.append(corners) #绘制角点 img = cv2.drawChessboardCorners(img, (9, 6), corners, ret) # 可视化 #显示4个绘画过角点的棋盘 draw_index = draw_index + 1 if draw_index &lt;= 9: plt.subplot(3, 3, draw_index) plt.imshow(img) plt.title(fname) #求得这个相机的畸变系数，在后面的所有图像的矫正都可以使用这一组系数来完成。 ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, image_size, None, None) plt.show() # 持久化成字典，可以将对象以文件的形式存放在磁盘上。 #类似于tensorflow里面的持久化，方便后面复用。 cal_para = &#123;&#125; cal_para["mtx"] = mtx cal_para["dist"] = dist pickle.dump(cal_para, open(cal_save_path, "wb")) calibrate_camera('cam_cal_in/calibration*.jpg', 'calibration_paraeters.pkl') 打印： 注意： 畸变参数（distortion parameters）一般能够由五个参数来采集，我们使用 D=(k1,k2,p1,p2,k3) 来表示。 k1，k2，k3 表示径向畸变参数； p1，p2 表示切向畸变参数； cv2.calibrateCamera()函数返回的就是畸变参数，在后面的所有图像的矫正都可以使用这一组参数来完成。 3.2 测试棋盘图片1234567891011121314151617181920212223242526#2.测试棋盘图片#加载保存的模型with open('calibration_paraeters.pkl', mode='rb') as f: dist_pickle = pickle.load(f) mtx = dist_pickle["mtx"] dist = dist_pickle["dist"]#图像失真校正，返回未失真的图像def cal_undistort(img, mtx, dist): undist = cv2.undistort(img, mtx, dist, None, mtx) return undist# 读取一张棋盘图片img = cv2.imread('cam_cal_in/calibration1.jpg')#调用cal_undistort()函数undistorted = cal_undistort(img, mtx, dist)#可视化矫正前后图片f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 9))f.tight_layout()ax1.imshow(img)ax1.set_title('Original Image', fontsize=18)ax2.imshow(undistorted)ax2.set_title('Undistorted Image', fontsize=18)plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)plt.show() 打印： 3.3 测试实际路况图片12345678910111213141516#3.测试实际路况图片#读取一张实际路况图片origin_img = cv2.imread('test_images/test1.jpg')#调用cal_undistort()函数test_img = cal_undistort(origin_img, mtx, dist)#可视化矫正前后图片（方法二）f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))f.tight_layout()ax1.imshow(cv2.cvtColor(origin_img, cv2.COLOR_BGR2RGB))ax1.set_title('Original Image', fontsize=18)ax2.imshow(cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB))ax2.set_title('Undistorted Image', fontsize=18)plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)plt.show() 打印： 我们发现经过标定以后，相机拍出来的图像更接近于真实情况，因失真造成的”扭曲的直线”也被纠正过来。 以上。]]></content>
      <categories>
        <category>小项目</category>
      </categories>
      <tags>
        <tag>单目摄像头</tag>
        <tag>人脸检测</tag>
        <tag>移动物体检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[无人驾驶环境感知--基于霍夫变换的车道线检测]]></title>
    <url>%2F2019%2F02%2F20%2F%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6%E6%84%9F%E7%9F%A5--%E5%9F%BA%E4%BA%8E%E9%9C%8D%E5%A4%AB%E5%8F%98%E6%8D%A2%E7%9A%84%E8%BD%A6%E9%81%93%E7%BA%BF%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[前言车道线检测属于结构化道路检测，是无人驾驶感知的重要一环。本文是基于霍夫变换实现图片和视频流的车道线检测。经典的霍夫变换是侦测图片中的直线，之后，霍夫变换不仅能识别直线，也能够识别任何形状，常见的有圆形、椭圆形。 正文1.导入包123456import matplotlib.pyplot as pltimport matplotlib.image as mpingimport numpy as npimport cv2from moviepy.editor import VideoFileClipfrom pylab import axis 注意：如果没有对应的库，请自行pip安装。 2.可移植性最强的运行程序图片集检测代码： 123456789101112131415161718192021#图片检测 test_imgs = utils.get_images_by_dir('images_in') #读取路径images中所有的图片results=[]for img in test_imgs: res = image_input(img) #对每张图片分别进行处理 results.append(res) #每张处理后的图片依次放置到列表results中 count = 0for result in results: cv2.imwrite('images_out/%s.png' % str(count),result[:,:,::-1]) #每张图片依次保存到images_out文件夹中 count +=1 count = 0 for i in range(len(results)): plt.figure(figsize=(10,48)) #显示图片尺寸 plt.subplot(len(results),1,i+1) #len(results)行1列 plt.title('image_out %s' %str(count)) #标题 count +=1 axis('off') #不显示坐标轴 plt.imshow(results[i]) #显示 截图： utils.py代码： 123456789import osimport matplotlib.image as mping#读取指定路径下的所有图片def get_images_by_dir(dirname): img_names = os.listdir(dirname) img_paths = [dirname+'/'+img_name for img_name in img_names] imgs = [mping.imread(path) for path in img_paths] #用mping读取图片 return imgs 注意： 两段代码实现的功能是将文件夹images_in中的图片集分别进行处理（检测车道直线），然后保存到另一文件夹images_out中。 .py文件和images_in以及images_out在同一目录内。 有两种读图、存图的方式：cv2.imread，这种方式是以BGR格式图片读取的，显示的也是BGR格式，对应的用cv2.imwrite就可以保存成RGB格式。 另外一种是mping.imread，这种方式是以RGB格式图片读取的，显示的也是RGB格式。 所以如果显示或者保存RGB格式，就有可能需要用[:,:,::-1]将BGR格式转换成RGB格式。 打印： 视频流检测代码： 1234567#视频检测def annotated_video(input_file, output_file): video = VideoFileClip(input_file) #将输入视频文件分解为一帧一帧 annotated_video = video.fl_image(image_input) #对每一帧进行处理 annotated_video.write_videofile(output_file, audio=False) #合成输出视频文件，并且屏蔽声音 annotated_video("project_video.mp4","outputvideo07.mp4") 输入输出视频文件 打印： 1234567pygame 1.9.4Hello from the pygame community. https://www.pygame.org/contribute.htmlMoviepy - Building video outputvideo07.mp4.Moviepy - Writing video outputvideo07.mp4Moviepy - Done !Moviepy - video ready outputvideo07.mp4 注意：视频的本质还是图像帧。 3.主程序代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#主函数def main(image_in): print("原始图片尺寸：" + str(image_in.shape)) image = filter_colors(image_in) #filter_colors()函数 #print("filter_colors: "+str(image.shape)) #plt.imshow(image) gray = grayscale(image) #grayscale()函数 #print("grayscale: "+str(gray.shape)) #plt.imshow(gray) blur_gray = gaussian_blur(gray, kernel_size) #gaussian_blur()函数 #print("gaussian_blur: "+str(blur_gray.shape)) #plt.imshow(blur_gray) edges = canny(blur_gray, low_threshold, high_threshold) #canny()函数 #print("canny: "+str(edges.shape)) #plt.imshow(edges) imshape = image.shape #(宽、长、通道数) '''rows, cols = image_in.shape[:2] bottom_left = [cols*0.1, rows*0.95] top_left = [cols*0.4, rows*0.6] bottom_right = [cols*0.9, rows*0.95] top_right = [cols*0.6, rows*0.6] vertices = np.array([[bottom_left, top_left, top_right, bottom_right]], dtype=np.int32) print(vertices)''' vertices = np.array([[((imshape[1] * (1-trap_bottom_width)) // 2, imshape[0]),\ ((imshape[1] * (1-trap_top_width)) // 2, imshape[0] - imshape[0] * trap_height),\ (imshape[1]-(imshape[1] * (1-trap_top_width)) // 2, imshape[0] - imshape[0] * trap_height),\ (imshape[1]-(imshape[1] * (1-trap_bottom_width)) // 2, imshape[0])]]\ , dtype = np.int32) #四个坐标值确定感兴趣区域（梯形） masked_edges = region_of_interest(edges, vertices) #region_of_interest()函数 #print("region_of_interest:",masked_edges.shape) #plt.imshow(masked_edges) line_image = hough_lines(masked_edges, rho, theta, threshold, min_line_length, max_line_gap) #hough_lines()函数 #print("hough_lines:",line_image.shape) #plt.imshow(line_image) initial_image = image_in.astype('uint8') annotated_image = weighted_img(line_image, initial_image) #weighted_img()函数 #print("weighted_img:",annotated_image.shape) #plt.imshow(annotated_image) return annotated_image 打印： 12345678原始图片尺寸：(960, 1280, 3)filter_colors: (960, 1280, 3)grayscale: (960, 1280)gaussian_blur: (960, 1280)canny: (960, 1280)region_of_interest: (960, 1280)hough_lines: (960, 1280, 3)weighted_img: (960, 1280, 3) 注意： 主函数的输入是原始图片，输出是处理好的图片。其实就是整个图片的处理过程，不过中间调用多个子函数来实现对图片的一步步处理。 确定感兴趣区域 可能比较抽象，画图就好理解很多。 （感兴趣区域） 4.子函数4.1 定义全局变量功能：作为各个子函数中的参数。 1234567891011121314151617181920#设定所有参数kernel_size = 3#Canny边缘检测low_threshold = 50high_threshold = 120#感兴趣区域trap_bottom_width = 1 #百分比表达法trap_top_width = 0.3 # 同上 0.3合适trap_height = 0.41 #大部分合适0.41#霍夫变换rho = 1.0 #半径分辨率theta = 1 * np.pi/180 #角度分辨率threshold = 40 #大于此阈值的可当作线段min_line_length = 20 #指定线段最小长度 20大部分合适max_line_gap = 300 #间隙大于300则把两条线段当成一条线段， #值越大，允许线段上的断裂越大，越有可能检出潜在的直线段 4.2 各子函数注意： 各子函数打印图片为同一张图片。 4.2.1 filter_colors()函数代码： 1234567891011121314151617181920#过滤图像使图像只有白色和黄色(正常车道线为白色或者黄色)def filter_colors(img): #白色图象 white_threshold = 90 lower_white = np.uint8([0, white_threshold, 0]) upper_white = np.uint8([255, 255, 255]) #白色 white_mask = cv2.inRange(img , lower_white, upper_white) #lower_white~upper_white的范围值变为225，其他范围变为0 white_image = cv2.bitwise_and(img, img , mask=white_mask) #黄色图象 hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV) #转换到HSV颜色空间 lower_yellow = np.uint8([10,0,100]) upper_yellow = np.uint8([40,255,255]) yellow_mask = cv2.inRange(hsv, lower_yellow, upper_yellow) yellow_image = cv2.bitwise_and(img, img, mask=yellow_mask) #对两张图进行合成 image2 = cv2.addWeighted(white_image, 1., yellow_image, 1., 0.) #将两张图合在一起，1.和1.是两张图的权重 return image2 打印： 4.2.2 grayscale()函数代码： 123def grayscale(img): gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) return gray #转为灰度图像 打印： 4.2.3 gaussian_blur()函数功能： 高通滤波器是根据像素与邻近像素的亮度差值来提升该像素的亮度。 低通滤波器则是在像素与周围像素的亮度差值小于一个特定值时，平滑该像素的亮度。 主要用于去噪和模糊化，如高斯模糊是最常用的模糊滤波器，是一个削弱高频信号强度的低通滤波器。 代码： 12def gaussian_blur(img, kernel_size): return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0) #高斯模糊 打印： 4.2.4 canny()函数功能： 边缘在人类视觉和计算机视觉中起着重要作用。 OpenCV提供了许多边缘检测滤波函数，如Laplacian(), Sobel()以及Scharr()。 这些滤滤函数会将非边缘区域转为黑色，将边缘区域转为白色或其他饱和的颜色。 但它们又很容易将噪声错误地识别为边缘。解决方案就是在找到边缘之前对图像进行模糊处理。 代码： 12def canny(img, low_threshold, high_threshold): return cv2.Canny(img, low_threshold, high_threshold,kernel_size) #canny边缘检测 打印： 4.2.5 region_of_interest()函数功能： 感兴趣区域，就是我们从图像中选择一个图像区域，这个区域就是图像分析所关注的焦点。我们圈定这个区域，那么我们要处理的图像就从大图像变为一个小图像区域了，这样以便进行进一步处理，可以大大减小处理时间。 代码： 1234567891011121314151617def region_of_interest(img, vertices): mask = np.zeros_like(img) #定义一个和img维度一样大小的新矩阵（掩膜），初始化为全0 ，即全黑图像。 #根据输入图片来给掩膜填充3个或者1个颜色通道 if len(img.shape) &gt; 2: channel_count = img.shape[2] #通道数 ignore_mask_color = (255,) * channel_count else: ignore_mask_color = 255 #在全黑掩膜图像上填充“vertices”形状的图形，图形填充为白色。 cv2.fillPoly(mask, vertices, ignore_mask_color) #白色区域保留，黑色区域剔除（白显黑隐） masked_image = cv2.bitwise_and(img, mask) #（黑色，白色） return masked_image 打印： 注意： 事实上vertices即为所选取的感兴趣区域，注意上文对该段的解释。 感兴趣区域类似于photoshop里面的图层蒙版效果。 4.2.6 hough_lines()函数功能：在二值图像中查找直线。 代码： 12345678def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap): lines = cv2.HoughLinesP(img, rho, theta, threshold, minLineLength=min_line_len, maxLineGap=max_line_gap) #霍夫变换 #print(lines)np.array([]), #print(img.shape) line_img = np.zeros((*img.shape, 3), dtype=np.uint8) #带一个星号（*）参数的函数传入的参数存储为一个元组（tuple） #plt.imshow(img) draw_lines(line_img, lines) #画线 return line_img 打印： 注意：各参数 image： 必须是二值图像，推荐使用canny边缘检测的结果图像 。 rho: 线段以像素为单位的距离精度，double类型的，推荐用1.0。 theta： 线段以弧度为单位的角度精度，推荐用np.pi/180。 threshod: 累加平面的阈值参数，int类型，超过设定阈值才被检测出线段，值越大，基本上意味着检出的线段越长，检出的线段个数越少。根据情况推荐先用100试试。 lines：这个参数的意义未知，发现不同的lines对结果没影响，但是不要忽略了它的存在 。 minLineLength：线段以像素为单位的最小长度，根据应用场景设置 。 maxLineGap：同一方向上两条线段判定为一条线段的最大允许间隔（断裂），超过了设定值，则把两条线段当成一条线段，值越大，允许线段上的断裂越大，越有可能检出潜在的直线段。 4.2.7draw_lines()函数功能：在所有直线中选取符合实际场景（车道线）的直线，这里采用的是平均斜率法，然后再标记出该直线。 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899def draw_lines(img, lines, color=[255, 255, 0], thickness=15): if lines is None: return if len(lines) == 0: return draw_right = True draw_left = True ##找到所有直线的斜率 #只关心斜率绝对值大于阈值 slope_threshold = 0.5 #斜率阈值 slopes = [] #斜率 new_lines = [] #所有直线 for line in lines: x1, y1, x2, y2 = line[0] #计算斜率 if x2 - x1 == 0.: #避免除以0 slope = 999. else: slope = (y2 - y1) / (x2 - x1) if abs(slope) &gt; slope_threshold: slopes.append(slope) new_lines.append(line) lines = new_lines #所有直线 right_lines = [] #右车道所有直线 left_lines = [] #左车道所有直线 for i, line in enumerate(lines): x1, y1, x2, y2 = line[0] img_x_center = img.shape[1] / 2 #图片长度的一半 if slopes[i] &gt; 0 and x1 &gt; img_x_center and x2 &gt; img_x_center: right_lines.append(line) elif slopes[i] &lt; 0 and x1 &lt; img_x_center and x2 &lt; img_x_center: left_lines.append(line) ##运行线性回归找到最适合的车道线 #右车道 right_lines_x = [] right_lines_y = [] for line in right_lines: x1, y1, x2, y2 = line[0] right_lines_x.append(x1) right_lines_x.append(x2) right_lines_y.append(y1) right_lines_y.append(y2) #拟合得到右侧直线的斜率（right_m）和截距（right_b） if len(right_lines_x) &gt; 0: right_m, right_b = np.polyfit(right_lines_x, right_lines_y, 1) #多项式拟合函数 y = m*x + b else: right_m, right_b = 1, 1 #自定义 draw_right = False #左车道 left_lines_x = [] left_lines_y = [] for line in left_lines: x1, y1, x2, y2 = line[0] left_lines_x.append(x1) left_lines_x.append(x2) left_lines_y.append(y1) left_lines_y.append(y2) #拟合得到左侧直线的斜率（left_m）和截距（left_b） if len(left_lines_x) &gt; 0: left_m, left_b = np.polyfit(left_lines_x, left_lines_y, 1) else: left_m, left_b = 1, 1 draw_left = False #依据拟合的直线以及纵坐标求得横坐标 y1 = img.shape[0] y2 = img.shape[0] * (1-trap_height) right_x1 = (y1 - right_b) / right_m right_x2 = (y2 - right_b) / right_m left_x1 = (y1 - left_b) / left_m left_x2 = (y2 - left_b) / left_m #坐标值从float转化为 int y1 = int(y1) y2 = int(y2) right_x1 = int(right_x1) right_x2 = int(right_x2) left_x1 = int(left_x1) left_x2 = int(left_x2) #根据两侧的两个点坐标分别在图上划线，自定义颜色和线条粗细 if draw_right: cv2.line(img, (right_x1, y1), (right_x2, y2), color, thickness) if draw_left: cv2.line(img, (left_x1, y1), (left_x2, y2), color, thickness) 注意： ##找到所有直线的斜率 先通过斜率阈值筛选直线。 ##运行线性回归找到最适合的车道线。 然后利用已有的点拟合直线得到直线的斜率和截距，然后再利用已知的y坐标值求得对应的x坐标值，最后再利用（x，y）坐标值在原图上画线，即找到最合适的车道线。 直线表达式为y = m*x + b 4.2.8 weighted_img()函数代码： 12def weighted_img(img, initial_img, α=0.8, β=1.0, λ=0.): #将两张图合在一起，α和β是两张图的权重 return cv2.addWeighted(initial_img, α, img, β, λ) 打印： 注意：返回的是已经处理完的图片。 后记对于整张图而言，我们可以通过主函数和子函数这个角度来审视找到车道线的过程。 还可以通过范围搜索这个角度来审视。 第一步，感兴趣区域，缩小图片检索范围。 第二步，霍夫曼检测，检测所有直线（段）。 第三步，斜率阈值筛选，去除一部分直线（段）。 第四步，线性回归，找到最适合的直线（段）。]]></content>
      <categories>
        <category>无人驾驶环境感知</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>车道线</tag>
        <tag>霍夫变换</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[无人驾驶环境感知--基于tensorflow的交通标志识别（0.96）]]></title>
    <url>%2F2019%2F02%2F08%2F%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6%E6%84%9F%E7%9F%A5--%E5%9F%BA%E4%BA%8Etensorflow%E7%9A%84%E4%BA%A4%E9%80%9A%E6%A0%87%E5%BF%97%E8%AF%86%E5%88%AB%EF%BC%880.96%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前言 ​ 交通标志是行驶环境中的目标检测之一，其他的还有车道线检测、车辆检测、交通标志检测。它是无人驾驶感知的重要一环。本文基于谷歌的深度学习框架tensorflow完成数据集的训练与测试，达到识别交通标志的效果。目前做到的准确率为0.96。本文适合于对深度学习有一定了解，最好用过深度学习框架的童鞋。 正文 1.导入库12345import utilimport pickleimport tensorflow as tffrom sklearn.utils import shufflefrom tensorflow.contrib.layers import flatten 注意：如果没有对应的库，请自行pip安装。 2.载入数据代码： 12345678910111213###载入数据training_file = 'data/train.p' #一种可以传输或存储的格式validation_file='data/valid.p'testing_file = 'data/test.p'with open(training_file, mode='rb') as f: train = pickle.load(f) #从“文件”中，读取字符串，将它们反序列化转换为Python的数据对象，可以正常像操作数据类型的这些方法来操作它们.with open(validation_file, mode='rb') as f: valid = pickle.load(f)with open(testing_file, mode='rb') as f: test = pickle.load(f) X_train, y_train = train['features'], train['labels']X_valid, y_valid = valid['features'], valid['labels']X_test, y_test = test['features'], test['labels'] 注意： train.p文件类似于一个文件夹的压缩包，取代了传统的大文件夹里面套小文件夹（标签），小文件夹里面存放若干图片读取的麻烦。 如果想要保存一些结果或者数据以方便后续使用，可以使用pickle模块。该模块接受几乎所有的python对象，并且将其转换成字符串表，该过程叫做封装。从字符串中重构该对象，称为拆封。这些字符串表示可以方便的存储和传输。由于上述特殊性，自然要用pickle模块来读取。 X_train是一个列表，里面存放所有的训练集图片。 y_train是一个列表，里面存放的均为数字，个数与X_train里面的图片数一致，数字的大小为图片对应的标签。 3.探索数据代码： 123456789101112###探索数据n_train = X_train.shape[0] #训练集照片总数n_validation = X_valid.shape[0] #验证集照片总数n_test = X_test.shape[0] #测试集照片总数image_shape = X_train.shape[1:] #训练集每张图片大小n_classes = len(set(y_train)) #标签数，即训练集所有照片被分成43类， #每一类为同一种交通标志，只有细微差异。print("Number of training examples =", n_train)print("Number of validation examples =", n_validation)print("Number of testing examples =", n_test)print("Image data shape =", image_shape)print("Number of classes =", n_classes) 打印： 12345Number of training examples = 34799Number of validation examples = 4410Number of testing examples = 12630Image data shape = (32, 32, 3)Number of classes = 43 注意： ​ 关于训练集、验证集、测试集。有个不恰当的比喻，深度学习好比高考。训练集为前期的输入，即用家庭作业巩固学习内容；验证集为阶段性测验，用来检验所学的成果并作出修正；测试集则是最后的高考，一局出结果。所以基本现象是，准确率 训练集&gt;验证集&gt;测试集。 4.数据预处理功能：规范化 代码： 1234###数据预处理#规范化X_train_normalised = util.normalise_images(X_train, X_train)X_valid_normalised = util.normalise_images(X_valid, X_train) util.py 12345678import numpy as npdef normalise_images(imgs, dist): std = np.std(dist) #计算每一个维度上数据的标准差 #std = 128 mean = np.mean(dist) #计算每一个维度上数据的均值 #mean = 128 return (imgs - mean) / std #在每一个维度上都减去该均值。 #然后在数据的每一维度上除以该维度上数据的标准差。 注意： 机器学习里有一句名言：数据和特征决定了机器学习的上限，而模型和算法的应用只是让我们逼近这个上限。这个说法形象且深刻的提出前期数据处理和特征分析的重要性。 ​ 数据预处理中，标准的第一步是数据归一化。虽然这里有一系列可行的方法，但是这一步通常是根据数据的具体情况而明确选择的。数据归一化常用的方法包含如下几种： 简单缩放 规范化(使数据集中所有特征都具有零均值和单位方差) 简单缩放 ​ 为了使得最终的数据向量落在 [0，1] 或 [ -1，1] 的区间内（根据数据情况而定）。 ​ 在处理自然图像时，我们获得的像素值在 [0，255] 区间中，常用的处理是将这些像素值 直接除以 255，使它们 缩放到 [0，1] 中。 规范化 ​ 规范化指的是（独立地）使得数据的每一个维度具有零均值和单位方差。这是归一化中最常见的方法并被广泛地使用（例如，在使用支持向量机（SVM）时，特征标准化常被建议用作预处理的一部分）。在实际应用中，特征标准化的具体做法是：首先计算每一个维度上数据的均值（使用全体数据计算），之后在每一个维度上都减去该均值。下一步便是在数据的每一维度上除以该维度上数据的标准差。 5.设计神经网络层功能： 深度学习分为深层神经网络、卷积神经网络、循环神经网络。其中深层神经网络（全连接层）最为常见，卷积神经网络适合用来进行图像处理，循环神经网络适合进行自然语言处理。 这里是基于经典卷积网络模型——LeNet-5模型。 我使用的是3层的卷积层加上3个全连接层，准确率达到0.96；之前还使用过一个全连接层，准确率达到0.48；还有2层的卷积层加上4个全连接层，准确率达到0.93。 5.1张量代码： 1234567inputs = tf.placeholder(tf.float32,shape = [None,32, 32, 3], name = 'inputs') #定义张量inputs #保存了三个属性：类型、维度、类型labels = tf.placeholder(tf.int32, shape = [None], name = 'labels') #定义张量labels，是预测结果one_hot_y = tf.one_hot(labels,n_classes) #独热标签dropout_placeholder_conv = tf.placeholder(tf.float32) #定义张量dropout_placeholder_convdropout_placeholder_fc = tf.placeholder(tf.float32) #定义张量dropout_placeholder_fc##前向传播 注意： Tensorflow中的张量和Numpy中的数组不同，Tensorflow计算的结果不是一个具体的数字，而是一个张量的结构。 独热编码一般是在有监督学习中对数据集进行标注时候使用的，指的是在分类问题中，将存在数据类别的那一类用X表示，不存在的用Y表示，这里的X常常是1， Y常常是0。 5.2 前向传播代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def mymodel(images,dropout_conv_pct,dropout_fc_pct): mu = 0 sigma = 0.1 prev_conv_layer = inputs input_img_dimensions = [32, 32, 3] conv_filter_size = 3 conv_depth_start = 32 conv_layers_count = 3 fc_output_dims = [120,84] output_classes = n_classes conv_input_depth = input_img_dimensions[-1] #3层卷积层、过滤器尺寸是3x3，过滤器深度是32，3层全连接层 print("[mymodel] Building neural network [conv layers=&#123;0&#125;, conv filter size=&#123;1&#125;, conv start depth=&#123;2&#125;, fc layers=&#123;3&#125;]".format( conv_layers_count, conv_filter_size, conv_depth_start, len(fc_output_dims)+1)) #用for循环实现3层卷积层 for i in range(0,conv_layers_count): conv_output_depth = conv_depth_start * (2 ** (i)) #3X3卷积层 conv_W = tf.Variable(tf.truncated_normal(shape=(conv_filter_size, conv_filter_size, conv_input_depth, conv_output_depth), mean = mu, stddev = sigma)) conv_b = tf.Variable(tf.zeros(conv_output_depth)) conv_output = tf.nn.conv2d(prev_conv_layer, conv_W, strides=[1, 1, 1, 1], padding='VALID', name="conv_&#123;0&#125;".format(i)) + conv_b conv_output = tf.nn.relu(conv_output, name="conv_&#123;0&#125;_relu".format(i)) #通过relu激活函数完成去线性化 # 2x2池化层 conv_output = tf.nn.max_pool(conv_output, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID') # 应用dropout 防止过拟合，随机将部分节点的输出改为0 conv_output = tf.nn.dropout(conv_output, dropout_conv_pct) # 相应地设置循环变量 prev_conv_layer = conv_output conv_input_depth = conv_output_depth #扁平化(在保留第0轴的情况下对输入的张量进行Flatten) fc0 = flatten(prev_conv_layer) # 全连接层1,2 prev_layer = fc0 for output_dim in fc_output_dims: fcn_W = tf.Variable(tf.truncated_normal(shape=(prev_layer.get_shape().as_list()[-1], output_dim), mean = mu, stddev = sigma)) fcn_b = tf.Variable(tf.zeros(output_dim)) prev_layer = tf.nn.dropout(tf.nn.relu(tf.matmul(prev_layer, fcn_W) + fcn_b), dropout_fc_pct) #全连接层3 fc_final_W = tf.Variable(tf.truncated_normal(shape=(prev_layer.get_shape().as_list()[-1], output_classes), mean = mu, stddev = sigma)) fc_final_b = tf.Variable(tf.zeros(output_classes)) logits = tf.matmul(prev_layer, fc_final_W) + fc_final_b return logits 过程： ​ 这里我们的输入是32x32x3,第一层是3x3x32的卷积层，输出是30x30x32，然后跟着一个池化层，变成15X15X32； ​ 第二层是3x3x64的卷积层，输出是13x13x64，后面也是跟着一个池化层，变成6x6x64。 ​ 第三层是3x3x128的卷积层，输出是4x4x128，后面也是跟着一个池化层，变成2x2x64。 ​ 扁平化后2x2x64 = 512，所以全连接层设置为（512,120）、（120,84）、（84,43）。 打印： （可视化前向传播） 卷积层、池化层过程（数学角度） conv_output = tf.nn.conv2d(prev_conv_layer, conv_W, strides=[1, 1, 1, 1], padding=’VALID’, name=”conv_{0}”.format(i)) + conv_b ​ 其中有个参数 padding，在进行卷积运算，tensorflow提供两种SAME和VALID两种选择。其中”SAME“表示添加全0填充，“VALID“表示不添加。卷积层输出深度与卷积层过滤器深度一致。 （全零填充） （不全零填充，向上取整） 5.3 评估模型1234567891011121314151617181920212223242526272829##评估模型def evaluate_model(X_data, Y_data, batch_size): num_examples = len(X_data) total_accuracy = 0.0 total_loss = 0.0 sess = tf.get_default_session() for offset in range(0, num_examples, batch_size): batch_x, batch_y = X_data[offset:offset+batch_size], Y_data[offset:offset+batch_size] # 计算该512个数据样本的准确度和损失 accuracy = sess.run(accuracy_operation, feed_dict=&#123; dropout_placeholder_conv: 1.0, dropout_placeholder_fc: 1.0, inputs: batch_x, labels: batch_y &#125;) loss = sess.run(loss_operation, feed_dict=&#123; dropout_placeholder_conv: 1.0, dropout_placeholder_fc: 1.0, inputs: batch_x, labels: batch_y &#125;) # 整个数据集的期望 total_accuracy += (accuracy * len(batch_x)) total_loss += (loss * len(batch_x)) # 整个数据集的准确率和损失率 return (total_accuracy / num_examples, total_loss / num_examples) 6.训练神经网络功能： ​ 通过训练集和验证集训练神经网络，用来测试。因为训练的过程花费时间较长，为了让训练结果可以复用，需要将训练得到的神经网络模型持久化。这样在测试新的数据集，只需要先从持久化后模型文件中还原被保存的模型就可以了。 6.1定义全局变量代码： 123456789#定义一些全局变量epochs = 40learning_rate = 0.001batch_size = 512save_model_path = "datas/Traffice_sign_classifier"dropout_conv_keep_pct = 0.75dropout_fc_keep_pct = 0.5max_accuracy = 0PRINT_FREQ = 100 6.2反向传播代码： 123456789101112131415#前向传播logits = mymodel(inputs,dropout_placeholder_conv,dropout_placeholder_fc)#交叉熵来刻画损失函数（预测值和真实值） cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)#交叉熵平均值loss_operation = tf.reduce_mean(cross_entropy)#优化器optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)#通过传入loss_operation参数，使其得以更新并最小化training_operation =optimizer.minimize(loss_operation)#tf.equal函数判断两个张量的每一维是否相等，如果想等返回TRUE#tf.argmax函数来得到预测值和真实值对应的类别编号correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y,1))#首先将布尔型数据转换成实数型数据，然后计算平均值。这个平均值就是这个模型在这一组数据上的准确性。accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) 注意： ​ 神经网络模型的效果以及优化的目标是通过损失函数来定义的。 ​ 经典损失函数有处理分类问题的Softmax回归之后的交叉熵损失函数（cross_entropy = tf.nn.softmax_cross_entropy_with_logits(y,y_ )）（y为预测值，y_为真实值），还有处理回归问题的均方误差损失函数（mse = tf.resuce_mean（tf.square(y_) - y ））。 ​ 目前Tensorflow支持7种不同的优化器，比较常用的优化方法有三种：tf.train.GradientDescentOptimizer、tf.train.AdamOptimizer和tf.train.MomentumOptimizer。 6.3训练代码： 1234567891011121314151617181920212223242526272829#初始化会话并开始训练过程with tf.Session() as sess: sess.run(tf.global_variables_initializer()) #初始化所有变量 #40轮，每次选取512个样本进行训练。也就是训练结束一共进行了（num_examples/batch_size)*epochs次。 num_examples = len(X_train_normalised) print("Training mymodel [epochs=&#123;0&#125;, batch_size=&#123;1&#125;]...\n".format(epochs, batch_size)) #40轮训练 for i in range(epochs): X_train, Y_train = shuffle(X_train_normalised, y_train) #将序列的所有元素随机排序 #每一轮训练 for offset in range(0, num_examples, batch_size): end = offset + batch_size batch_x, batch_y = X_train[offset:end], Y_train[offset:end] sess.run(training_operation, feed_dict=&#123; inputs: batch_x, labels: batch_y, dropout_placeholder_conv:0.75, dropout_placeholder_fc:0.5, &#125;) training_accuracy, training_loss = evaluate_model(X_train_normalised, y_train, batch_size) validation_accuracy, validation_loss = evaluate_model(X_valid_normalised, y_valid, batch_size) #if i == 0 or (i+1) % PRINT_FREQ == 0: print("[&#123;0&#125;]\ttrain:loss=&#123;1:.4f&#125;, acc=&#123;2:.4f&#125; | val:loss=&#123;3:.4f&#125;, acc=&#123;4:.4f&#125;".format( i+1,training_loss, training_accuracy, validation_loss, validation_accuracy)) 6.4持久化代码： 123456789#tensorflow持久化 if validation_accuracy &gt; max_accuracy: max_acc = validation_accuracy #模型保存，先要创建一个Saver对象：如 saver=tf.train.Saver() #当然，如果你只想保存最后一代的模型，则只需要将max_to_keep设置为1即可 saver = tf.train.Saver(max_to_keep=1) save_path = saver.save(sess,save_model_path) #保存路径 model_file_name = "&#123;0&#125;.chkpt".format(save_model_path) print("Model &#123;0&#125; saved".format(model_file_name)) 打印： 7.测试神经网络功能：测试新的数据集，只需要先从持久化后模型文件中还原被保存的模型就可以了。可以省去训练模型耗费的大量时间。 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128###1.读取数据training_file = 'datas/train.p'testing_file = 'datas/test.p' with open(testing_file, mode='rb') as f: test = pickle.load(f)with open(training_file, mode='rb') as f: train = pickle.load(f)X_train, y_train = train['features'], train['labels']X_test, y_test = test['features'], test['labels']n_classes = len(set(y_test))######2：数据预处理X_test_normalised = util.normalise_images(X_test, X_train)print(X_test_normalised.shape,y_test.shape)###3:设计神经网络层inputs = tf.placeholder(tf.float32,shape = [None,32, 32, 3], name = 'inputs')labels = tf.placeholder(tf.int32, shape = [None], name = 'labels')one_hot_y = tf.one_hot(labels,n_classes)dropout_placeholder_conv = tf.placeholder(tf.float32)dropout_placeholder_fc = tf.placeholder(tf.float32) def mymodel(images,dropout_conv_pct, dropout_fc_pct): mu = 0 sigma = 0.1 prev_conv_layer = inputs input_img_dimensions = [32, 32, 3] conv_filter_size = 3 conv_depth_start = 32 conv_layers_count = 3 fc_output_dims = [120,84] output_classes = n_classes conv_input_depth = input_img_dimensions[-1] print("[mymodel] Building neural network [conv layers=&#123;0&#125;, conv filter size=&#123;1&#125;, conv start depth=&#123;2&#125;, fc layers=&#123;3&#125;]".format( conv_layers_count, conv_filter_size, conv_depth_start, len(fc_output_dims))) for i in range(0,conv_layers_count): # 层深度呈指数增长 conv_output_depth = conv_depth_start * (2 ** (i)) conv_W = tf.Variable(tf.truncated_normal(shape=(conv_filter_size, conv_filter_size, conv_input_depth, conv_output_depth), mean = mu, stddev = sigma)) conv_b = tf.Variable(tf.zeros(conv_output_depth)) conv_output = tf.nn.conv2d(prev_conv_layer, conv_W, strides=[1, 1, 1, 1], padding='VALID', name="conv_&#123;0&#125;".format(i)) + conv_b conv_output = tf.nn.relu(conv_output, name="conv_&#123;0&#125;_relu".format(i)) print(conv_output) # 2x2池化层 conv_output = tf.nn.max_pool(conv_output, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID') print(conv_output) # 应用dropout 防止过拟合 conv_output = tf.nn.dropout(conv_output, dropout_conv_pct) print(conv_output) # 相应地设置循环变量 prev_conv_layer = conv_output conv_input_depth = conv_output_depth # 在保留第0轴的情况下对输入的张量进行Flatten(扁平化) fc0 = flatten(prev_conv_layer) print(fc0) # 全连接层1，2 prev_layer = fc0 for output_dim in fc_output_dims: fcn_W = tf.Variable(tf.truncated_normal(shape=(prev_layer.get_shape().as_list()[-1], output_dim), mean = mu, stddev = sigma)) fcn_b = tf.Variable(tf.zeros(output_dim)) prev_layer = tf.nn.dropout(tf.nn.relu(tf.matmul(prev_layer, fcn_W) + fcn_b), dropout_fc_pct) print(prev_layer) # 全连接层3 fc_final_W = tf.Variable(tf.truncated_normal(shape=(prev_layer.get_shape().as_list()[-1], output_classes), mean = mu, stddev = sigma)) fc_final_b = tf.Variable(tf.zeros(output_classes)) logits = tf.matmul(prev_layer, fc_final_W) + fc_final_b print(logits) return logits def evaluate_model(X_data, Y_data, batch_size): num_examples = len(X_data) total_accuracy = 0.0 sess = tf.get_default_session() for offset in range(0, num_examples, batch_size): batch_x, batch_y = X_data[offset:offset+batch_size], Y_data[offset:offset+batch_size] # 计算该批次的准确度和损失 accuracy = sess.run(accuracy_operation, feed_dict=&#123; dropout_placeholder_conv: 1.0, dropout_placeholder_fc: 1.0, inputs: batch_x, labels: batch_y &#125;) # 批量元素总数加权精度 total_accuracy += (accuracy * len(batch_x)) # 在整个数据集上生成真正的平均精度 return (total_accuracy / num_examples) learning_rate = 0.001batch_size = 512logits = mymodel(inputs,dropout_placeholder_conv,dropout_placeholder_fc)cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=one_hot_y, logits=logits)loss_operation = tf.reduce_mean(cross_entropy)optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)training_operation =optimizer.minimize(loss_operation)correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))###4.测试神经网络loaded_graph = tf.Graph() save_model_path = "datas/Traffice_sign_classifier"with tf.Session() as sess: sess.run(tf.global_variables_initializer()) saver = tf.train.Saver() saver.restore(sess,save_model_path) #加载模型 test_accuracy = evaluate_model(X_test_normalised,y_test,batch_size) print("[mymodel - Test Set]\tacc=&#123;0:.4f&#125;".format(test_accuracy)) tf.reset_default_graph() #清除当前默认图中堆栈，重置默认图，实现模型参数的多次读取 注意： ​ 和训练.py文件基本一样，只是需要额外加一个加载模型的步骤。 打印： 后记 ​ 后续可以将训练好的模型应用于图像和视频流。]]></content>
      <categories>
        <category>无人驾驶环境感知</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>TensorFlow</tag>
        <tag>交通标志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于多态的职工管理系统]]></title>
    <url>%2F2019%2F01%2F16%2F%E8%81%8C%E5%B7%A5%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[1.管理系统需求职工管理系统可以用来管理公司内所有员工的信息。 本文主要利用C++来实现一个基于多态的职工管理系统。 公司中职工分为三类：普通员工、经理、老板、显示信息时，需要显示职工编号、职工姓名、职工岗位、以及职责 普通员工职责：完成经理交给的任务 经理职责：完成老板交给的任务，并下发任务给员工 老板职责：管理公司所有事物 管理系统中需要实现的功能如下： 退出管理程序：退出当前管理系统 增加职工信息：实现批量添加职工功能，将信息录入到文件中，职工信息为：职工编号、姓名、部门编号 显示职工信息：显示公司内所有职工的信息 删除离职职工：按照编号删除指定的职工 修改职工信息：按照编号修改职工个人信息 查找职工信息：按照职工的编号或者职工的姓名进行查找相关的人员信息 按照编号排序：按照职工编号，进行排序，排序规则由用户指定 清空所有文档：清空文件中记录的所有职工信息（清空前需要再次确认，防止误删） 系统界面效果图如下： 2.创建项目步骤： 创建新项目（基于多态的职工管理系统） 添加文件（职工管理系统） 3.创建管理类管理类负责的内容如下： 与用户的沟通菜单界面 对职工增删改查的操作 与文件的读写交互 3.1创建文件在头文件和源文件下分别创建workerManager.h和workerManager.cpp文件 3.2头文件实现在workerManager.h中设计管理类 代码如下： 1234567891011121314#pragma once //防止头文件重复包含#include &lt;iostream&gt; //包含输入输出流头文件using namespace std; //使用标准命名空间class WorkerManager&#123;public: //构造函数 WorkerManager(); //析构函数 ~WorkerManager()&#125;; 3.3源文件实现在workerManager.cpp中将构造和析构函数补全 12345678910#include"workerManager.h"WorkerManager::WorkerManager()&#123;&#125;WorkerManager::~WorkerManager()&#123;&#125; 至此职工管理类以创建完毕 4.菜单功能功能描述：与用户的沟通界面 4.1添加成员函数在管理类workerManager.h文件中添加成员函数void Show_Menu(); 12//展示菜单 void Show_Menu(); 4.2菜单功能实现在管理类workerManager.cpp中实现Show_Menu()函数 123456789101112131415void WorkerManager::Show_Menu()&#123; cout &lt;&lt; "********************************"&lt;&lt; endl; cout &lt;&lt; "**********欢迎使用职工管理系统！****"&lt;&lt; endl; cout &lt;&lt; "**********0.退出管理程序**********"&lt;&lt; endl; cout &lt;&lt; "**********1.增加职工信息**********"&lt;&lt; endl; cout &lt;&lt; "**********2.显示职工信息**********"&lt;&lt; endl; cout &lt;&lt; "**********3.删除离职职工**********"&lt;&lt; endl; cout &lt;&lt; "**********4.修改职工信息**********"&lt;&lt; endl; cout &lt;&lt; "**********5.查找职工信息**********"&lt;&lt; endl; cout &lt;&lt; "**********6.按照编号排序**********"&lt;&lt; endl; cout &lt;&lt; "**********7.清空所有文档**********"&lt;&lt; endl; cout &lt;&lt; "********************************"&lt;&lt; endl; cout &lt;&lt; endl;&#125; 4.3测试菜单功能在职工管理系统.cpp中测试菜单功能 代码： 12345678910111213141516#include &lt;iostream&gt;using namespace std;#include"workerManager.h"int main()&#123; //实例化管理者对象 WorkerManager wm; //调用展示菜单成员函数 wm.Show_Menu(); system("pause"); return 0;&#125; 5.退出功能5.1提供功能接口在main函数中提供分支选择，提供每个功能接口 代码： 12345678910111213141516171819202122232425262728293031323334353637383940int main()&#123; //实例化管理者对象 WorkerManager wm; int choice = 0; while (true) &#123; //调用展示菜单成员函数 wm.Show_Menu(); cout &lt;&lt; "请输入你的选择" &lt;&lt; endl; cin &gt;&gt; choice; switch (choice) &#123; case 0://退出系统 break; case 1://添加职工 break; case 2://显示职工 break; case 3://删除职工 break; case 4://修改职工 break; case 5://查找职工 break; case 6://排序职工 break; case 7://清空文件 break; default: system("cls"); break; &#125; &#125; system("pause"); return 0; &#125; 5.2实现退出功能在workerManager.h中提供退出系统的成员函数void_exitSystem(); 在workerManager.cpp提供具体的功能实现 123456void WorkerManager::exitSystem()&#123; cout &lt;&lt; "欢迎下次使用" &lt;&lt; endl; system("pause"); exit(0);&#125; 5.3测试功能在main函数分支0选项中,调用退出程序的接口 123case 0://退出系统 wm.exitSystem(); break; 6.创建职工类6.1创建职工抽象类职工的分类为：普通员工、经理、老板 将三种职工抽象到一个类（worker）中，利用多态管理不同员工种类 职工的属性为：职工编号、职工姓名、职工所在部门编号 职工的行为为：岗位职责信息描述，获取岗位名称 头文件文件夹下 创建文件worker.h文件并且添加如下代码： 123456789101112131415161718#pragma once //防止头文件重复包含#include &lt;iostream&gt; //包含输入输出流头文件#include&lt;string&gt;using namespace std; //使用标准命名空间//职工抽象基类class Worker&#123;public: //显示个人信息 virtual void showInfo() = 0; //获取岗位名称 virtual string getDepName() = 0; int m_ID;//职工编号 string m_Name;//职工姓名 int m_DeptId;//职工所在部门名称编号&#125;; 6.2创建普通员工类普通员工继承职工抽象类，并重写父类中纯虚函数 在头文件和源文件的文件夹下分别创建employee.h和employee.cpp文件 employee.h中代码如下： 1234567891011121314151617181920#pragma once //防止头文件重复包含#include &lt;iostream&gt; //包含输入输出流头文件#include"worker.h"using namespace std; //使用标准命名空间//员工类class Employee:public Worker //继承&#123;public: //构造函数 Employee(int id, string name, int dId); //显示个人信息 virtual void showInfo(); //获取岗位名称 virtual string getDepName();&#125;; employee.cpp中代码如下： 1234567891011121314151617181920#include "employee.h"Employee::Employee(int id, string name, int dId)&#123; this-&gt;m_ID = id; this-&gt;m_Name = name; this-&gt;m_DeptId = dId;&#125;void Employee::showInfo()&#123; cout &lt;&lt; "职工编号：" &lt;&lt; this-&gt;m_ID &lt;&lt; "\t职工姓名：" &lt;&lt; this-&gt;m_Name &lt;&lt; "\t岗位：" &lt;&lt; this-&gt;getDepName() &lt;&lt; "\t岗位职责：完成经理交给的任务" &lt;&lt; endl;&#125;string Employee::getDepName()&#123; return string("员工");&#125; 6.3创建经理类经理类继承职工抽象类，并重写父类中纯虚函数，和普通员工类似 在头文件和源文件的文件夹下分别创建manager.h和manager.cpp文件 manager.h中代码如下： 1234567891011121314151617181920#pragma once //防止头文件重复包含#include &lt;iostream&gt; //包含输入输出流头文件#include"worker.h"using namespace std; //使用标准命名空间//经理类class Manager :public Worker //继承&#123;public: //构造函数 Manager(int id, string name, int dId); //显示个人信息 virtual void showInfo(); //获取岗位名称 virtual string getDepName();&#125;; manager.cpp中代码如下： 1234567891011121314151617181920#include "manager.h"Manager::Manager(int id, string name, int dId)&#123; this-&gt;m_ID = id; this-&gt;m_Name = name; this-&gt;m_DeptId = dId;&#125;void Manager::showInfo()&#123; cout &lt;&lt; "职工编号：" &lt;&lt; this-&gt;m_ID &lt;&lt; "\t职工姓名：" &lt;&lt; this-&gt;m_Name &lt;&lt; "\t岗位：" &lt;&lt; this-&gt;getDepName() &lt;&lt; "\t岗位职责：完成老板交给的任务,并下发任务给员工" &lt;&lt; endl;&#125;string Manager::getDepName()&#123; return string("经理");&#125; 6.4创建老板类老板类继承职工抽象类，并重写父类中纯虚函数，和普通员工类似 在头文件和源文件的文件夹下分别创建boss.h和boss.cpp文件 boss.h中代码如下： 1234567891011121314151617181920#pragma once //防止头文件重复包含#include &lt;iostream&gt; //包含输入输出流头文件#include"worker.h"using namespace std; //使用标准命名空间//老板类class Boss :public Worker //继承&#123;public: //构造函数 Boss(int id, string name, int dId); //显示个人信息 virtual void showInfo(); //获取岗位名称 virtual string getDepName();&#125;; boss.cpp中代码如下： 1234567891011121314151617181920#include "boss.h"Boss::Boss(int id, string name, int dId)&#123; this-&gt;m_ID = id; this-&gt;m_Name = name; this-&gt;m_DeptId = dId;&#125;void Boss::showInfo()&#123; cout &lt;&lt; "职工编号：" &lt;&lt; this-&gt;m_ID &lt;&lt; "\t职工姓名：" &lt;&lt; this-&gt;m_Name &lt;&lt; "\t岗位：" &lt;&lt; this-&gt;getDepName() &lt;&lt; "\t岗位职责：管理公司所有事物" &lt;&lt; endl;&#125;string Boss::getDepName()&#123; return string("总裁");&#125; 6.5测试多态在职工管理系统.cpp中添加测试函数，并且运行能够产生多态 测试代码如下： 12345678910111213//测试代码 Worker * worker = NULL; worker = new Employee(1, "张三", 1); worker-&gt;showInfo(); delete worker; worker = new Manager(2, "李四", 2); worker-&gt;showInfo(); delete worker; worker = new Boss(3, "王五", 3); worker-&gt;showInfo(); delete worker; 7.添加职工功能描述：批量添加职工，并且保存到文件中 7.1功能分析用户在批量创建时，可能会创建不同种类的职工 如果将所有不同种类的员工都放入到一个数组中，可以将所有员工的指针维护到一个数组里 如果想在程序中维护做这个不定长度的数组，可以将数组创建到堆区。并利用Worker **的指针维护 7.2功能实现在WorkerManager.h头文件中添加成员属性 12345//记录文件中人数个数int m_EmpNum;//员工数组的指针Worker **m_EmpArray; 在WorkerManager.cpp构造函数中初始化属性 123456//初始化人数this-&gt;m_EmpNum = 0;//初始化数组指针this-&gt;m_EmpArray = NULL; 在WorkerManager.h中添加成员函数 12//增加职工void Add_Emp(); 在WorkerManager.cpp中实现该函数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980void WorkerManager::Add_Emp()&#123; cout &lt;&lt; "请输入增加职工数量：" &lt;&lt; endl; int addNum = 0; cin &gt;&gt; addNum; if (addNum &gt; 0) &#123; //计算新空间大小 int newSize = this-&gt;m_EmpNum + addNum;//新空间人数 = 原来记录人数 + 新增人数 //开辟新空间 Worker ** newSpace = new Worker*[newSize]; //将原空间下内容存放到新空间下 if (this-&gt;m_EmpArray !=NULL) &#123; for (int i = 0; i &lt; this-&gt;m_EmpNum; i++) &#123; newSpace[i] = this-&gt;m_EmpArray[i]; &#125; &#125; //输入新数据 for (int i = 0; i &lt; addNum; i++) &#123; int id; string name; int dSelect; cout &lt;&lt; "请输入第" &lt;&lt; i + 1 &lt;&lt; "个新职工编号：" &lt;&lt; endl; cin &gt;&gt; id; cout &lt;&lt; "请输入第" &lt;&lt; i + 1 &lt;&lt; "个新职工姓名：" &lt;&lt; endl; cin &gt;&gt; name; cout &lt;&lt; "请选择该职工的岗位：" &lt;&lt; endl; cout &lt;&lt; "1.普通职工" &lt;&lt; endl; cout &lt;&lt; "2.经理" &lt;&lt; endl; cout &lt;&lt; "3.老板" &lt;&lt; endl; cin &gt;&gt; dSelect; Worker * worker = NULL; switch (dSelect) &#123; case 1: worker = new Employee(id, name, 1); break; case 2: worker = new Manager(id, name, 2); break; case 3: worker = new Boss(id, name, 3); break; default: break; &#125; newSpace[this-&gt;m_EmpNum + i] = worker; &#125; //释放原有空间 delete[] this-&gt;m_EmpArray; //更改新空间的指向 this-&gt;m_EmpArray = newSpace; //更新新的个数 this-&gt;m_EmpNum = newSize; //提示信息 cout &lt;&lt; "成功添加" &lt;&lt; addNum &lt;&lt; "名新职工" &lt;&lt; endl; &#125; else &#123; cout &lt;&lt; "输出有误" &lt;&lt; endl; &#125; system("pause"); system("cls");&#125; 7.3测试添加123case 1://添加职工 wm.Add_Emp(); break; 8.文件交互-写文件功能描述：对文件进行读写 文件管理类中需要一个与文件进行交互的功能，对文件进行读写操作 8.1设定文件路径在workerManager.h中添加宏常量，并且包含头文件fstream 12#include &lt;fstream&gt;#define FILENAME "empFile.txt" 8.2成员函数声明在workerManager.h中添加成员函数void save() 12//保存文件void save(); 8.3保存文件功能实现12345678910111213void WorkerManager::save()&#123; ofstream ofs; ofs.open(FILENAME, ios::out); for (int i = 0; i &lt; this-&gt;m_EmpNum; i++) &#123; ofs &lt;&lt; this-&gt;m_EmpArray[i]-&gt;m_ID &lt;&lt; " " &lt;&lt; this-&gt;m_EmpArray[i]-&gt;m_Name &lt;&lt; " " &lt;&lt; this-&gt;m_EmpArray[i]-&gt;m_DeptId &lt;&lt; endl; &#125; ofs.close;&#125; 8.4保存文件功能测试在添加职工功能中添加成功后添加保存文件函数 9.文件交互-读文件功能描述：将文件内中的内容读取到程序中 虽然我们实现了添加职工后保存到文件的操作，但是每次开始运行程序，并没有将文件中数据读取到程序中 而我们的程序功能中还有清空文件的需求 因此构造函数初始化数据的情况分为三种 第一次使用，文本未创建 文件存在，但是数据被用户清空 文件存在，并且保存职工的所有信息 9.1文件未创建在workerManager.h中添加新的成员属性m_FileEmpty标志文件是否为空 12//标志文件是否为空bool m_FileIsEmpty; 修改WorkerManager.cpp中构造函数代码 123456789101112131415161718WorkerManager::WorkerManager()&#123; //1.文件不存在 ifstream ifs; ifs.open(FILENAME, ios::in);//读文件 if (!ifs.is_open()) &#123; cout &lt;&lt; "文件不存在" &lt;&lt; endl; //初始化人数 this-&gt;m_EmpNum = 0; //初始化数组指针 this-&gt;m_EmpArray = NULL; //初始化文件为空 this-&gt;m_FileIsEmpty = true; ifs.close(); return; &#125; 删除文件后，测试文件不存在时初始化数据功能 9.2文件存在且数据为空在workerManager.cpp中的构造函数追加代码： 12345678910111213141516//2.文件存在 数据为空 char ch; ifs &gt;&gt; ch; if (ifs.eof()) &#123; //文件为空 cout &lt;&lt; "文件为空！" &lt;&lt; endl; //初始化人数 this-&gt;m_EmpNum = 0; //初始化数组指针 this-&gt;m_EmpArray = NULL; //初始化文件为空 this-&gt;m_FileIsEmpty = true; ifs.close(); return; &#125; 9.3文件存在且保存职工数据9.3.1获取记录的职工人数在workerManager.h中添加成员函数int get_EmpNum(); 12//统计人数int get_EmpNum(); workerManager.cpp中实现 12345678910111213141516171819int WorkerManager::get_EmpNum()&#123; ifstream ifs; ifs.open(FILENAME, ios::in); int id; string name; int dID; int num = 0; while (ifs &gt;&gt; id &amp;&amp; ifs &gt;&gt; name &amp;&amp; ifs &gt;&gt; dID) &#123; //记录人数 num++; &#125; ifs.close(); return num;&#125; 在workerManager.cpp中继续追加代码： 1234//3.当文件存在，并且记录数据int num = this-&gt;get_EmpNum();cout &lt;&lt; "职工人数为：" &lt;&lt; num &lt;&lt; endl;this-&gt;m_EmpNum = num; 9.3.2初始化数组 初始化数组 根据职工的数据以及职工数据，初始化workManager中的Worker**m_EmpArray指针 在workerManager.h中添加成员函数 void init_Emp() 12//初始化员工void init_Emp(); 在workerManager.cpp中实现 12345678910111213141516171819202122232425262728293031void WorkerManager::init_Emp()&#123; ifstream ifs; ifs.open(FILENAME, ios::in); int id; string name; int dId; int index = 0; while (ifs &gt;&gt; id &amp;&amp; ifs &gt;&gt; name &amp;&amp; ifs &gt;&gt; dId) &#123; Worker * worker = NULL; //根据不同的部门Id创建不同对象 if (dId == 1) &#123; worker = new Employee(id, name, dId); &#125; else if (dId == 2) &#123; worker = new Manager(id, name, dId); &#125; else &#123; worker = new Boss(id, name, dId); &#125; //存放在数组中 this-&gt;m_EmpArray[index] = worker; index++; &#125;&#125; 10.显示职工功能描述：显示当前所有职工信息 10.1 显示职工函数声明在workerManager.h中添加成员函数void Show_Emp(); 12//显示职工void Show_Emp(); 10.2显示职工函数实现在workerManager.cpp中实现成员函数void Show_Emp(); 1234567891011121314151617void WorkerManager::Show_Emp()&#123; if (this-&gt;m_FileIsEmpty) &#123; cout &lt;&lt; "文件不存在或者记录为空" &lt;&lt; endl; &#125; else &#123; for (int i = 0; i &lt; m_EmpNum; i++) &#123; //利用多态调用接口 this-&gt;m_EmpArray[i]-&gt;showInfo; &#125; &#125; system("pause"); system("cls");&#125; 10.3显示职工函数测试测试1：文件不存在或者为空情况 测试2：文件存在且有记录情况 11.删除职工功能描述：按照职工的编号进行删除职工操作 11.1删除职工函数声明在workerManager.h中添加成员函数添加void Del_Emp()； 11.2职工是否存在函数声明很多功能都需要用到根据职工是否存在来进行操作如：删除职工、修改职工、查找职工 因此添加该公告函数，以便后续调用 在workerManager.h中添加成员函数int IsExist(int id); 11.3职工是否存在函数实现在workerManager.cpp中实现成员函数int IsExist(int id); 12345678910111213141516int WorkerManager::IsExist(int id)&#123; int index = -1; for (int i = 0; i &lt; this-&gt;m_EmpNum; i++) &#123; if (this-&gt;m_EmpArray[i]-&gt;m_ID == id) &#123; //找到职工 index = i; break; &#125; &#125; return index;&#125; 11.4删除职工函数实现在workerManager.cpp中实现成员函数void Del_Emp()； 1234567891011121314151617181920212223242526272829303132333435363738//删除职工void WorkerManager::Del_Emp()&#123; if (this-&gt;m_FileIsEmpty) &#123; cout &lt;&lt; "文件不存在或记录为空！" &lt;&lt; endl; &#125; else &#123; //按照职工编号删除 cout &lt;&lt; "请输入想要删除职工编号：" &lt;&lt; endl; int id = 0; cin &gt;&gt; id; int index = this-&gt;IsExist(id); if (index != -1)//说明职工存在，并且要删除掉index位置上的职工 &#123; for (int i = index; i &lt; this-&gt;m_EmpNum - 1; i++) &#123; //数据前移 this-&gt;m_EmpArray[i] = this-&gt;m_EmpNum[i + 1]; &#125; this-&gt;m_EmpNum--;//更新数组中记录人员个数 //数据同步更新到文件中 this-&gt;save(); cout &lt;&lt; "删除成功" &lt;&lt; endl; &#125; else &#123; cout &lt;&lt; "删除失败，未找到该职工" &lt;&lt; endl; &#125; &#125; system("pause"); system("cls");&#125; 11.5测试删除职工测试1：删除不存在职工情况 测试2：文件存在职工情况 12.修改职工12.1修改职工函数声明在workerManager.h中添加成员函数void Mod_Emp(); 12.2修改职工函数实现在workerManager.cpp中实现成员函数void Mod_Emp(); 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970//修改职工void WorkerManager::Mod_Emp()&#123; if (this-&gt;m_FileIsEmpty) &#123; cout &lt;&lt; "文件不存在或者记录为空" &lt;&lt; endl; &#125; else &#123; cout &lt;&lt; "请输入修改职工编号：" &lt;&lt; endl; int id; cin &gt;&gt; id; int ret = this-&gt;IsExist(id); if (ret != -1) &#123; //查找到编号的职工 delete this-&gt;m_EmpArray[ret]; int newId = 0; string newName = ""; int dSelect = 0; cout &lt;&lt; "查到：" &lt;&lt; id &lt;&lt; "号职工，请输入新职工号：" &lt;&lt; endl; cin &gt;&gt; newId; cout &lt;&lt; "请输入新姓名：" &lt;&lt; endl; cin &gt;&gt; newName; cout &lt;&lt; "请输入岗位：" &lt;&lt; endl; cout &lt;&lt; "1.普通职工" &lt;&lt; endl; cout &lt;&lt; "2.经理" &lt;&lt; endl; cout &lt;&lt; "3.老板" &lt;&lt; endl; cin &gt;&gt; dSelect; Worker * worker = NULL; switch (dSelect) &#123; case 1: worker = new Employee(newId, newName, dSelect); break; case 2: worker = new Manager(newId, newName, dSelect); break; case 3: worker = new Boss(newId, newName, dSelect); break; default: break; &#125; //更新数据 到数组中 this-&gt;m_EmpArray[ret] = worker; //提示信息 cout &lt;&lt; "修改成功" &lt;&lt; endl; //保存到文件中 this-&gt;save(); &#125; else &#123; cout &lt;&lt; "修改失败，查无此人" &lt;&lt; endl; &#125; &#125; //清屏 system("pause"); system("cls");&#125; 13.查找职工功能描述：提供两种职工查找方式，一种按照职工编号，一种按照职工姓名 13.1查找职工函数声明在workerManager.h中添加成员函数void Find_Emp(); 13.2查找职工函数实现在workerManager.h中实现成员函数void Find_Emp(); 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071//查找职工void WorkerManager::Find_Emp()&#123; if (this-&gt;m_FileIsEmpty) &#123; cout &lt;&lt; "文件不存在或者记录为空" &lt;&lt; endl; &#125; else &#123; cout &lt;&lt; "请输入查找的方式：" &lt;&lt; endl; cout &lt;&lt; "1.按职工编号查找" &lt;&lt; endl; cout &lt;&lt; "2.按职工姓名查找" &lt;&lt; endl; int select = 0; cin &gt;&gt; select; if (select ==1 ) &#123; //按照编号查 int id; cout &lt;&lt; "请输入查找的职工编号：" &lt;&lt; endl; cin &gt;&gt; id; int ret = IsExist(id); if (ret != -1) &#123; cout &lt;&lt; "查找成功！该职工信息如下：" &lt;&lt; endl; this-&gt;m_EmpArray[ret]-&gt;showInfo(); &#125; else &#123; cout &lt;&lt; "查找失败，查无此人" &lt;&lt; endl; &#125; &#125; else if(select == 2) &#123; //按照姓名查 string name; cout &lt;&lt; "请输入查找的姓名：" &lt;&lt; endl; cin &gt;&gt; name; // 加入判断是否查到的标志 bool flag = false; //默认未找到职工 for (int i = 0; i &lt; m_EmpNum; i++) &#123; if (this-&gt;m_EmpArray[i]-&gt;m_Name == name) &#123; cout &lt;&lt; "查找成功！职工编号为：" &lt;&lt; this-&gt;m_EmpArray[i]-&gt;m_ID &lt;&lt;"号职工信息如下："&lt;&lt; endl; flag = true; this-&gt;m_EmpArray[i]-&gt;showInfo(); &#125; &#125; if (flag = false) &#123; cout &lt;&lt; "查找失败，查无此人" &lt;&lt; endl; &#125; &#125; else &#123; cout &lt;&lt; "输入选项有误！" &lt;&lt; endl; &#125; &#125; //按任意键清屏 system("pause"); system("cls");&#125; 13.3测试查找职工测试1：按照职工编号查找-查找不存在职工 测试2：按照职工编号查找-查找存在职工 测试3：按照职工姓名查找-查找不存在职工 测试4：按照职工姓名查找-查找存在职工（如果出现重名，也一并显示，在文件中可以添加重名职工） 14.排序功能描述：按照职工编号进行排序 14.1排序函数声明 在workerManager.h中添加成员函数void Sort_Emp(); 14.2排序函数实现 在workerManager.cpp中实现成员函数void Sort_Emp(); 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//排序职工void WorkerManager::Sort_Emp()&#123; if (this-&gt;m_FileIsEmpty) &#123; cout &lt;&lt; "文件不存在或者记录为空" &lt;&lt; endl; //清屏 system("pause"); system("cls"); &#125; else &#123; cout &lt;&lt; "请选择排序的方式：" &lt;&lt; endl; cout &lt;&lt; "1.按职工编号进行升序排" &lt;&lt; endl; cout &lt;&lt; "2.按职工编号进行降序排" &lt;&lt; endl; int select = 0; cin &gt;&gt; select; for (int i = 0; i &lt; m_EmpNum; i++) &#123; int minOrMax = i;//声明最小值或者最大值下标 for (int j = i + 1; j &lt; this-&gt;m_EmpNum; j++) &#123; if (select == 1)//升序 &#123; if (this-&gt;m_EmpArray[minOrMax]-&gt;m_ID &gt; this-&gt;m_EmpArray[j]-&gt;m_ID) &#123; minOrMax = j; &#125; &#125; else //降序 &#123; if (this-&gt;m_EmpArray[minOrMax]-&gt;m_ID &lt; this-&gt;m_EmpArray[j]-&gt;m_ID) &#123; minOrMax = j; &#125; &#125; &#125; //判断一开始认定 最小值或最大值 是不是 计算的 最小值或最大值，如果不是 交换数据 if (i != minOrMax ) &#123; Worker * temp = this-&gt;m_EmpArray[i]; this-&gt;m_EmpArray[i] = this-&gt;m_EmpArray[minOrMax]; this-&gt;m_EmpArray[minOrMax] = temp; &#125; &#125; cout &lt;&lt; "排序成功！排序后的结果为：" &lt;&lt; endl; this-&gt;save(); this-&gt;Show_Emp();&#125; 15.清空文件15.1清空函数声明 在workerManager.h中添加成员函数void Clean_File(); 15.2清空函数实现 在workerManager.cpp中实现成员函数void Clean_File(); 12345678910111213141516171819202122232425262728293031323334353637//清空文件void WorkerManager::Clean_File()&#123; cout &lt;&lt; "确定清空？" &lt;&lt; endl; cout &lt;&lt; "1.确定" &lt;&lt; endl; cout &lt;&lt; "2.返回" &lt;&lt; endl; int select = 0; cin &gt;&gt; select; if (select == 1) &#123; //清空文件 ofstream ofs(FILENAME, ios::trunc); ofs.close(); if (this-&gt;m_EmpArray != NULL) &#123; for (int i = 0; i &lt; this-&gt;m_EmpNum; i++) &#123; //删除堆区的每个职工对象 delete this-&gt;m_EmpArray[i]; this-&gt;m_EmpArray[i] = NULL; //删除堆区数组指针 delete[] this-&gt;m_EmpArray; this-&gt;m_EmpArray = NULL; this-&gt;m_EmpNum = 0; this-&gt;m_FileIsEmpty = true; &#125; cout &lt;&lt; "清除成功！" &lt;&lt; endl; &#125; //清屏 system("pause"); system("cls"); &#125;&#125; 以上。]]></content>
      <categories>
        <category>小项目</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通讯录管理系统]]></title>
    <url>%2F2019%2F01%2F04%2F%E9%80%9A%E8%AE%AF%E5%BD%95%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[1.系统需求通讯录是一个可以记录亲人、好友信息的工具。 本博客主要是利用C++来实现一个通讯录管理系统。 系统需要实现的功能如下： 添加联系人：向通讯录中添加新人，信息包括（姓名、性别、年龄、联系电话、家庭住址）最多记录10000人 显示联系人：显示通讯录中所有联系人信息 删除联系人：按照姓名进行删除指定联系人 查找联系人：按照姓名查看指定联系人信息 修改联系人：按照姓名重新修改指定联系人 清空联系人：清空通讯录中所有信息 退出通讯录：退出当前使用的通讯录 2.菜单功能功能描述：用户选择功能的界面 效果图如下： 步骤： 封装函数void showMenu（）显示该界面 在main函数中调用封装好的函数 代码： 12345678910111213//显示菜单界面void showMenu()&#123; cout &lt;&lt; "**************************" &lt;&lt; endl; cout &lt;&lt; "***** 1.添加联系人 *****" &lt;&lt; endl; cout &lt;&lt; "***** 2.显示联系人 ***** " &lt;&lt; endl; cout &lt;&lt; "***** 3.删除联系人 ***** " &lt;&lt; endl; cout &lt;&lt; "***** 4.查找联系人 ***** " &lt;&lt; endl; cout &lt;&lt; "***** 5.修改联系人 ***** " &lt;&lt; endl; cout &lt;&lt; "***** 6.清空联系人 ***** " &lt;&lt; endl; cout &lt;&lt; "***** 7.退出通讯录 ***** " &lt;&lt; endl; cout &lt;&lt; "**************************" &lt;&lt; endl;&#125; 注意：为了使界面更加的美观整洁，可以适当添加 *。 3.退出功能功能描述：退出通讯录系统 思路：根据用户不同的选择，进入不同的功能，可以选择switch分支结构，将整个架构进行搭建 当用户选择7的时候，执行退出，选择其他先不做操作，也不会退出程序 12345678910111213141516171819202122232425262728293031323334int main()&#123; int select = 0; //创建用户选择输入的变量 while (true) &#123; //菜单调用 showMenu(); cin &gt;&gt; select; switch (select) &#123; case 1: //添加联系人 break; case 2: //显示联系人 break; case 3: //删除联系人 break; case 4: //查找联系人 break; case 5: //修改联系人 break; case 6: //清空联系人 break; case 7: //退出通讯录 cout &lt;&lt; "欢迎下次使用" &lt;&lt; endl; system("pause"); return 0; break; default: break; &#125; &#125; 注意：按7退出switch分支结构，但仍在while循环中，即会显示菜单。 5.添加联系人功能描述：实现添加联系人功能联系人上限为1000人，联系人信息包括姓名、性别、年龄、联系电话、家庭住址。 步骤： 设计联系人结构体 设计通讯录结构体 main函数中创建通讯录 封装添加联系人函数 测试添加联系人功能 5.1设计联系人结构体联系人信息包括：姓名、性别、年龄、联系电话、家庭住址 代码： 12345678910#define MAX 1000//设计联系人结构体struct Person&#123; string m_Name; int m_Sex; int m_Age; string m_Phone; string m_Addr;&#125;; 5.2设计通讯录结构体在通讯录结构体中，维护一个容量为1000的存放联系人的数组，并记录当前通讯录中联系人数量 123456789#define MAX 1000//设计通讯录结构体struct Addressbooks&#123; //通讯录中保存的联系人数组 struct Person personArray[MAX]; //通讯录中当前记录联系人个数 int m_Size;&#125;; 5.3 main函数中创建通讯录1234//创建通讯录结构体变量Addressbooks abs;//初始化通讯录中当前人员个数abs.m_Size = 0; 5.4 封装添加联系人函数1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162//1.添加联系人void addPerson(Addressbooks * abs)&#123; if (abs -&gt;m_Size ==MAX) &#123; cout &lt;&lt; "通讯录已满，无法添加！" &lt;&lt; endl; return; &#125; else &#123; //添加具体联系人 //姓名 string name; cout &lt;&lt; "请输入姓名：" &lt;&lt; endl; cin &gt;&gt; name; abs-&gt;personArray[abs-&gt;m_Size].m_Name = name; //性别 cout &lt;&lt; "请输入性别：" &lt;&lt; endl; cout &lt;&lt; "1 --男 " &lt;&lt; endl; cout &lt;&lt; "2 --女 " &lt;&lt; endl; int sex = 0; while (true) &#123; cin &gt;&gt; sex; if (sex == 1 || sex == 2) &#123; abs-&gt;personArray[abs-&gt;m_Size].m_Sex = sex; break; &#125; cout &lt;&lt; "输入有误，请重新输入" &lt;&lt; endl; &#125; //年龄 cout &lt;&lt; "请输入年龄：" &lt;&lt; endl; int age = 0; cin &gt;&gt; age; abs-&gt;personArray[abs-&gt;m_Size].m_Age = age; //电话 cout &lt;&lt; "请输入电话：" &lt;&lt; endl; string phone; cin &gt;&gt; phone; abs-&gt;personArray[abs-&gt;m_Size].m_Phone = phone; //住址 cout &lt;&lt; "请输入家庭住址：" &lt;&lt; endl; string address; cin &gt;&gt; address; abs-&gt;personArray[abs-&gt;m_Size].m_Addr = address; //更新通讯录人数 abs-&gt;m_Size++; cout &lt;&lt; "添加成功" &lt;&lt; endl; system("pause");//请按任意键继续 system("cls");//清屏操作 &#125;&#125; 5.5测试添加联系人功能123case 1: //添加联系人 addPerson(&amp;abs); //利用地址传递，可以修饰实参 break; 注意： 判断通讯录人数有没有满 使用清屏操作，只显示菜单，继续使用通讯录。 6.显示联系人功能描述：显示通讯录中已有联系人的信息 步骤： 封装显示联系人函数 测试显示联系人功能 6.1 封装显示联系人函数思路：判断如果当前通讯录中没有人员，就提示记录为空，人数大于0，遍历显示通讯录中信息 代码： 123456789101112131415161718192021//2.显示所有联系人void showPerson(Addressbooks * abs)&#123; if (abs-&gt;m_Size == 0) &#123; cout &lt;&lt; "当前记录为空" &lt;&lt; endl; &#125; else &#123; for (int i = 0; i &lt; abs-&gt;m_Size; i++) &#123; cout &lt;&lt; "姓名：" &lt;&lt; abs-&gt;personArray[i].m_Name &lt;&lt; " "; cout &lt;&lt; "性别：" &lt;&lt; (abs-&gt;personArray[i].m_Sex == 1 ?"男":"女") &lt;&lt; " "; cout &lt;&lt; "年龄：" &lt;&lt; abs-&gt;personArray[i].m_Age &lt;&lt; " "; cout &lt;&lt; "联系人方式：" &lt;&lt; abs-&gt;personArray[i].m_Phone &lt;&lt; " "; cout &lt;&lt; "家庭住址：" &lt;&lt; abs-&gt;personArray[i].m_Addr &lt;&lt; endl; &#125; &#125; system("pause");//请按任意键继续 system("cls");//清屏操作&#125; 6.2 测试显示联系人功能123case 2: //显示联系人 showPerson(&amp;abs); break; 7.删除联系人功能描述：按照姓名进行删除指定联系人 步骤： 封装检测联系人是否存在 封装删除联系人函数 测试删除联系人函数 7.1 封装检测联系人是否存在思路：删除联系人前，我们需要先判断用户输入的联系人是否存在，如果存在删除，不存在提示用户没有要删除的联系人 因此我们可以把检测联系人是否存在封装成一个函数中，如果存在，返回联系人在通讯录中的位置，不存在返回-1 代码： 1234567891011121314//检测联系人函数//检测联系人是否存在，如果存在，返回联系人所在数组中的具体位置，不存在返回-1//参数1 通讯录 参数2 对比姓名int isExist(Addressbooks * abs, string name)&#123; for (int i = 0; i &lt; abs-&gt;m_Size; i++) &#123; if (abs-&gt;personArray[i].m_Name == name ) &#123; return i; &#125; &#125; return -1;&#125; 7.2 封装删除联系人函数123456789101112131415161718192021222324252627//3.删除指定联系人void deletePerson(Addressbooks * abs)&#123; cout &lt;&lt; "请输入您要删除的联系人" &lt;&lt; endl; string name; cin &gt;&gt; name; int ret = isExist(abs,name); if (ret != -1) &#123; for (int i = ret; i &lt; abs-&gt;m_Size; i++) &#123; abs-&gt;personArray[i] = abs-&gt;personArray[i + 1]; &#125; abs-&gt;m_Size--; cout &lt;&lt; "删除成功" &lt;&lt; endl; &#125; else &#123; cout &lt;&lt; "查无此人" &lt;&lt; endl; &#125; system("pause");//请按任意键继续 system("cls");//清屏操作&#125; 7.3 测试删除联系人函数123case 3: //删除联系人 deletePerson(&amp;abs); break; 注意： 删除联系人思路为下标为2位置的数据，将下标为2位置后的数据做向前移动，并让通讯录中记录人员的个数做-1的操作。 8.查找联系人功能描述：按照姓名查看指定联系人信息 步骤： 封装查找联系人函数 测试查找联系人函数 8.1封装查找联系人函数思路：判断用户指定的联系人是否存在，如果存在显示信息，不存在则提示查无此人 代码： 123456789101112131415161718192021222324void searchPerson(Addressbooks * abs)&#123; cout &lt;&lt; "请输入您要查找的联系人" &lt;&lt; endl; string name; cin &gt;&gt; name; int ret = isExist(abs, name); if (ret != -1) &#123; cout &lt;&lt; "姓名：" &lt;&lt; abs-&gt;personArray[ret].m_Name &lt;&lt; " "; cout &lt;&lt; "性别：" &lt;&lt; (abs-&gt;personArray[ret].m_Sex == 1 ? "男" : "女") &lt;&lt; " "; cout &lt;&lt; "年龄：" &lt;&lt; abs-&gt;personArray[ret].m_Age &lt;&lt; " "; cout &lt;&lt; "联系人方式：" &lt;&lt; abs-&gt;personArray[ret].m_Phone &lt;&lt; " "; cout &lt;&lt; "家庭住址：" &lt;&lt; abs-&gt;personArray[ret].m_Addr &lt;&lt; endl; &#125; else &#123; cout &lt;&lt; "查无此人" &lt;&lt; endl; &#125; system("pause");//请按任意键继续 system("cls");//清屏操作&#125; 8.2测试查找联系人函数123case 4: //查找联系人 searchPerson(&amp;abs); break; 9.修改联系人功能描述：按照姓名重新修改指定联系人 步骤： 封装修改联系人函数 测试修改联系人函数 9.1封装修改联系人函数思路：查找用户输入的联系人，如果查找成功进行修改操作，查找失败提示查无此人 代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162//5.修改联系人void modifyPerson(Addressbooks * abs)&#123; cout &lt;&lt; "请输入您要修改的联系人" &lt;&lt; endl; string name; cin &gt;&gt; name; int ret = isExist(abs, name); if (ret != -1) &#123; //姓名 string name; cout &lt;&lt; "请输入姓名：" &lt;&lt; endl; cin &gt;&gt; name; abs-&gt;personArray[ret].m_Name = name; //性别 cout &lt;&lt; "请输入性别：" &lt;&lt; endl; cout &lt;&lt; "1 --男 " &lt;&lt; endl; cout &lt;&lt; "2 --女 " &lt;&lt; endl; int sex = 0; while (true) &#123; cin &gt;&gt; sex; if (sex == 1 || sex == 2) &#123; abs-&gt;personArray[ret].m_Sex = sex; break; &#125; cout &lt;&lt; "输入有误，请重新输入" &lt;&lt; endl; &#125; //年龄 cout &lt;&lt; "请输入年龄：" &lt;&lt; endl; int age = 0; cin &gt;&gt; age; abs-&gt;personArray[ret].m_Age = age; //电话 cout &lt;&lt; "请输入电话：" &lt;&lt; endl; string phone; cin &gt;&gt; phone; abs-&gt;personArray[ret].m_Phone = phone; //住址 cout &lt;&lt; "请输入家庭住址：" &lt;&lt; endl; string address; cin &gt;&gt; address; abs-&gt;personArray[ret].m_Addr = address; &#125; else &#123; cout &lt;&lt; "查无此人" &lt;&lt; endl; &#125; system("pause");//请按任意键继续 system("cls");//清屏操作&#125; 9.2 测试修改联系人函数123case 5: //修改联系人 modifyPerson(&amp;abs); break; 注意：这里基本操作和添加联系人类似 10.清空联系人功能描述：清空通讯录中所有信息 步骤： 封装清空联系人联系人函数 测试清空联系人联系人函数 10.1封装清空联系人联系人函数思路：只要将通讯录中记录的联系人数量重置为0，做逻辑清空即可 代码： 12345678//6.清空通讯录void cleanPerson(Addressbooks * abs)&#123; abs-&gt;m_Size = 0; //将当期记录联系人数重置为0；做逻辑清空操作 cout &lt;&lt; "通讯录已清空" &lt;&lt; endl; system("pause");//请按任意键继续 system("cls");//清屏操作&#125; 10.1封装清空联系人联系人函数123case 6: //清空联系人 cleanPerson(&amp;abs); break; 以上。]]></content>
      <categories>
        <category>小项目</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
</search>
